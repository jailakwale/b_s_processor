<!DOCTYPE html>
<!-- saved from url=(0099)https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794 -->
<html lang="en" data-rh="lang"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><script async="" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/branch-latest.min.js"></script><script async="" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/analytics.js"></script><script defer="" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/16180790160.js"></script><title>Text Classification with NLP: Tf-Idf vs Word2Vec vs BERT | by Mauro Di Pietro | Towards Data Science</title><meta data-rh="true" name="viewport" content="width=device-width,minimum-scale=1,initial-scale=1"><meta data-rh="true" name="theme-color" content="#000000"><meta data-rh="true" name="twitter:app:name:iphone" content="Medium"><meta data-rh="true" name="twitter:app:id:iphone" content="828256236"><meta data-rh="true" property="al:ios:app_name" content="Medium"><meta data-rh="true" property="al:ios:app_store_id" content="828256236"><meta data-rh="true" property="al:android:package" content="com.medium.reader"><meta data-rh="true" property="fb:app_id" content="542599432471018"><meta data-rh="true" property="og:site_name" content="Medium"><meta data-rh="true" property="og:type" content="article"><meta data-rh="true" property="article:published_time" content="2021-07-15T08:24:21.612Z"><meta data-rh="true" name="title" content="Text Classification with NLP: Tf-Idf vs Word2Vec vs BERT | by Mauro Di Pietro | Towards Data Science"><meta data-rh="true" property="og:title" content="Text Classification with NLP: Tf-Idf vs Word2Vec vs BERT"><meta data-rh="true" property="twitter:title" content="Text Classification with NLP: Tf-Idf vs Word2Vec vs BERT"><meta data-rh="true" name="twitter:site" content="@TDataScience"><meta data-rh="true" name="twitter:app:url:iphone" content="medium://p/41ff868d1794"><meta data-rh="true" property="al:android:url" content="medium://p/41ff868d1794"><meta data-rh="true" property="al:ios:url" content="medium://p/41ff868d1794"><meta data-rh="true" property="al:android:app_name" content="Medium"><meta data-rh="true" name="description" content="In this article, using NLP and Python, I will explain 3 different strategies for text multiclass classification: the old-fashioned Bag-of-Words (with Tf-Idf ), the famous Word Embedding (with…"><meta data-rh="true" property="og:description" content="Preprocessing, Model Design, Evaluation, Explainability for Bag-of-Words, Word Embedding, Language models"><meta data-rh="true" property="twitter:description" content="Preprocessing, Model Design, Evaluation, Explainability for Bag-of-Words, Word Embedding, Language models"><meta data-rh="true" property="og:url" content="https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794"><meta data-rh="true" property="al:web:url" content="https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794"><meta data-rh="true" property="og:image" content="https://miro.medium.com/freeze/max/1200/1*T8WWibd7u8b7gfgeG0LgAA.gif"><meta data-rh="true" name="twitter:image:src" content="https://miro.medium.com/freeze/max/1200/1*T8WWibd7u8b7gfgeG0LgAA.gif"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="article:author" content="https://mdipietro09.medium.com"><meta data-rh="true" name="twitter:creator" content="@maurodp90"><meta data-rh="true" name="author" content="Mauro Di Pietro"><meta data-rh="true" name="robots" content="index,follow,max-image-preview:large"><meta data-rh="true" name="referrer" content="unsafe-url"><meta data-rh="true" name="twitter:label1" content="Reading time"><meta data-rh="true" name="twitter:data1" content="22 min read"><link data-rh="true" rel="search" type="application/opensearchdescription+xml" title="Medium" href="https://towardsdatascience.com/osd.xml"><link data-rh="true" rel="apple-touch-icon" sizes="152x152" href="https://miro.medium.com/fit/c/152/152/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"><link data-rh="true" rel="apple-touch-icon" sizes="120x120" href="https://miro.medium.com/fit/c/120/120/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"><link data-rh="true" rel="apple-touch-icon" sizes="76x76" href="https://miro.medium.com/fit/c/76/76/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"><link data-rh="true" rel="apple-touch-icon" sizes="60x60" href="https://miro.medium.com/fit/c/60/60/1*sHhtYhaCe2Uc3IU0IgKwIQ.png"><link data-rh="true" rel="mask-icon" href="https://cdn-static-1.medium.com/_/fp/icons/Medium-Avatar-500x500.svg" color="#171717"><link data-rh="true" id="glyph_preload_link" rel="preload" as="style" type="text/css" href="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/unbound.css"><link data-rh="true" id="glyph_link" rel="stylesheet" type="text/css" href="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/unbound.css"><link data-rh="true" rel="author" href="https://mdipietro09.medium.com/"><link data-rh="true" rel="canonical" href="https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794"><link data-rh="true" rel="alternate" href="android-app://com.medium.reader/https/medium.com/p/41ff868d1794"><link data-rh="true" rel="icon" href="https://miro.medium.com/fit/c/256/256/1*ChFMdf--f5jbm-AYv6VdYA@2x.png"><script data-rh="true" type="application/ld+json">{"@context":"http:\u002F\u002Fschema.org","@type":"NewsArticle","image":["https:\u002F\u002Fmiro.medium.com\u002Ffreeze\u002Fmax\u002F1200\u002F1*T8WWibd7u8b7gfgeG0LgAA.gif"],"url":"https:\u002F\u002Ftowardsdatascience.com\u002Ftext-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794","dateCreated":"2020-07-18T20:57:22.467Z","datePublished":"2020-07-18T20:57:22.467Z","dateModified":"2021-07-15T08:24:25.430Z","headline":"Text Classification with NLP: Tf-Idf vs Word2Vec vs BERT","name":"Text Classification with NLP: Tf-Idf vs Word2Vec vs BERT","description":"In this article, using NLP and Python, I will explain 3 different strategies for text multiclass classification: the old-fashioned Bag-of-Words (with Tf-Idf ), the famous Word Embedding (with…","identifier":"41ff868d1794","author":{"@type":"Person","name":"Mauro Di Pietro","url":"https:\u002F\u002Fmdipietro09.medium.com"},"creator":["Mauro Di Pietro"],"publisher":{"@type":"Organization","name":"Towards Data Science","url":"towardsdatascience.com","logo":{"@type":"ImageObject","width":165,"height":60,"url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F330\u002F1*mG6i4Bh_LgixUYXJgQpYsg@2x.png"}},"mainEntityOfPage":"https:\u002F\u002Ftowardsdatascience.com\u002Ftext-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794","isAccessibleForFree":"False","hasPart":{"@type":"WebPageElement","isAccessibleForFree":"False","cssSelector":".meteredContent"}}</script><link rel="preload" href="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/16180790160.js" as="script"><style type="text/css" data-fela-rehydration="575" data-fela-type="STATIC">html{box-sizing:border-box}*, *:before, *:after{box-sizing:inherit}body{margin:0;padding:0;text-rendering:optimizeLegibility;-webkit-font-smoothing:antialiased;color:rgba(0,0,0,0.8);position:relative;min-height:100vh}h1, h2, h3, h4, h5, h6, dl, dd, ol, ul, menu, figure, blockquote, p, pre, form{margin:0}menu, ol, ul{padding:0;list-style:none;list-style-image:none}main{display:block}a{color:inherit;text-decoration:none}a, button, input{-webkit-tap-highlight-color:transparent}img, svg{vertical-align:middle}button{background:transparent;overflow:visible}button, input, optgroup, select, textarea{margin:0}:root{--reach-tabs:1;--reach-menu-button:1}</style><style type="text/css" data-fela-rehydration="575" data-fela-type="KEYFRAME">@-webkit-keyframes k1{0%{opacity:0;transform:translateY(-60px)}100%{opacity:1;transform:translateY(0px)}}@-moz-keyframes k1{0%{opacity:0;transform:translateY(-60px)}100%{opacity:1;transform:translateY(0px)}}@keyframes k1{0%{opacity:0;transform:translateY(-60px)}100%{opacity:1;transform:translateY(0px)}}@-webkit-keyframes k2{0%{opacity:1;transform:translateY(0px)}100%{opacity:0;transform:translateY(-60px)}}@-moz-keyframes k2{0%{opacity:1;transform:translateY(0px)}100%{opacity:0;transform:translateY(-60px)}}@keyframes k2{0%{opacity:1;transform:translateY(0px)}100%{opacity:0;transform:translateY(-60px)}}@-webkit-keyframes k3{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-moz-keyframes k3{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@keyframes k3{0%{transform:scale(1)}50%{transform:scale(1.1)}100%{transform:scale(1)}}@-webkit-keyframes k4{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@-moz-keyframes k4{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@keyframes k4{0%{opacity:0.8}50%{opacity:0.5}100%{opacity:0.8}}@-webkit-keyframes k5{0%{transform:matrix(0.97, 0, 0, 1, 0, 12);opacity:0}20%{transform:matrix(0.99, 0, 0, 1, 0, 2);opacity:0.7}40%{transform:matrix(1, 0, 0, 1, 0, -1);opacity:1}70%{transform:matrix(1, 0, 0, 1, 0, 0);opacity:1}100%{transform:matrix(1, 0, 0, 1, 0, 0);opacity:1}}@-moz-keyframes k5{0%{transform:matrix(0.97, 0, 0, 1, 0, 12);opacity:0}20%{transform:matrix(0.99, 0, 0, 1, 0, 2);opacity:0.7}40%{transform:matrix(1, 0, 0, 1, 0, -1);opacity:1}70%{transform:matrix(1, 0, 0, 1, 0, 0);opacity:1}100%{transform:matrix(1, 0, 0, 1, 0, 0);opacity:1}}@keyframes k5{0%{transform:matrix(0.97, 0, 0, 1, 0, 12);opacity:0}20%{transform:matrix(0.99, 0, 0, 1, 0, 2);opacity:0.7}40%{transform:matrix(1, 0, 0, 1, 0, -1);opacity:1}70%{transform:matrix(1, 0, 0, 1, 0, 0);opacity:1}100%{transform:matrix(1, 0, 0, 1, 0, 0);opacity:1}}</style><style type="text/css" data-fela-rehydration="575" data-fela-type="RULE">.a{font-family:medium-content-sans-serif-font, -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Oxygen, Ubuntu, Cantarell, "Open Sans", "Helvetica Neue", sans-serif}.b{font-weight:400}.c{background-color:rgba(255, 255, 255, 1)}.l{height:100vh}.m{width:100vw}.n{display:flex}.o{align-items:center}.p{justify-content:center}.q{height:25px}.r{fill:rgba(41, 41, 41, 1)}.s{display:block}.t{margin-bottom:36px}.v{width:100%}.z{flex:0 0 auto}.ab{justify-self:flex-end}.ac{z-index:500}.ae{visibility:visible}.af{overflow-x:scroll}.ag{white-space:nowrap}.ah{scrollbar-width:none}.ai{-ms-overflow-style:none}.aj::-webkit-scrollbar{display:none}.ak{box-shadow:inset 0 -1px 0 rgba(230, 230, 230, 1)}.al{min-height:184px}.ao{flex-direction:column}.ap{background-color:#355876}.aq{display:none}.as{border-bottom:none}.at{position:relative}.az{max-width:1192px}.ba{min-width:0}.bb{height:62px}.bc{flex-direction:row}.bd{flex:1 0 auto}.be{margin-left:0px}.bf{font-family:sohne, "Helvetica Neue", Helvetica, Arial, sans-serif}.bg{font-size:14px}.bh{line-height:20px}.bi{color:rgba(197, 210, 225, 1)}.bj{color:rgba(233, 241, 250, 1)}.bk{fill:rgba(233, 241, 250, 1)}.bl{font-size:inherit}.bm{border:inherit}.bn{font-family:inherit}.bo{letter-spacing:inherit}.bp{font-weight:inherit}.bq{padding:0}.br{margin:0}.bv:disabled{cursor:default}.bw:disabled{color:rgba(163, 208, 162, 0.5)}.bx:disabled{fill:rgba(163, 208, 162, 0.5)}.by{min-height:115px}.bz{justify-content:space-between}.cf{align-items:flex-start}.cg{margin-bottom:0px}.ch{margin-top:-32px}.ci{flex-wrap:wrap}.cl{margin-top:32px}.cm{margin-right:24px}.co{height:35px}.cp{width:112px}.cq{margin-right:12px}.cr{height:32px}.cs{overflow:visible}.ct{border-radius:1000px}.cu{background-color:rgba(53, 88, 118, 0.8)}.cv{fill:rgba(197, 210, 225, 1)}.cw{color:inherit}.cx{fill:rgba(117, 117, 117, 1)}.cy{padding:4px}.cz{margin-left:8px}.da{margin-right:10px}.db{display:inline-block}.dc{border:none}.dd{outline:none}.de{font:inherit}.df{font-size:16px}.dg{opacity:0}.dh{background-color:transparent}.di::placeholder{color:rgba(197, 210, 225, 1)}.dj{padding:0px}.dk{width:0px}.dl{transition:width 140ms ease-in, padding 140ms ease-in}.dm{fill:inherit}.dp:disabled{color:rgba(197, 210, 225, 1)}.dq:disabled{fill:rgba(197, 210, 225, 1)}.dr{padding:4px 12px 6px}.ds{background:0}.dt{border-color:rgba(215, 226, 238, 1)}.dv:disabled{cursor:inherit}.dw:disabled{opacity:0.3}.dx:disabled:hover{color:rgba(233, 241, 250, 1)}.dy:disabled:hover{fill:rgba(233, 241, 250, 1)}.dz:disabled:hover{border-color:rgba(215, 226, 238, 1)}.ea{border-radius:99em}.eb{border-width:1px}.ec{border-style:solid}.ed{box-sizing:border-box}.ee{text-decoration:none}.ef{fill:rgba(251, 255, 255, 1)}.eg{padding-top:1px}.eh{height:70px}.ej{line-height:24px}.ek:before{margin-bottom:-10px}.el:before{content:""}.em:before{display:table}.en:before{border-collapse:collapse}.eo:after{margin-top:-6px}.ep:after{content:""}.eq:after{display:table}.er:after{border-collapse:collapse}.es{color:rgba(117, 117, 117, 1)}.et{margin-right:32px}.eu{margin-bottom:-16px}.ev{margin-top:-14px}.ew{color:rgba(255, 255, 255, 1)}.ex{padding:7px 16px 9px}.ey{fill:rgba(255, 255, 255, 1)}.ez{background:rgba(102, 138, 170, 1)}.fa{border-color:rgba(102, 138, 170, 1)}.fd:disabled:hover{background:rgba(102, 138, 170, 1)}.fe:disabled:hover{border-color:rgba(102, 138, 170, 1)}.ff{display:inline-flex}.fi:disabled{color:rgba(117, 117, 117, 1)}.fj:disabled{fill:rgba(117, 117, 117, 1)}.fk{margin-left:12px}.fl{margin:0 12px}.fm{position:absolute}.fn{right:24px}.fo{margin:0px}.fp{border:0px}.fq{cursor:pointer}.fr{stroke:rgba(117, 117, 117, 1)}.fu{border-top:none}.fv{left:0}.fw{position:fixed}.fx{right:0}.fy{top:0}.fz{visibility:hidden}.gb{height:60px}.ge{color:rgba(102, 138, 170, 1)}.gf{fill:rgba(102, 138, 170, 1)}.gi{padding-left:24px}.gj{padding-right:24px}.gk{margin-left:auto}.gl{margin-right:auto}.gm{max-width:728px}.gn{background:rgba(255, 255, 255, 1)}.go{border:1px solid rgba(230, 230, 230, 1)}.gp{border-radius:4px}.gq{box-shadow:0 1px 4px rgba(230, 230, 230, 1)}.gr{max-height:100vh}.gs{overflow-y:auto}.gt{top:calc(100vh + 100px)}.gu{bottom:calc(100vh + 100px)}.gv{width:10px}.gw{pointer-events:none}.gx{word-break:break-word}.gy{word-wrap:break-word}.gz:after{display:block}.ha:after{clear:both}.hb{max-width:680px}.hc{max-width:1205px}.hi{clear:both}.hk{cursor:zoom-in}.hl{z-index:auto}.hn{max-width:100%}.ho{height:auto}.hp{line-height:1.23}.hq{letter-spacing:0}.hr{font-style:normal}.hs{font-weight:700}.in{margin-bottom:-0.27em}.io{color:rgba(41, 41, 41, 1)}.ip{line-height:1.394}.jf{margin-bottom:-0.42em}.jj{border-radius:50%}.jk{height:28px}.jl{width:28px}.jm{fill:rgba(61, 61, 61, 1)}.jn{margin-top:-2px}.jo{padding-left:4px}.jp{margin:0 4px}.jq{margin:0 7px}.jr{align-items:flex-end}.ka{padding-right:8px}.kb{margin-right:8px}.kc path{fill:rgba(41, 41, 41, 1)}.kd{margin-right:-4px}.kg{line-height:1.18}.kh{letter-spacing:-0.022em}.ki{font-weight:500}.kv{margin-bottom:-0.31em}.kw{line-height:1.58}.kx{letter-spacing:-0.004em}.ky{font-family:charter, Georgia, Cambria, "Times New Roman", Times, serif}.lr{margin-bottom:-0.46em}.ls{font-style:italic}.lt{max-width:1548px}.lz{transition:opacity 100ms 400ms}.ma{height:100%}.mb{overflow:hidden}.mc{will-change:transform}.md{transform:translateZ(0)}.me{margin:auto}.mf{background-color:rgba(242, 242, 242, 1)}.mg{padding-bottom:48.857142857142854%}.mh{height:0}.mi{filter:blur(20px)}.mj{transform:scale(1.1)}.mp{text-decoration:underline}.mq{box-shadow:inset 0 0 0 1px rgba(230, 230, 230, 1)}.mr{padding:16px 20px}.ms{flex:1 1 auto}.mu{max-height:40px}.mv{text-overflow:ellipsis}.mw{display:-webkit-box}.mx{-webkit-line-clamp:2}.my{-webkit-box-orient:vertical}.na{margin-top:8px}.nb{margin-top:12px}.nc{font-size:13px}.nd{width:160px}.ne{background-image:url(https://miro.medium.com/max/320/0*720BmLEupzuQgjj7)}.nf{background-origin:border-box}.ng{background-size:cover}.nh{height:167px}.ni{background-position:50% 50%}.nj{background-image:url(https://miro.medium.com/max/320/0*PXqNW2Dynndqsmt1)}.nk{list-style-type:disc}.nl{margin-left:30px}.nm{padding-left:0px}.ns{margin-bottom:14px}.nt{padding-top:24px}.nu{padding-bottom:10px}.nv{background-color:rgba(8, 8, 8, 1)}.nw{height:3px}.nx{width:3px}.ny{margin-right:20px}.nz{padding:20px}.oa{background:rgba(242, 242, 242, 1)}.ob{overflow-x:auto}.oc{font-family:Menlo, Monaco, "Courier New", Courier, monospace}.od{margin-top:-0.09em}.oe{margin-bottom:-0.09em}.of{white-space:pre-wrap}.ol{max-width:2145px}.om{padding-bottom:15.285714285714285%}.on{max-width:1532px}.oo{padding-bottom:35.14285714285714%}.op{max-width:1067px}.oq{padding-bottom:43.714285714285715%}.or{max-width:1095px}.os{padding-bottom:20.857142857142858%}.ot{max-width:1494px}.ou{padding-bottom:22.42857142857143%}.ov{max-width:1601px}.ow{padding-bottom:18.57142857142857%}.ox{margin-top:10px}.oy{text-align:center}.pb{font-style:inherit}.pc{max-width:1427px}.pd{padding-bottom:35%}.pe{list-style-type:decimal}.pf{max-width:1479px}.pg{padding-bottom:21.428571428571427%}.ph{max-width:1430px}.pi{padding-bottom:35.285714285714285%}.pj{max-width:1437px}.pk{padding-bottom:93.57142857142857%}.pl{max-width:607px}.pm{padding-bottom:7.248764415156508%}.pn{max-width:1947px}.po{padding-bottom:13.857142857142858%}.pp{max-width:1012px}.pq{padding-bottom:39.714285714285715%}.pr{max-width:120px}.ps{padding-bottom:26.666666666666668%}.pt{max-width:1453px}.pu{padding-bottom:33.285714285714285%}.pv{max-width:1333px}.pw{padding-bottom:11.142857142857142%}.px{max-width:1432px}.py{padding-bottom:33.42857142857143%}.pz{max-width:490px}.qa{padding-bottom:10.612244897959183%}.qb{max-width:1627px}.qc{padding-bottom:59.57142857142857%}.qd{max-width:1241px}.qe{padding-bottom:61%}.qf{max-width:1514px}.qg{padding-bottom:23.714285714285715%}.qh{padding-bottom:96.57142857142857%}.qi{max-width:594px}.qj{padding-bottom:6.397306397306397%}.qk{max-width:867px}.ql{padding-bottom:48.714285714285715%}.qm{max-width:1039px}.qn{padding-bottom:18.42857142857143%}.qo{max-width:1037px}.qp{padding-bottom:17.42857142857143%}.qq{max-width:1611px}.qr{padding-bottom:32.857142857142854%}.qs{max-width:1921px}.qt{padding-bottom:25.57142857142857%}.qu{max-width:1552px}.qv{padding-bottom:50.57142857142857%}.qw{padding-bottom:41.42857142857143%}.qx{max-width:1965px}.qy{padding-bottom:6%}.qz{max-width:1464px}.ra{padding-bottom:93%}.rb{box-shadow:inset 3px 0 0 0 rgba(41, 41, 41, 1)}.rc{padding-left:23px}.rd{margin-left:-20px}.re{background-image:url(https://miro.medium.com/max/320/1*pZVocDqN8uRdnGyrrWbnlQ.png)}.rf{background-image:url(https://miro.medium.com/max/320/1*t5Cte8JzO1x4btdQ7VS0iA.png)}.rg{background-image:url(https://miro.medium.com/max/320/1*uo1-f6SPeaXqAvy-viO39w.png)}.rh{will-change:opacity}.ri{width:188px}.rj{left:50%}.rk{transform:translateX(406px)}.rl{top:calc(65px + 54px + 14px)}.ro{will-change:opacity, transform}.rp{transform:translateY(159px)}.rr{width:197px}.rs{margin-bottom:20px}.rt{padding-bottom:5px}.ru{padding-top:2px}.rv{padding-top:20px}.rw{color:rgba(242, 242, 242, 1)}.rx{fill:rgba(242, 242, 242, 1)}.ry{border-color:rgba(242, 242, 242, 1)}.se{padding-top:32px}.sf{border-top:1px solid rgba(230, 230, 230, 1)}.sg{justify-content:space-evenly}.sm{outline:0}.sn{border:0}.so{user-select:none}.sp> svg{pointer-events:none}.sr{-webkit-user-select:none}.tb button{text-align:left}.tc{margin-top:2px}.td{opacity:1}.te{padding-left:6px}.tf{margin-top:1px}.tg{margin-top:40px}.th{padding-bottom:25px}.ti{margin-top:25px}.tj{max-width:155px}.tq{top:1px}.tt{margin-left:24px}.tu{margin-top:4px}.tv{margin-left:4px}.tw{margin-top:5px}.tx{padding-bottom:40px}.ty{list-style-type:none}.tz{margin-bottom:8px}.ua{line-height:22px}.ub{border-radius:3px}.uc{padding:5px 10px}.ud{padding-bottom:4px}.ue{background-color:rgba(250, 250, 250, 1)}.uu{-webkit-line-clamp:1}.uv{padding-top:5px}.uw{padding-right:168px}.ux{padding-top:25px}.vd{margin-bottom:96px}.bs:hover{cursor:pointer}.bt:hover{color:rgba(251, 255, 255, 1)}.bu:hover{fill:rgba(251, 255, 255, 1)}.dn:hover{color:rgba(242, 248, 253, 1)}.do:hover{fill:rgba(242, 248, 253, 1)}.du:hover{border-color:rgba(251, 255, 255, 1)}.fb:hover{background:rgba(90, 118, 144, 1)}.fc:hover{border-color:rgba(90, 118, 144, 1)}.fg:hover{color:rgba(25, 25, 25, 1)}.fh:hover{fill:rgba(25, 25, 25, 1)}.gg:hover{color:rgba(90, 118, 144, 1)}.gh:hover{fill:rgba(90, 118, 144, 1)}.ke:hover{fill:rgba(8, 8, 8, 1)}.rz:hover{background:rgba(242, 242, 242, 1)}.sa:hover{border-color:rgba(242, 242, 242, 1)}.sb:hover{cursor:wait}.sc:hover{color:rgba(242, 242, 242, 1)}.sd:hover{fill:rgba(242, 242, 242, 1)}.st:hover{fill:rgba(117, 117, 117, 1)}.hm:focus{transform:scale(1.01)}.kf:focus{fill:rgba(8, 8, 8, 1)}.ss:focus{fill:rgba(117, 117, 117, 1)}.sq:active{border-style:none}</style><style type="text/css" data-fela-rehydration="575" data-fela-type="RULE" media="all and (min-width: 1080px)">.d{display:none}.w{display:flex}.ay{margin:0 64px}.hh{margin-top:32px}.ij{font-size:46px}.ik{margin-top:0.6em}.il{line-height:56px}.im{letter-spacing:-0.011em}.jc{font-size:22px}.jd{margin-top:0.92em}.je{line-height:28px}.jy{margin-left:30px}.kt{margin-top:1.72em}.ku{letter-spacing:0}.ln{font-size:21px}.lo{margin-top:0.86em}.lp{line-height:32px}.lq{letter-spacing:-0.003em}.ly{margin-top:56px}.mo{margin-top:2em}.nr{margin-top:1.05em}.ok{margin-top:1.91em}.sl{margin-right:5px}.ta{margin-top:0px}.tp{margin-top:5px}.ts{display:inline-block}.ur{font-size:20px}.us{line-height:24px}.ut{max-height:24px}.vc{margin:0}</style><style type="text/css" data-fela-rehydration="575" data-fela-type="RULE" media="all and (max-width: 1079.98px)">.e{display:none}.jx{margin-left:30px}.oz{margin-left:auto}.pa{text-align:center}.sk{margin-right:5px}.sz{margin-top:0px}.to{margin-top:5px}.tr{display:inline-block}</style><style type="text/css" data-fela-rehydration="575" data-fela-type="RULE" media="all and (max-width: 903.98px)">.f{display:none}.jw{margin-left:30px}.sj{margin-right:5px}.sy{margin-top:0px}.tm{display:inline-block}.tn{margin-top:5px}</style><style type="text/css" data-fela-rehydration="575" data-fela-type="RULE" media="all and (max-width: 727.98px)">.g{display:none}.u{margin-bottom:20px}.am{box-shadow:inset 0 -1px 0 rgba(230, 230, 230, 1)}.an{min-height:230px}.ar{display:block}.ca{min-height:98px}.cb{display:flex}.cc{align-items:flex-start}.cd{flex-direction:column}.ce{justify-content:flex-end}.cj{margin-bottom:28px}.ck{margin-top:0px}.cn{margin-top:28px}.ei{margin:0}.fs{border-top:1px solid rgba(230, 230, 230, 1)}.ft{border-bottom:1px solid rgba(230, 230, 230, 1)}.gc{align-items:center}.gd{flex:1 0 auto}.jh{margin-top:32px}.ji{flex-direction:column-reverse}.ju{margin-bottom:30px}.jv{margin-left:0px}.mt{padding:10px 12px 10px}.si{margin-left:8px}.sw{margin-top:2px}.sx{margin-right:16px}.tl{display:inline-block}</style><style type="text/css" data-fela-rehydration="575" data-fela-type="RULE" media="all and (max-width: 551.98px)">.h{display:none}.au{margin:0 24px}.hd{margin-top:24px}.ht{font-size:32px}.hu{margin-top:0.64em}.hv{line-height:40px}.hw{letter-spacing:-0.016em}.iq{font-size:18px}.ir{margin-top:0.79em}.is{line-height:24px}.jg{margin-top:32px}.js{margin-bottom:30px}.jt{margin-left:0px}.kj{font-size:20px}.kk{margin-top:1.23em}.kl{letter-spacing:0}.kz{margin-top:0.67em}.la{line-height:28px}.lb{letter-spacing:-0.003em}.lu{margin-top:40px}.mk{margin-top:1.56em}.nn{margin-top:1.34em}.og{margin-top:1.41em}.sh{margin-left:8px}.su{margin-top:2px}.sv{margin-right:16px}.tk{display:inline-block}.uf{font-size:16px}.ug{line-height:20px}.uh{max-height:20px}.uy{margin:0}</style><style type="text/css" data-fela-rehydration="575" data-fela-type="RULE" media="all and (min-width: 904px) and (max-width: 1079.98px)">.i{display:none}.x{display:flex}.ax{margin:0 64px}.hg{margin-top:32px}.if{font-size:46px}.ig{margin-top:0.6em}.ih{line-height:56px}.ii{letter-spacing:-0.011em}.iz{font-size:22px}.ja{margin-top:0.92em}.jb{line-height:28px}.kr{margin-top:1.72em}.ks{letter-spacing:0}.lj{font-size:21px}.lk{margin-top:0.86em}.ll{line-height:32px}.lm{letter-spacing:-0.003em}.lx{margin-top:56px}.mn{margin-top:2em}.nq{margin-top:1.05em}.oj{margin-top:1.91em}.uo{font-size:20px}.up{line-height:24px}.uq{max-height:24px}.vb{margin:0}</style><style type="text/css" data-fela-rehydration="575" data-fela-type="RULE" media="all and (min-width: 728px) and (max-width: 903.98px)">.j{display:none}.y{display:flex}.aw{margin:0 48px}.hf{margin-top:32px}.ib{font-size:46px}.ic{margin-top:0.6em}.id{line-height:56px}.ie{letter-spacing:-0.011em}.iw{font-size:22px}.ix{margin-top:0.92em}.iy{line-height:28px}.kp{margin-top:1.72em}.kq{letter-spacing:0}.lf{font-size:21px}.lg{margin-top:0.86em}.lh{line-height:32px}.li{letter-spacing:-0.003em}.lw{margin-top:56px}.mm{margin-top:2em}.np{margin-top:1.05em}.oi{margin-top:1.91em}.ul{font-size:20px}.um{line-height:24px}.un{max-height:24px}.va{margin:0}</style><style type="text/css" data-fela-rehydration="575" data-fela-type="RULE" media="all and (min-width: 552px) and (max-width: 727.98px)">.k{display:none}.av{margin:0 24px}.he{margin-top:24px}.hx{font-size:32px}.hy{margin-top:0.64em}.hz{line-height:40px}.ia{letter-spacing:-0.016em}.it{font-size:18px}.iu{margin-top:0.79em}.iv{line-height:24px}.km{font-size:20px}.kn{margin-top:1.23em}.ko{letter-spacing:0}.lc{margin-top:0.67em}.ld{line-height:28px}.le{letter-spacing:-0.003em}.lv{margin-top:40px}.ml{margin-top:1.56em}.no{margin-top:1.34em}.oh{margin-top:1.41em}.ui{font-size:16px}.uj{line-height:20px}.uk{max-height:20px}.uz{margin:0}</style><style type="text/css" data-fela-rehydration="575" data-fela-type="RULE" media="print">.jz{display:none}</style><style type="text/css" data-fela-rehydration="575" data-fela-type="RULE" media="(prefers-reduced-motion: no-preference)">.ga{animation:k2 .2s ease-in-out both}.hj{transition:transform 300ms cubic-bezier(0.2, 0, 0.2, 1)}.rm{transition:opacity 200ms}</style><style type="text/css" data-fela-rehydration="575" data-fela-type="RULE" media="(orientation: landscape) and (max-width: 903.98px)">.mz{max-height:none}</style><style type="text/css" data-fela-rehydration="575" data-fela-type="RULE" media="all and (max-width: 1230px)">.rn{display:none}</style><style type="text/css" data-fela-rehydration="575" data-fela-type="RULE" media="all and (max-width: 1240px)">.rq{display:none}</style><script data-rh="true">(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-24232453-2', 'auto');
ga('send', 'pageview');</script><script type="text/javascript" data-rh="true">(function(b,r,a,n,c,h,_,s,d,k){if(!b[n]||!b[n]._q){for(;s<_.length;)c(h,_[s++]);d=r.createElement(a);d.async=1;d.src="https://cdn.branch.io/branch-latest.min.js";k=r.getElementsByTagName(a)[0];k.parentNode.insertBefore(d,k);b[n]=h}})(window,document,"script","branch",function(b,r){b[r]=function(){b._q.push([r,arguments])}},{_q:[],_v:1},"addListener applyCode autoAppIndex banner closeBanner closeJourney creditHistory credits data deepview deepviewCta first getCode init link logout redeem referrals removeListener sendSMS setBranchViewData setIdentity track validateCode trackCommerceEvent logEvent".split(" "), 0);
branch.init('key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm', {metadata: {}, 'no_journeys': true, 'disable_exit_animation': true, 'disable_entry_animation': true, 'tracking_disabled': null}, function(err, data) {});</script></head><body><div id="root"><div class="a b c"><div class="d e f g h i j k"></div><script>document.domain = document.domain;</script><div><script>if (window.self !== window.top) window.location = "about:blank"</script></div><div class="s"><div class="t s u"><div class="ak al s am an"><div class="n ao ap"><div class="aq ar"><div class="as s at ac"><div class="n p"><div class="au av aw ax ay az ba v"><div class="bb n o"><div class="n o bc bd"><div class="ag"><div class="ae" id="li-ShowPostUnderCollection-navbar-open-in-app-button"><div class="be aq ar"><span class="bf b bg bh bi"><a href="https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F41ff868d1794&amp;~feature=LiOpenInAppButton&amp;~channel=ShowPostUnderCollection&amp;~stage=mobileNavBar&amp;source=post_page-----41ff868d1794--------------------------------" class="bj bk bl bm bn bo bp bq br bs bt bu bv vf vg" rel="noopener nofollow">Open in app</a></span></div></div></div></div><a href="https://medium.com/?source=post_page-----41ff868d1794--------------------------------" rel="noopener" aria-label="Homepage"><svg viewBox="0 0 1043.63 592.71" class="q bk"><g data-name="Layer 2"><g data-name="Layer 1"><path d="M588.67 296.36c0 163.67-131.78 296.35-294.33 296.35S0 460 0 296.36 131.78 0 294.34 0s294.33 132.69 294.33 296.36M911.56 296.36c0 154.06-65.89 279-147.17 279s-147.17-124.94-147.17-279 65.88-279 147.16-279 147.17 124.9 147.17 279M1043.63 296.36c0 138-23.17 249.94-51.76 249.94s-51.75-111.91-51.75-249.94 23.17-249.94 51.75-249.94 51.76 111.9 51.76 249.94"></path></g></g></svg></a></div></div></div></div></div><div class="n p"><div class="au av aw ax ay az ba v"><div class="by n o bc bz ca cb cc cd ce"><div class="v n cf bz"><div class="n v"><div class="cg ch v n o bc ci cj ck cb cc cd"><div class="cl cm s cn"><a rel="noopener" aria-label="Publication Homepage" href="https://towardsdatascience.com/?source=post_page-----41ff868d1794--------------------------------"><div class="co cp s"><img alt="Towards Data Science" class="" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_AGyTPCaRzVqL77kFwUwHKg.png" width="112" height="35"></div></a></div></div></div><div class="w x y k h z ab o ac ae"><div class="cq cr cs n o"><div class="ct cu"><div class="n" aria-hidden="false" aria-describedby="publisherMenu" aria-labelledby="publisherMenu"><button aria-controls="publisherMenu" aria-expanded="false" aria-label="Publisher Menu" class="cw cx bl bm bn bo bp bq br bs"><div class="cy s"><svg class="cv bu" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></div></button></div></div><div class="cz da s"><div class="ct cu"><div class="db" aria-hidden="false" aria-describedby="searchResults" aria-labelledby="searchResults"><div class="n"><button class="cw dm bl bm bn bo bp bq br bs dn do bv dp dq" aria-label="Search"><span class="cy s"><svg width="25" height="25" viewBox="0 0 25 25" class="cv"><path d="M20.07 18.93l-4.16-4.15a6 6 0 1 0-.88.88l4.15 4.16a.62.62 0 1 0 .89-.89zM6.5 11a4.75 4.75 0 1 1 9.5 0 4.75 4.75 0 0 1-9.5 0z"></path></svg></span></button><input role="combobox" aria-controls="searchResults" aria-expanded="false" aria-hidden="true" aria-label="search" tabindex="-1" class="dc dd de df bh dg dh bj di at dj dk dl" placeholder="Search" value=""></div></div></div></div><div class="ae" id="li-post-page-navbar-upsell-button"><div class="cq s g"><div><a href="https://medium.com/plans?source=upgrade_membership---nav_full----------------------------------" class="bf b bg bh bj dr bk ds dt bt bu du bs dv dw dx dy dz ea eb ec ed db ee" rel="noopener">Upgrade</a></div></div></div></div><a href="https://medium.com/?source=post_page-----41ff868d1794--------------------------------" rel="noopener" aria-label="Homepage"><svg viewBox="0 0 1043.63 592.71" class="q ef"><g data-name="Layer 2"><g data-name="Layer 1"><path d="M588.67 296.36c0 163.67-131.78 296.35-294.33 296.35S0 460 0 296.36 131.78 0 294.34 0s294.33 132.69 294.33 296.36M911.56 296.36c0 154.06-65.89 279-147.17 279s-147.17-124.94-147.17-279 65.88-279 147.16-279 147.17 124.9 147.17 279M1043.63 296.36c0 138-23.17 249.94-51.76 249.94s-51.75-111.91-51.75-249.94 23.17-249.94 51.75-249.94 51.76 111.9 51.76 249.94"></path></g></g></svg></a></div></div></div></div></div></div><div class="s"><div class="n p"><div class="au av aw ax ay az ba v"><div class="af ag ah ai aj"><div class="eg eh n o"><div class="s ei"><span class="bf b df ej ek el em en eo ep eq er es"><div class="n o"><div class="et s"><div class="eu ev s"><div class="db" aria-hidden="false" aria-describedby="collectionFollowPopover" aria-labelledby="collectionFollowPopover"><button class="bf b bg bh ew ex ey ez fa fb fc bs dv dw fd fe ea eb ec ed db ee" aria-controls="collectionFollowPopover" aria-expanded="false"><div class="n bc">Follow</div></button></div></div></div><div class="cq ff ao"><a class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" rel="noopener" href="https://towardsdatascience.com/followers?source=post_page-----41ff868d1794--------------------------------">566K Followers</a></div><div class="fk s g">·</div><div class="fk s g"><nav class="n o"><span class="fl n ao"><a class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" rel="noopener" href="https://towardsdatascience.com/tagged/editors-pick?source=post_page-----41ff868d1794--------------------------------">Editors' Picks</a></span><span class="fl n ao"><a class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" rel="noopener" href="https://towardsdatascience.com/tagged/tds-features?source=post_page-----41ff868d1794--------------------------------">Features</a></span><span class="fl n ao"><a class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" rel="noopener" href="https://towardsdatascience.com/tagged/deep-dives?source=post_page-----41ff868d1794--------------------------------">Deep Dives</a></span><span class="fl n ao"><a class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" rel="noopener" href="https://towardsdatascience.com/how-to-get-the-most-out-of-towards-data-science-3bf37f75a345?source=post_page-----41ff868d1794--------------------------------">Grow</a></span><span class="fl n ao"><a class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" rel="noopener" href="https://towardsdatascience.com/questions-96667b06af5?source=post_page-----41ff868d1794--------------------------------">Contribute</a></span></nav></div><div class="fk n ao g"><a class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" rel="noopener" href="https://towardsdatascience.com/about?source=post_page-----41ff868d1794--------------------------------">About</a></div></div></span></div><div class="aq fm fn ar"><button class="n o p fo fp dj fq" aria-label="Expand navbar"><svg width="14" height="14" class="fr"><path d="M0 .5h14M0 7h14M0 13.5h14"></path></svg></button></div></div></div></div></div></div></div><div class="fs ft fu as c fv td fw fx fy ae ac ws"><div class="n p"><div class="au av aw ax ay az ba v"><div class="gb v aq fy ac cb gc"><div class="aq cb gc gd"><div class="ae" id="li-ShowPostUnderCollection-navbar-open-in-app-button"><div class="be aq ar"><span class="bf b bg bh es"><a href="https://rsci.app.link/?%24canonical_url=https%3A%2F%2Fmedium.com%2Fp%2F41ff868d1794&amp;~feature=LiOpenInAppButton&amp;~channel=ShowPostUnderCollection&amp;~stage=mobileNavBar&amp;source=post_page-----41ff868d1794--------------------------------" class="ge gf bl bm bn bo bp bq br bs gg gh bv vf vg" rel="noopener nofollow">Open in app</a></span></div></div></div><a href="https://medium.com/?source=post_page-----41ff868d1794--------------------------------" rel="noopener" aria-label="Homepage"><svg viewBox="0 0 1043.63 592.71" class="q r"><g data-name="Layer 2"><g data-name="Layer 1"><path d="M588.67 296.36c0 163.67-131.78 296.35-294.33 296.35S0 460 0 296.36 131.78 0 294.34 0s294.33 132.69 294.33 296.36M911.56 296.36c0 154.06-65.89 279-147.17 279s-147.17-124.94-147.17-279 65.88-279 147.16-279 147.17 124.9 147.17 279M1043.63 296.36c0 138-23.17 249.94-51.76 249.94s-51.75-111.91-51.75-249.94 23.17-249.94 51.75-249.94 51.76 111.9 51.76 249.94"></path></g></g></svg></a></div></div></div></div></div><div class="s jz"><div class="yf yg v ma fw yh yi fq dg gw yj" aria-hidden="true" role="presentation"></div><div class="yk fw yl ym yn yf ma ed yo yp yq td yr ys yt xl yu yv yw yx fz" aria-hidden="true"><div class="yy yz n o bc bz"><div class="n bc"><h2 class="bf ki za ej hq io">Responses (17)</h2></div><div class="n bc"><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="72" aria-labelledby="72"><a href="https://policy.medium.com/medium-rules-30e5502c4eb4?source=responses-----41ff868d1794--------------------------------" class="jm fh" target="_blank" rel="noopener"><svg width="25" height="25" viewBox="0 0 25 25"><path fill-rule="evenodd" clip-rule="evenodd" d="M11.99 5.04c.26-.21.64-.22.91-.01.97.72 1.77 1.21 2.6 1.54.83.32 1.72.48 2.89.5.41.01.74.35.74.76-.02 3.62-.43 6.26-1.45 8.21-1.03 1.98-2.66 3.21-4.97 4.08a.75.75 0 0 1-.53 0c-2.25-.87-3.86-2.1-4.9-4.07-1.02-1.95-1.46-4.59-1.48-8.22 0-.41.33-.75.75-.76 1.19-.02 2.1-.18 2.92-.5.82-.32 1.6-.81 2.52-1.53zm.46.9c-.9.69-1.71 1.21-2.62 1.56a8.9 8.9 0 0 1-3.02.57c.03 3.45.46 5.82 1.36 7.51.88 1.69 2.25 2.77 4.28 3.57 2.1-.8 3.47-1.89 4.34-3.57.89-1.7 1.3-4.07 1.34-7.51a8.8 8.8 0 0 1-3-.57 11.8 11.8 0 0 1-2.68-1.56zm0 9.15a2.67 2.67 0 1 0 0-5.34 2.67 2.67 0 0 0 0 5.34zm0 1a3.67 3.67 0 1 0 0-7.34 3.67 3.67 0 0 0 0 7.34zm-1.82-3.77l.53-.53.91.92 1.63-1.63.52.53-2.15 2.15-1.44-1.44z"></path></svg></a></div></div><div class="s at zb"><div class="s at fy fx"><button class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" data-testid="close-button" aria-label="close"><svg width="25" height="25" viewBox="0 0 25 25" class="cx"><path d="M18.13 6.11l-5.61 5.61-5.6-5.61-.81.8 5.61 5.61-5.61 5.61.8.8 5.61-5.6 5.61 5.6.8-.8-5.6-5.6 5.6-5.62"></path></svg></button></div></div></div></div><div><div class="bf b bg bh io"><div class="ej"><div class="ze rs s"><div class="ym abe gp n ao abf abg abh"><div class="n o bz at zz abi dg abc abj"><div class="n o"><img alt="Assan Sanogo" class="s jj cr zh" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/0_7GZZawY-4v3DYqIS" width="32" height="32"><div class="fk n cf ao p"><p class="bf b bg bh io">Assan Sanogo</p></div></div></div><div class="n ao"><div class="abk abl"><div class="abo s"><div data-gramm="false" role="textbox" data-slate-editor="true" data-slate-node="value" contenteditable="true" style="outline: none; white-space: pre-wrap; overflow-wrap: break-word;"><div data-slate-node="element"><p><span data-slate-node="text"><span data-slate-leaf="true"><span contenteditable="false" style="pointer-events: none; display: inline-block; width: 0px; max-width: 100%; white-space: nowrap; opacity: 0.333; user-select: none; font-style: normal; font-weight: normal; text-decoration: none;">What are your thoughts?</span><span data-slate-zero-width="n" data-slate-length="0">﻿<br></span></span></span></p></div></div></div></div><div class="bz abm es n abb dg abc"><span class="bf b bg bh es"><div class="n"><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="86" aria-labelledby="86"><span class="jp abp abq abr gp ff p abs abt"><svg width="21" height="21"><path d="M10.3 18H4.4l.1-.9.8-.12c.55-.11.78-.23.78-.45V5.37c0-.22-.11-.34-.9-.45H4.5l-.11-.9h6.25c4.02 0 5.58 1.24 5.58 3.14 0 1.9-1.78 3.12-3.79 3.46v.11c2.7.34 4.25 1.56 4.25 3.57 0 2.35-2 3.7-6.37 3.7h.02-.02zM9.98 5.02h-1v5.47h1.23c1.79 0 2.79-1.23 2.79-2.68 0-1.69-1-2.8-3-2.8v.01zm-.22 6.36h-.78V17l1.22.22h.22c1.67 0 3.01-1 3.01-2.8 0-2.11-1.56-3-3.69-3h.02z" fill-rule="evenodd"></path></svg></span></div></div><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="87" aria-labelledby="87"><span class="jp abp abq abr gp ff p abs abt"><svg width="21" height="21"><path d="M9.85 18.04c-.54 0-2.03-.64-1.92-.85L9.95 9.5l-.64-.22-1.38 1.5-.43-.43c.53-1.17 1.7-2.67 2.77-2.67.54 0 2.24.54 2.14.86l-2.14 7.78.54.22 1.6-1.07.42.43c-.64 1.06-1.92 2.13-2.98 2.13zm2.34-11.73c-.96 0-1.38-.64-1.38-1.39 0-1.07.74-1.92 1.49-1.92.85 0 1.39.64 1.39 1.5-.11 1.06-.75 1.8-1.5 1.8z" fill-rule="evenodd"></path></svg></span></div></div></div></span><div class="zz aba n abb dg abc"><div class="abd"><button class="bf b bg bh io dr r ds abu abv ke abw bs dv dw abx aby abz ea eb ec ed db ee">Cancel</button></div><button class="bf b bg bh ew dr ey aca acb acc acd bs dv dw ace acf ea eb ec ed db ee" disabled="">Respond</button></div></div></div></div><div class="abn n abb dg abc"><span role="checkbox" aria-checked="false" tabindex="0"><label class="n o"><div class="kb acw acx n acy at"><input class="acg ach aci fm dg acj ack acl acm acn aco acp" type="checkbox" disabled=""><span class="o oa acq eb ec acr ed ew acs n ey act p acu"><svg width="11" height="11" viewBox="0 0 11 11" class="ey acv"><path d="M0 6.31l3.7 3.7.9.91.67-1.1 5.3-8.79L8.84 0l-5.3 8.8 1.57-.2-3.7-3.7L0 6.3z"></path></svg></span></div><p class="bf b nc bh es">Also publish to my profile</p></label></span></div></div></div></div></div><div class="zc oy n ao p o ls zd"><p class="bf b df ej es">There are currently no responses for this story.</p><p class="bf b df ej es">Be the first to respond.</p></div></div></div><article class="meteredContent"><section class="gi gj gk gl v gm ed s"><div><div class="ae" id="li-highlight-meter-2-highlight-box"><div class="vi gp vj at vk"><div class="vl vm n bz vn" style="display: block;"><p class="bf b df ej io"><span class="ae" id="li-highlight-meter-2-copy"><div id="responsive" style="display: flex; justify-content: space-between; gap: 1.5rem; margin-bottom: 8px;"><div style="height: 4px; background-color: rgb(124, 124, 124); width: 100%;"></div><div style="height: 4px; background-color: rgb(124, 124, 124); width: 100%;"></div><div style="height: 4px; width: 100%; background-color: rgb(222, 222, 222);"></div></div>You have <b>1</b> free member-only story left this month. </span><div class="vo ag"><span class="ae" id="li-highlight-meter-2-link"><span><a href="https://medium.com/plans?source=upgrade_membership---post_counter--41ff868d1794--------------------------------" class="cw dm bl bm bn bo bp bq br bs bv fi fj mp" rel="noopener">Upgrade for unlimited access.</a></span></span></div></p></div></div></div></div></section><span class="s"></span><div><div class="fm fv wo gu gv gw"></div><section class="gx gy gz ep ha"><div class="n p"><div class="au av aw ax ay hb ba v"><figure class="hd he hf hg hh hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl hc"><img alt="" class="v hn ho" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_LgqxDMP5qD1HE_uM33zZrg.png" width="700" height="77" role="presentation"></div></div></figure><div class=""><h1 id="7d8a" class="hp hq hr bf hs ht hu hv hw hx hy hz ia ib ic id ie if ig ih ii ij ik il im in io">Text Classification with NLP: Tf-Idf vs Word2Vec vs BERT</h1></div><div class=""><h2 id="bd0f" class="ip hq hr bf b iq ir is it iu iv iw ix iy iz ja jb jc jd je jf es">Preprocessing, Model Design, Evaluation, Explainability for Bag-of-Words, Word Embedding, Language models</h2><div class="cl"><div class="n bz jg jh ji"><div class="o n"><div><a href="https://mdipietro09.medium.com/?source=post_page-----41ff868d1794--------------------------------" rel="noopener"><img alt="Mauro Di Pietro" class="s jj jk jl" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/2_vdOG1aNO6cMj0IYIQuKxDQ.jpeg" width="28" height="28"></a></div><div class="cz v n ci"><div class="n"><div style="flex:1"><span class="bf b bg bh io"><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="5" aria-labelledby="5"><a href="https://mdipietro09.medium.com/?source=post_page-----41ff868d1794--------------------------------" class="" rel="noopener"><p class="bf b bg bh ge">Mauro Di Pietro</p></a></div></div></span></div></div><span class="bf b bg bh es"><a class="" rel="noopener" href="https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794?source=post_page-----41ff868d1794--------------------------------"><p class="bf b bg bh es"><span class="jp"></span>Jul 18, 2020<span class="jq">·</span>22 min read<svg class="jm jn jo" width="15" height="15" viewBox="0 0 15 15" aria-label="Member only content"><path d="M7.44 2.32c.03-.1.09-.1.12 0l1.2 3.53a.29.29 0 0 0 .26.2h3.88c.11 0 .13.04.04.1L9.8 8.33a.27.27 0 0 0-.1.29l1.2 3.53c.03.1-.01.13-.1.07l-3.14-2.18a.3.3 0 0 0-.32 0L4.2 12.22c-.1.06-.14.03-.1-.07l1.2-3.53a.27.27 0 0 0-.1-.3L2.06 6.16c-.1-.06-.07-.12.03-.12h3.89a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></p></a></span></div></div><div class="n jr js jt ju jv jw jx jy jz"><div class="n o"><div class="ka s"><div class="db" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="1" aria-labelledby="1"><button class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post"><svg width="25" height="25" class="r"><g fill-rule="evenodd"><path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01"></path><path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2"></path></g></svg></button></div></div></div></div><div class="kb s"><div class="db" aria-hidden="true"><div class="db v" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="cw cx bl bm bn bo bp bq br bs"><svg width="25" height="25" viewBox="0 0 25 25" fill="none" class="kc"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="#292929"></path></svg></button></div></div></div><div class="s bd"><div class="db" aria-hidden="false" aria-describedby="creatorActionOverflowMenu" aria-labelledby="creatorActionOverflowMenu"><div class="db" aria-hidden="false" aria-describedby="removeFromPublicationPopover" aria-labelledby="removeFromPublicationPopover"><div class="kd s z"><button class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" aria-controls="creatorActionOverflowMenu" aria-expanded="false" aria-label="More options"><svg class="r ke kf" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div><h2 id="8f03" class="kg kh hr bf ki kj kk is kl km kn iv ko iw kp iy kq iz kr jb ks jc kt je ku kv io" data-selectable-paragraph="">Summary</h2><p id="84c0" class="kw kx hr ky b iq kz la lb it lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr gx io" data-selectable-paragraph="">In this article, using NLP and Python, I will explain 3 different strategies for text multiclass classification: the old-fashioned <em class="ls">Bag-of-Words </em>(with Tf-Idf )<em class="ls">, </em>the famous <em class="ls">Word Embedding (</em>with Word2Vec), and the cutting edge<em class="ls"> Language models</em> (with BERT).</p><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl lt"><div class="me s at mf"><div class="mg mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_T8WWibd7u8b7gfgeG0LgAA.gif" width="700" height="342" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="700" height="342" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_T8WWibd7u8b7gfgeG0LgAA(1).gif" srcset="https://miro.medium.com/max/552/1*T8WWibd7u8b7gfgeG0LgAA.gif 276w, https://miro.medium.com/max/1104/1*T8WWibd7u8b7gfgeG0LgAA.gif 552w, https://miro.medium.com/max/1280/1*T8WWibd7u8b7gfgeG0LgAA.gif 640w, https://miro.medium.com/max/1400/1*T8WWibd7u8b7gfgeG0LgAA.gif 700w" sizes="700px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1400/1*T8WWibd7u8b7gfgeG0LgAA.gif" width="700" height="342" srcSet="https://miro.medium.com/max/552/1*T8WWibd7u8b7gfgeG0LgAA.gif 276w, https://miro.medium.com/max/1104/1*T8WWibd7u8b7gfgeG0LgAA.gif 552w, https://miro.medium.com/max/1280/1*T8WWibd7u8b7gfgeG0LgAA.gif 640w, https://miro.medium.com/max/1400/1*T8WWibd7u8b7gfgeG0LgAA.gif 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="d857" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph=""><a href="https://en.wikipedia.org/wiki/Natural_language_processing" class="cw mp" rel="noopener nofollow"><strong class="ky hs">NLP (Natural Language Processing)</strong></a> is the field of artificial intelligence that studies the interactions between computers and human languages, in particular how to program computers to process and analyze large amounts of natural language data. NLP is often applied for classifying text data. <strong class="ky hs">Text classification</strong> is the problem of assigning categories to text data according to its content.</p><p id="deee" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">There<span id="rmm"><span id="rmm"><span id="rmm"><span id="rmm"><span id="rmm"><span id="rmm"> </span></span></span></span></span></span>are different techniques to extract information from raw text data and use it to train a classification model. This tutorial compares the old school approach of <em class="ls">Bag-of-Words</em> (used with a simple machine learning algorithm), the popular <em class="ls">Word Embedding</em> model (used with a deep learning neural network), and the state of the art <em class="ls">Language models</em> (used with transfer learning from attention-based transformers) that have completely revolutionized the NLP landscape.</p><p id="d290" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">I will present some useful Python code that can be easily applied in other similar cases (just copy, paste, run) and walk through every line of code with comments so that you can replicate this example (link to the full code below).</p><div class="hd he hf hg hh mq"><a href="https://github.com/mdipietro09/DataScience_ArtificialIntelligence_Utils/blob/master/natural_language_processing/example_text_classification.ipynb" target="_blank" rel="noopener nofollow"><div class="dj n z"><div class="mr n ao p ms mt"><h2 class="bf hs df bh mb mu mv mw mx my mz hq io">mdipietro09/DataScience_ArtificialIntelligence_Utils</h2><div class="na s"><h3 class="bf b df bh mb mu mv mw mx my mz es">Permalink Dismiss GitHub is home to over 50 million developers working together to host and review code, manage…</h3></div><div class="nb s"><p class="bf b nc bh mb mu mv mw mx my mz es">github.com</p></div></div><div class="nd s"><div class="ne s nf ng nh nd ni hn mq"></div></div></div></a></div><p id="27b4" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">I will use the “<strong class="ky hs">News category dataset</strong>” in which you are provided with news headlines from the year 2012 to 2018 obtained from <em class="ls">HuffPost </em>and you are asked to classify them with the right category, therefore this is a multiclass classification problem (link below).</p><div class="hd he hf hg hh mq"><a href="https://www.kaggle.com/rmisra/news-category-dataset" target="_blank" rel="noopener nofollow"><div class="dj n z"><div class="mr n ao p ms mt"><h2 class="bf hs df bh mb mu mv mw mx my mz hq io">News Category Dataset</h2><div class="na s"><h3 class="bf b df bh mb mu mv mw mx my mz es">Identify the type of news based on headlines and short descriptions</h3></div><div class="nb s"><p class="bf b nc bh mb mu mv mw mx my mz es">www.kaggle.com</p></div></div><div class="nd s"><div class="nj s nf ng nh nd ni hn mq"></div></div></div></a></div><p id="841e" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">In particular, I will go through:</p><ul class=""><li id="4920" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr nk nl nm io" data-selectable-paragraph="">Setup: import packages, read data, Preprocessing, Partitioning.</li><li id="e2a3" class="kw kx hr ky b iq nn la lb it no ld le lf np lh li lj nq ll lm ln nr lp lq lr nk nl nm io" data-selectable-paragraph="">Bag-of-Words: Feature Engineering &amp; Feature Selection &amp; Machine Learning with <em class="ls">scikit-learn</em>, Testing &amp; Evaluation, Explainability with <em class="ls">lime</em>.</li><li id="4c08" class="kw kx hr ky b iq nn la lb it no ld le lf np lh li lj nq ll lm ln nr lp lq lr nk nl nm io" data-selectable-paragraph="">Word Embedding: Fitting a Word2Vec with <em class="ls">gensim</em>, Feature Engineering &amp; Deep Learning with <em class="ls">tensorflow/keras</em>, Testing &amp; Evaluation, Explainability with the Attention mechanism.</li><li id="afd8" class="kw kx hr ky b iq nn la lb it no ld le lf np lh li lj nq ll lm ln nr lp lq lr nk nl nm io" data-selectable-paragraph="">Language Models: Feature Engineering with <em class="ls">transformers</em>, Transfer Learning from pre-trained BERT with <em class="ls">transformers </em>and<em class="ls"> tensorflow/keras, </em>Testing &amp; Evaluation.</li></ul></div></div></section><div class="n p cl ns nt nu" role="separator"><span class="nv jj db nw nx ny"></span><span class="nv jj db nw nx ny"></span><span class="nv jj db nw nx"></span></div><section class="gx gy gz ep ha"><div class="n p"><div class="au av aw ax ay hb ba v"><h2 id="ec62" class="kg kh hr bf ki kj kk is kl km kn iv ko iw kp iy kq iz kr jb ks jc kt je ku kv io" data-selectable-paragraph="">Setup</h2><p id="eafa" class="kw kx hr ky b iq kz la lb it lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr gx io" data-selectable-paragraph="">First of all, I need to import the following libraries:</p><pre class="lu lv lw lx ly nz oa ob"><span id="a49d" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph=""><strong class="oc hs">## for data<br></strong>import <strong class="oc hs">json<br></strong>import <strong class="oc hs">pandas </strong>as pd<br>import <strong class="oc hs">numpy </strong>as np</span><span id="9c21" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## for plotting</strong><br>import <strong class="oc hs">matplotlib</strong>.pyplot as plt<br>import <strong class="oc hs">seaborn </strong>as sns</span><span id="a39f" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## for processing<br></strong>import <strong class="oc hs">re</strong><br>import <strong class="oc hs">nltk</strong></span><span id="10de" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## for bag-of-words</strong><br>from <strong class="oc hs">sklearn </strong>import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing</span><span id="2f48" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## for explainer</strong><br>from <strong class="oc hs">lime </strong>import lime_text</span><span id="80df" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## for word embedding</strong><br>import <strong class="oc hs">gensim<br></strong>import gensim.downloader as gensim_api</span><span id="7b35" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## for deep learning</strong><br>from <strong class="oc hs">tensorflow</strong>.keras import models, layers, preprocessing as kprocessing<br>from tensorflow.keras import backend as K</span><span id="73bd" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## for bert language model</strong><br>import <strong class="oc hs">transformers</strong></span></pre><p id="9c06" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">The dataset is contained into a json file, so I will first read it into a list of dictionaries with <em class="ls">json </em>and then transform it into a <em class="ls">pandas </em>Dataframe.</p><pre class="lu lv lw lx ly nz oa ob"><span id="7f3b" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph="">lst_dics = []<br>with <strong class="oc hs">open</strong>('data.json', mode='r', errors='ignore') as json_file:<br>    for dic in json_file:<br>        lst_dics.append( json<strong class="oc hs">.loads</strong>(dic) )</span><span id="4125" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## print the first one</strong><br>lst_dics[0]</span></pre><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl ol"><div class="me s at mf"><div class="om mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_N7xAYy2MBRJHKMBXnxMi0A.png" width="700" height="107" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="700" height="107" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_N7xAYy2MBRJHKMBXnxMi0A(1).png" srcset="https://miro.medium.com/max/552/1*N7xAYy2MBRJHKMBXnxMi0A.png 276w, https://miro.medium.com/max/1104/1*N7xAYy2MBRJHKMBXnxMi0A.png 552w, https://miro.medium.com/max/1280/1*N7xAYy2MBRJHKMBXnxMi0A.png 640w, https://miro.medium.com/max/1400/1*N7xAYy2MBRJHKMBXnxMi0A.png 700w" sizes="700px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1400/1*N7xAYy2MBRJHKMBXnxMi0A.png" width="700" height="107" srcSet="https://miro.medium.com/max/552/1*N7xAYy2MBRJHKMBXnxMi0A.png 276w, https://miro.medium.com/max/1104/1*N7xAYy2MBRJHKMBXnxMi0A.png 552w, https://miro.medium.com/max/1280/1*N7xAYy2MBRJHKMBXnxMi0A.png 640w, https://miro.medium.com/max/1400/1*N7xAYy2MBRJHKMBXnxMi0A.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="6721" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">The original dataset contains over 30 categories, but for the purposes of this tutorial, I will work with a subset of 3: Entertainment, Politics, and Tech.</p><pre class="lu lv lw lx ly nz oa ob"><span id="b876" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph=""><strong class="oc hs">## create dtf</strong><br>dtf = pd.DataFrame(lst_dics)</span><span id="09fd" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## filter categories</strong><br>dtf = dtf[ dtf["category"].isin(['<strong class="oc hs">ENTERTAINMENT</strong>','<strong class="oc hs">POLITICS</strong>','<strong class="oc hs">TECH</strong>']) ][["category","headline"]]</span><span id="0cbe" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## rename columns</strong><br>dtf = dtf.rename(columns={"category":"<strong class="oc hs">y</strong>", "headline":"<strong class="oc hs">text</strong>"})</span><span id="da4f" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## print 5 random rows</strong><br>dtf.sample(5)</span></pre><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl on"><div class="me s at mf"><div class="oo mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_iurA976CkC9i1Yi1L6hIIw.png" width="700" height="246" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="700" height="246" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_iurA976CkC9i1Yi1L6hIIw(1).png" srcset="https://miro.medium.com/max/552/1*iurA976CkC9i1Yi1L6hIIw.png 276w, https://miro.medium.com/max/1104/1*iurA976CkC9i1Yi1L6hIIw.png 552w, https://miro.medium.com/max/1280/1*iurA976CkC9i1Yi1L6hIIw.png 640w, https://miro.medium.com/max/1400/1*iurA976CkC9i1Yi1L6hIIw.png 700w" sizes="700px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1400/1*iurA976CkC9i1Yi1L6hIIw.png" width="700" height="246" srcSet="https://miro.medium.com/max/552/1*iurA976CkC9i1Yi1L6hIIw.png 276w, https://miro.medium.com/max/1104/1*iurA976CkC9i1Yi1L6hIIw.png 552w, https://miro.medium.com/max/1280/1*iurA976CkC9i1Yi1L6hIIw.png 640w, https://miro.medium.com/max/1400/1*iurA976CkC9i1Yi1L6hIIw.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="c391" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">In order to understand the composition of the dataset, I am going to look into the <strong class="ky hs">univariate distribution</strong> of the target by showing labels frequency with a bar plot.</p><pre class="lu lv lw lx ly nz oa ob"><span id="4dac" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph="">fig, ax = plt.subplots()<br>fig.suptitle(<strong class="oc hs">"y"</strong>, fontsize=12)<br>dtf[<strong class="oc hs">"y"</strong>].reset_index().groupby(<strong class="oc hs">"y"</strong>).count().sort_values(by= <br>       "index").plot(kind="barh", legend=False, <br>        ax=ax).grid(axis='x')<br>plt.show()</span></pre><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl op"><div class="me s at mf"><div class="oq mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_b7hN7kENZzF4wsck1ne0QA.png" width="700" height="306" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="700" height="306" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_b7hN7kENZzF4wsck1ne0QA(1).png" srcset="https://miro.medium.com/max/552/1*b7hN7kENZzF4wsck1ne0QA.png 276w, https://miro.medium.com/max/1104/1*b7hN7kENZzF4wsck1ne0QA.png 552w, https://miro.medium.com/max/1280/1*b7hN7kENZzF4wsck1ne0QA.png 640w, https://miro.medium.com/max/1400/1*b7hN7kENZzF4wsck1ne0QA.png 700w" sizes="700px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1400/1*b7hN7kENZzF4wsck1ne0QA.png" width="700" height="306" srcSet="https://miro.medium.com/max/552/1*b7hN7kENZzF4wsck1ne0QA.png 276w, https://miro.medium.com/max/1104/1*b7hN7kENZzF4wsck1ne0QA.png 552w, https://miro.medium.com/max/1280/1*b7hN7kENZzF4wsck1ne0QA.png 640w, https://miro.medium.com/max/1400/1*b7hN7kENZzF4wsck1ne0QA.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="a1d5" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">The dataset is imbalanced: the proportion of Tech news is really small compared to the others, this will make for models to recognize Tech news rather tough.</p><p id="8362" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">Before explaining and building the models, I am going to give an example of preprocessing by cleaning text, removing stop words, and applying lemmatization. I will write a function and apply it to the whole data set.</p><pre class="lu lv lw lx ly nz oa ob"><span id="8b11" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph=""><strong class="oc hs">'''<br>Preprocess a string.<br>:parameter<br>    :param text: string - name of column containing text<br>    :param lst_stopwords: list - list of stopwords to remove<br>    :param flg_stemm: bool - whether stemming is to be applied<br>    :param flg_lemm: bool - whether lemmitisation is to be applied<br>:return<br>    cleaned text<br>'''</strong><br>def <strong class="oc hs">utils_preprocess_text</strong>(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):<br>    <strong class="oc hs">## clean (convert to lowercase and remove punctuations and   <br>    characters and then strip)</strong><br>    text = re.sub(r'[^\w\s]', '', str(text).lower().strip())<br>            <br>    <strong class="oc hs">## Tokenize (convert from string to list)</strong><br>    lst_text = text.split()</span><span id="c171" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">    ## remove Stopwords</strong><br>    if lst_stopwords is not None:<br>        lst_text = [word for word in lst_text if word not in <br>                    lst_stopwords]<br>                <br>    <strong class="oc hs">## Stemming (remove -ing, -ly, ...)</strong><br>    if flg_stemm == True:<br>        ps = nltk.stem.porter.PorterStemmer()<br>        lst_text = [ps.stem(word) for word in lst_text]<br>                <br>    <strong class="oc hs">## Lemmatisation (convert the word into root word)</strong><br>    if flg_lemm == True:<br>        lem = nltk.stem.wordnet.WordNetLemmatizer()<br>        lst_text = [lem.lemmatize(word) for word in lst_text]<br>            <br>    <strong class="oc hs">## back to string from list</strong><br>    text = " ".join(lst_text)<br>    return text</span></pre><p id="3136" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">That function removes a set of words from the corpus if given. I can create a list of generic stop words for the English vocabulary with <em class="ls">nltk </em>(we could edit this list by adding or removing words).</p><pre class="lu lv lw lx ly nz oa ob"><span id="72e4" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph="">lst_stopwords = <strong class="oc hs">nltk</strong>.corpus.stopwords.words("<strong class="oc hs">english</strong>")<br>lst_stopwords</span></pre><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl or"><div class="me s at mf"><div class="os mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_k1fsJU_S0_WPZku6gg-qOQ.png" width="700" height="146" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="700" height="146" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_k1fsJU_S0_WPZku6gg-qOQ(1).png" srcset="https://miro.medium.com/max/552/1*k1fsJU_S0_WPZku6gg-qOQ.png 276w, https://miro.medium.com/max/1104/1*k1fsJU_S0_WPZku6gg-qOQ.png 552w, https://miro.medium.com/max/1280/1*k1fsJU_S0_WPZku6gg-qOQ.png 640w, https://miro.medium.com/max/1400/1*k1fsJU_S0_WPZku6gg-qOQ.png 700w" sizes="700px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1400/1*k1fsJU_S0_WPZku6gg-qOQ.png" width="700" height="146" srcSet="https://miro.medium.com/max/552/1*k1fsJU_S0_WPZku6gg-qOQ.png 276w, https://miro.medium.com/max/1104/1*k1fsJU_S0_WPZku6gg-qOQ.png 552w, https://miro.medium.com/max/1280/1*k1fsJU_S0_WPZku6gg-qOQ.png 640w, https://miro.medium.com/max/1400/1*k1fsJU_S0_WPZku6gg-qOQ.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="482f" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">Now I shall apply the function I wrote on the whole dataset and store the result in a new column named “<em class="ls">text_clean</em>” so that you can choose to work with the raw corpus or the preprocessed text.</p><pre class="lu lv lw lx ly nz oa ob"><span id="8fe2" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph="">dtf["<strong class="oc hs">text_clean</strong>"] = dtf["text"].apply(lambda x: <br>          <strong class="oc hs">utils_preprocess_text</strong>(x, flg_stemm=False, <strong class="oc hs">flg_lemm=True</strong>, <br>          <strong class="oc hs">lst_stopwords=lst_stopwords</strong>))</span><span id="4fdd" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph="">dtf.head()</span></pre><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl ot"><div class="me s at mf"><div class="ou mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_t-R6djtHnK4cBVqFrjfLgA.png" width="700" height="157" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="700" height="157" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_t-R6djtHnK4cBVqFrjfLgA(1).png" srcset="https://miro.medium.com/max/552/1*t-R6djtHnK4cBVqFrjfLgA.png 276w, https://miro.medium.com/max/1104/1*t-R6djtHnK4cBVqFrjfLgA.png 552w, https://miro.medium.com/max/1280/1*t-R6djtHnK4cBVqFrjfLgA.png 640w, https://miro.medium.com/max/1400/1*t-R6djtHnK4cBVqFrjfLgA.png 700w" sizes="700px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1400/1*t-R6djtHnK4cBVqFrjfLgA.png" width="700" height="157" srcSet="https://miro.medium.com/max/552/1*t-R6djtHnK4cBVqFrjfLgA.png 276w, https://miro.medium.com/max/1104/1*t-R6djtHnK4cBVqFrjfLgA.png 552w, https://miro.medium.com/max/1280/1*t-R6djtHnK4cBVqFrjfLgA.png 640w, https://miro.medium.com/max/1400/1*t-R6djtHnK4cBVqFrjfLgA.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="d7f8" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">If you are interested in a deeper text analysis and preprocessing, you can check <a class="cw mp" rel="noopener" href="https://towardsdatascience.com/text-analysis-feature-engineering-with-nlp-502d6ea9225d">this article</a>. With this in mind, I am going to partition the dataset into training set (70%) and test set (30%) in order to evaluate the models performance.</p><pre class="lu lv lw lx ly nz oa ob"><span id="00f2" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph=""><strong class="oc hs">## split dataset</strong><br>dtf_train, dtf_test = model_selection.<strong class="oc hs">train_test_split</strong>(dtf, test_size=0.3)</span><span id="4b93" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## get target</strong><br>y_train = dtf_train[<strong class="oc hs">"y"</strong>].values<br>y_test = dtf_test[<strong class="oc hs">"y"</strong>].values</span></pre><p id="e70d" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">Let’s get started, shall we?</p><h2 id="a81a" class="kg kh hr bf ki kj kk is kl km kn iv ko iw kp iy kq iz kr jb ks jc kt je ku kv io" data-selectable-paragraph="">Bag-of-Words</h2><p id="96c7" class="kw kx hr ky b iq kz la lb it lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr gx io" data-selectable-paragraph="">The <a href="https://en.wikipedia.org/wiki/Bag-of-words_model" class="cw mp" rel="noopener nofollow"><em class="ls">Bag-of-Words</em></a> model is simple: it builds a vocabulary from a corpus of documents and counts how many times the words appear in each document. To put it another way, each word in the vocabulary becomes a feature and a document is represented by a vector with the same length of the vocabulary (a “bag of words”). For instance, let’s take 3 sentences and represent them with this approach:</p><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl ov"><div class="me s at mf"><div class="ow mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_m1O25pvl8R5DlkhuJjRrDw.png" width="700" height="130" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="700" height="130" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_m1O25pvl8R5DlkhuJjRrDw(1).png" srcset="https://miro.medium.com/max/552/1*m1O25pvl8R5DlkhuJjRrDw.png 276w, https://miro.medium.com/max/1104/1*m1O25pvl8R5DlkhuJjRrDw.png 552w, https://miro.medium.com/max/1280/1*m1O25pvl8R5DlkhuJjRrDw.png 640w, https://miro.medium.com/max/1400/1*m1O25pvl8R5DlkhuJjRrDw.png 700w" sizes="700px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1400/1*m1O25pvl8R5DlkhuJjRrDw.png" width="700" height="130" srcSet="https://miro.medium.com/max/552/1*m1O25pvl8R5DlkhuJjRrDw.png 276w, https://miro.medium.com/max/1104/1*m1O25pvl8R5DlkhuJjRrDw.png 552w, https://miro.medium.com/max/1280/1*m1O25pvl8R5DlkhuJjRrDw.png 640w, https://miro.medium.com/max/1400/1*m1O25pvl8R5DlkhuJjRrDw.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="ox oy gm gk gl oz pa bf b bg bh es" data-selectable-paragraph="">Feature matrix shape: <strong class="bf ki"><em class="pb">Number of documents</em></strong><em class="pb"> </em>x<em class="pb"> </em><strong class="bf ki"><em class="pb">Length of vocabulary</em></strong></figcaption></figure><p id="f309" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">As you can imagine, this approach causes a significant dimensionality problem: the more documents you have the larger is the vocabulary, so the feature matrix will be a huge sparse matrix. Therefore, the Bag-of-Words model is usually preceded by an important preprocessing (word cleaning, stop words removal, stemming/lemmatization) aimed to reduce the dimensionality problem.</p><p id="1375" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">Terms frequency is not necessarily the best representation for text. In fact, you can find in the corpus common words with the highest frequency but little predictive power over the target variable. To address this problem there is an advanced variant of the Bag-of-Words that, instead of simple counting, uses the <strong class="ky hs">term frequency–inverse document frequency </strong>(or T<a href="https://en.wikipedia.org/wiki/Tf%E2%80%93idf" class="cw mp" rel="noopener nofollow">f–Idf</a>)<strong class="ky hs">. </strong>Basically, the value of a word increases proportionally to count, but it is inversely proportional to the frequency of the word in the corpus.</p><p id="3524" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">Let’s start with the <strong class="ky hs">Feature Engineering, </strong>the process to create features by extracting information from the data. I am going to use the Tf-Idf vectorizer with a limit of 10,000 words (so the length of my vocabulary will be 10k), capturing unigrams (i.e. “<em class="ls">new</em>” and “<em class="ls">york</em>”) and bigrams (i.e. “<em class="ls">new york</em>”). I will provide the code for the classic count vectorizer as well:</p><pre class="lu lv lw lx ly nz oa ob"><span id="98fe" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph=""><strong class="oc hs"><em class="ls">## Count (classic BoW)</em></strong><br><em class="ls">vectorizer = feature_extraction.text.</em><strong class="oc hs"><em class="ls">CountVectorizer</em></strong><em class="ls">(max_features=10000, </em>ngram_range=(1,2))<br><br><strong class="oc hs"><em class="ls">## Tf-Idf (advanced variant of BoW)</em></strong><br>vectorizer = feature_extraction.text.<strong class="oc hs">TfidfVectorizer</strong>(max_features=10000, ngram_range=(1,2))</span></pre><p id="187b" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">Now I will use the vectorizer on the preprocessed corpus of the train set to extract a vocabulary and create the feature matrix.</p><pre class="lu lv lw lx ly nz oa ob"><span id="1f61" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph="">corpus = dtf_train["<strong class="oc hs">text_clean</strong>"]</span><span id="eb41" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph="">vectorizer.fit(corpus)<br>X_train = vectorizer.transform(corpus)<br>dic_vocabulary = vectorizer.vocabulary_</span></pre><p id="0d8b" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">The feature matrix <em class="ls">X_train </em>has a shape of 34,265 (Number of documents in training) x 10,000 (Length of vocabulary) and it’s pretty sparse:</p><pre class="lu lv lw lx ly nz oa ob"><span id="12f1" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph="">sns.<strong class="oc hs">heatmap</strong>(X_train.todense()[:,np.random.randint(0,X.shape[1],100)]==0, vmin=0, vmax=1, cbar=False).set_title('Sparse Matrix Sample')</span></pre><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl pc"><div class="me s at mf"><div class="pd mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_CpZ9fxPY5iSEzgdyS021_Q.png" width="700" height="245" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="700" height="245" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_CpZ9fxPY5iSEzgdyS021_Q(1).png" srcset="https://miro.medium.com/max/552/1*CpZ9fxPY5iSEzgdyS021_Q.png 276w, https://miro.medium.com/max/1104/1*CpZ9fxPY5iSEzgdyS021_Q.png 552w, https://miro.medium.com/max/1280/1*CpZ9fxPY5iSEzgdyS021_Q.png 640w, https://miro.medium.com/max/1400/1*CpZ9fxPY5iSEzgdyS021_Q.png 700w" sizes="700px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1400/1*CpZ9fxPY5iSEzgdyS021_Q.png" width="700" height="245" srcSet="https://miro.medium.com/max/552/1*CpZ9fxPY5iSEzgdyS021_Q.png 276w, https://miro.medium.com/max/1104/1*CpZ9fxPY5iSEzgdyS021_Q.png 552w, https://miro.medium.com/max/1280/1*CpZ9fxPY5iSEzgdyS021_Q.png 640w, https://miro.medium.com/max/1400/1*CpZ9fxPY5iSEzgdyS021_Q.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="ox oy gm gk gl oz pa bf b bg bh es" data-selectable-paragraph="">Random sample from the feature matrix (non-zero values in black)</figcaption></figure><p id="9df7" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">In order to know the position of a certain word, we can look it up in the vocabulary:</p><pre class="lu lv lw lx ly nz oa ob"><span id="b534" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph="">word = "new york"</span><span id="d66b" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph="">dic_vocabulary[word]</span></pre><p id="931e" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">If the word exists in the vocabulary, this command prints a number <em class="ls">N</em>, meaning that the <em class="ls">N</em>th feature of the matrix is that word.</p><p id="18fd" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">In order to drop some columns and reduce the matrix dimensionality, we can carry out some <strong class="ky hs">Feature Selection</strong>, the process of selecting a subset of relevant variables. I will proceed as follows:</p><ol class=""><li id="d5ae" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr pe nl nm io" data-selectable-paragraph="">treat each category as binary (for example, the “Tech” category is 1 for the Tech news and 0 for the others);</li><li id="6d94" class="kw kx hr ky b iq nn la lb it no ld le lf np lh li lj nq ll lm ln nr lp lq lr pe nl nm io" data-selectable-paragraph=""><mark class="zn zo fq">perform a </mark><mark class="zn zo fq"><a href="https://en.wikipedia.org/wiki/Chi-squared_test" class="cw mp" rel="noopener nofollow">Chi-Square test</a></mark><mark class="zn zo fq"> to determine whether a feature and the (binary) target are independent;</mark></li><li id="31b3" class="kw kx hr ky b iq nn la lb it no ld le lf np lh li lj nq ll lm ln nr lp lq lr pe nl nm io" data-selectable-paragraph="">keep only the features with a certain p-value from the Chi-Square test.</li></ol><pre class="lu lv lw lx ly nz oa ob"><span id="3c2d" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph="">y = dtf_train["<strong class="oc hs">y</strong>"]<br>X_names = vectorizer.get_feature_names()<br>p_value_limit = 0.95</span><span id="70ae" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph="">dtf_features = pd.DataFrame()<br>for cat in np.unique(y):<br>    chi2, p = feature_selection.<strong class="oc hs">chi2</strong>(X_train, y==cat)<br>    dtf_features = dtf_features.append(pd.DataFrame(<br>                   {"feature":X_names, "score":1-p, "y":cat}))<br>    dtf_features = dtf_features.sort_values(["y","score"], <br>                    ascending=[True,False])<br>    dtf_features = dtf_features[dtf_features["score"]&gt;p_value_limit]</span><span id="4aaa" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph="">X_names = dtf_features["feature"].unique().tolist()</span></pre><p id="cf35" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">I reduced the number of features from 10,000 to 3,152 by keeping the most statistically relevant ones. Let’s print some:</p><pre class="lu lv lw lx ly nz oa ob"><span id="9c3d" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph="">for cat in np.unique(y):<br>   print("# {}:".format(cat))<br>   print("  . selected features:",<br>         len(dtf_features[dtf_features["y"]==cat]))<br>   print("  . top features:", ",".join(<br>dtf_features[dtf_features["y"]==cat]["feature"].values[:10]))<br>   print(" ")</span></pre><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl pf"><div class="me s at mf"><div class="pg mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_Fo0EjcD4Ibo2Jz1y6eNL0A.png" width="700" height="150" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="700" height="150" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_Fo0EjcD4Ibo2Jz1y6eNL0A(1).png" srcset="https://miro.medium.com/max/552/1*Fo0EjcD4Ibo2Jz1y6eNL0A.png 276w, https://miro.medium.com/max/1104/1*Fo0EjcD4Ibo2Jz1y6eNL0A.png 552w, https://miro.medium.com/max/1280/1*Fo0EjcD4Ibo2Jz1y6eNL0A.png 640w, https://miro.medium.com/max/1400/1*Fo0EjcD4Ibo2Jz1y6eNL0A.png 700w" sizes="700px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1400/1*Fo0EjcD4Ibo2Jz1y6eNL0A.png" width="700" height="150" srcSet="https://miro.medium.com/max/552/1*Fo0EjcD4Ibo2Jz1y6eNL0A.png 276w, https://miro.medium.com/max/1104/1*Fo0EjcD4Ibo2Jz1y6eNL0A.png 552w, https://miro.medium.com/max/1280/1*Fo0EjcD4Ibo2Jz1y6eNL0A.png 640w, https://miro.medium.com/max/1400/1*Fo0EjcD4Ibo2Jz1y6eNL0A.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="1761" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">We can refit the vectorizer on the corpus by giving this new set of words as input. That will produce a smaller feature matrix and a shorter vocabulary.</p><pre class="lu lv lw lx ly nz oa ob"><span id="9a1f" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph="">vectorizer = feature_extraction.text.<strong class="oc hs">TfidfVectorizer</strong>(vocabulary=X_names)</span><span id="123e" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph="">vectorizer.fit(corpus)<br>X_train = vectorizer.transform(corpus)<br>dic_vocabulary = vectorizer.vocabulary_</span></pre><p id="d6e5" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">The new feature matrix <em class="ls">X_train</em> has a shape of is 34,265 (Number of documents in training) x 3,152 (Length of the given vocabulary). Let’s see if the matrix is less sparse:</p><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl ph"><div class="me s at mf"><div class="pi mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_O8lMt_obkHbMXuOSg1bTRA.png" width="700" height="247" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="700" height="247" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_O8lMt_obkHbMXuOSg1bTRA(1).png" srcset="https://miro.medium.com/max/552/1*O8lMt_obkHbMXuOSg1bTRA.png 276w, https://miro.medium.com/max/1104/1*O8lMt_obkHbMXuOSg1bTRA.png 552w, https://miro.medium.com/max/1280/1*O8lMt_obkHbMXuOSg1bTRA.png 640w, https://miro.medium.com/max/1400/1*O8lMt_obkHbMXuOSg1bTRA.png 700w" sizes="700px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1400/1*O8lMt_obkHbMXuOSg1bTRA.png" width="700" height="247" srcSet="https://miro.medium.com/max/552/1*O8lMt_obkHbMXuOSg1bTRA.png 276w, https://miro.medium.com/max/1104/1*O8lMt_obkHbMXuOSg1bTRA.png 552w, https://miro.medium.com/max/1280/1*O8lMt_obkHbMXuOSg1bTRA.png 640w, https://miro.medium.com/max/1400/1*O8lMt_obkHbMXuOSg1bTRA.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="ox oy gm gk gl oz pa bf b bg bh es" data-selectable-paragraph="">Random sample from the new feature matrix (non-zero values in black)</figcaption></figure><p id="3a83" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">It’s time to train a <strong class="ky hs">machine learning model</strong> and test it. I recommend using a Naive Bayes algorithm: a probabilistic classifier that makes use of <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem" class="cw mp" rel="noopener nofollow">Bayes’ Theorem</a>, a rule that uses probability to make predictions based on prior knowledge of conditions that might be related. This algorithm is the most suitable for such large dataset as it considers each feature independently, calculates the probability of each category, and then predicts the category with the highest probability.</p><pre class="lu lv lw lx ly nz oa ob"><span id="a2de" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph="">classifier = naive_bayes.<strong class="oc hs">MultinomialNB</strong>()</span></pre><p id="4428" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">I’m going to train this classifier on the feature matrix and then test it on the transformed test set. To that end, I need to build a <em class="ls">scikit-learn</em> pipeline: a sequential application of a list of transformations and a final estimator. Putting the Tf-Idf vectorizer and the Naive Bayes classifier in a pipeline allows us to transform and predict test data in just one step.</p><pre class="lu lv lw lx ly nz oa ob"><span id="374d" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph=""><strong class="oc hs">## pipeline</strong><br>model = pipeline.<strong class="oc hs">Pipeline</strong>([("<strong class="oc hs">vectorizer</strong>", vectorizer),  <br>                           ("<strong class="oc hs">classifier</strong>", classifier)])</span><span id="8518" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## train classifier<br></strong>model["classifier"].fit(X_train, y_train)</span><span id="f0ee" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## test<br></strong>X_test = dtf_test["text_clean"].values<br>predicted = model.predict(X_test)<br>predicted_prob = model.predict_proba(X_test)</span></pre><p id="8016" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">We can now <strong class="ky hs">evaluate the performance</strong> of the Bag-of-Words model, I will use the following metrics:</p><ul class=""><li id="97f9" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr nk nl nm io" data-selectable-paragraph="">Accuracy: the fraction of predictions the model got right.</li><li id="1828" class="kw kx hr ky b iq nn la lb it no ld le lf np lh li lj nq ll lm ln nr lp lq lr nk nl nm io" data-selectable-paragraph="">Confusion Matrix: a summary table that breaks down the number of correct and incorrect predictions by each class.</li><li id="e253" class="kw kx hr ky b iq nn la lb it no ld le lf np lh li lj nq ll lm ln nr lp lq lr nk nl nm io" data-selectable-paragraph="">ROC: a plot that illustrates the true positive rate against the false positive rate at various threshold settings. The area under the curve (AUC) indicates the probability that the classifier will rank a randomly chosen positive observation higher than a randomly chosen negative one.</li><li id="c5f5" class="kw kx hr ky b iq nn la lb it no ld le lf np lh li lj nq ll lm ln nr lp lq lr nk nl nm io" data-selectable-paragraph="">Precision: the fraction of relevant instances among the retrieved instances.</li><li id="7682" class="kw kx hr ky b iq nn la lb it no ld le lf np lh li lj nq ll lm ln nr lp lq lr nk nl nm io" data-selectable-paragraph="">Recall: the fraction of the total amount of relevant instances that were actually retrieved.</li></ul><pre class="lu lv lw lx ly nz oa ob"><span id="4c56" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph="">classes = np.unique(y_test)<br>y_test_array = pd.get_dummies(y_test, drop_first=False).values<br>    </span><span id="52c1" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## Accuracy, Precision, Recall</strong><br>accuracy = metrics.accuracy_score(y_test, predicted)<br>auc = metrics.roc_auc_score(y_test, predicted_prob, <br>                            multi_class="ovr")<br>print("Accuracy:",  round(accuracy,2))<br>print("Auc:", round(auc,2))<br>print("Detail:")<br>print(metrics.classification_report(y_test, predicted))<br>    <br><strong class="oc hs">## Plot confusion matrix</strong><br>cm = metrics.confusion_matrix(y_test, predicted)<br>fig, ax = plt.subplots()<br>sns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, <br>            cbar=False)<br>ax.set(xlabel="Pred", ylabel="True", xticklabels=classes, <br>       yticklabels=classes, title="Confusion matrix")<br>plt.yticks(rotation=0)</span><span id="4b72" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><br>fig, ax = plt.subplots(nrows=1, ncols=2)<br><strong class="oc hs">## Plot roc</strong><br>for i in range(len(classes)):<br>    fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  <br>                           predicted_prob[:,i])<br>    ax[0].plot(fpr, tpr, lw=3, <br>              label='{0} (area={1:0.2f})'.format(classes[i], <br>                              metrics.auc(fpr, tpr))<br>               )<br>ax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')<br>ax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], <br>          xlabel='False Positive Rate', <br>          ylabel="True Positive Rate (Recall)", <br>          title="Receiver operating characteristic")<br>ax[0].legend(loc="lower right")<br>ax[0].grid(True)<br>    <br><strong class="oc hs">## Plot precision-recall curve<br></strong>for i in range(len(classes)):<br>    precision, recall, thresholds = metrics.precision_recall_curve(<br>                 y_test_array[:,i], predicted_prob[:,i])<br>    ax[1].plot(recall, precision, lw=3, <br>               label='{0} (area={1:0.2f})'.format(classes[i], <br>                                  metrics.auc(recall, precision))<br>              )<br>ax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', <br>          ylabel="Precision", title="Precision-Recall curve")<br>ax[1].legend(loc="best")<br>ax[1].grid(True)<br>plt.show()</span></pre><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl pj"><div class="me s at mf"><div class="pk mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_iPL_8iJOuTJ_mrLvftwUEw.png" width="700" height="655" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="700" height="655" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_iPL_8iJOuTJ_mrLvftwUEw(1).png" srcset="https://miro.medium.com/max/552/1*iPL_8iJOuTJ_mrLvftwUEw.png 276w, https://miro.medium.com/max/1104/1*iPL_8iJOuTJ_mrLvftwUEw.png 552w, https://miro.medium.com/max/1280/1*iPL_8iJOuTJ_mrLvftwUEw.png 640w, https://miro.medium.com/max/1400/1*iPL_8iJOuTJ_mrLvftwUEw.png 700w" sizes="700px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1400/1*iPL_8iJOuTJ_mrLvftwUEw.png" width="700" height="655" srcSet="https://miro.medium.com/max/552/1*iPL_8iJOuTJ_mrLvftwUEw.png 276w, https://miro.medium.com/max/1104/1*iPL_8iJOuTJ_mrLvftwUEw.png 552w, https://miro.medium.com/max/1280/1*iPL_8iJOuTJ_mrLvftwUEw.png 640w, https://miro.medium.com/max/1400/1*iPL_8iJOuTJ_mrLvftwUEw.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="7722" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">The BoW model got 85% of the test set right (Accuracy is 0.85), but struggles to recognize Tech news (only 252 predicted correctly).</p><p id="9d9a" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">Let’s try to understand why the model classifies news with a certain category and assess the <strong class="ky hs">explainability </strong>of these predictions. The <em class="ls">lime </em>package can help us to build an explainer. To give an illustration, I will take a random observation from the test set and see what the model predicts and why.</p><pre class="lu lv lw lx ly nz oa ob"><span id="31a1" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph=""><strong class="oc hs">## select observation<br></strong>i = 0<br>txt_instance = dtf_test["<strong class="oc hs">text</strong>"].iloc[i]</span><span id="346e" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## check true value and predicted value</strong><br>print("True:", y_test[i], "--&gt; Pred:", predicted[i], "| Prob:", round(np.max(predicted_prob[i]),2))</span><span id="9694" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## show explanation</strong><br>explainer = lime_text.<strong class="oc hs">LimeTextExplainer</strong>(class_names=<br>             np.unique(y_train))<br>explained = explainer.explain_instance(txt_instance, <br>             model.predict_proba, num_features=3)<br>explained.show_in_notebook(text=txt_instance, predict_proba=False)</span></pre><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div class="gk gl pl"><div class="me s at mf"><div class="pm mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_CFZTX1Ud0jOwNZMhrr4cWA.png" width="607" height="44" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="607" height="44" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_CFZTX1Ud0jOwNZMhrr4cWA(1).png" srcset="https://miro.medium.com/max/552/1*CFZTX1Ud0jOwNZMhrr4cWA.png 276w, https://miro.medium.com/max/1104/1*CFZTX1Ud0jOwNZMhrr4cWA.png 552w, https://miro.medium.com/max/1214/1*CFZTX1Ud0jOwNZMhrr4cWA.png 607w" sizes="607px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1214/1*CFZTX1Ud0jOwNZMhrr4cWA.png" width="607" height="44" srcSet="https://miro.medium.com/max/552/1*CFZTX1Ud0jOwNZMhrr4cWA.png 276w, https://miro.medium.com/max/1104/1*CFZTX1Ud0jOwNZMhrr4cWA.png 552w, https://miro.medium.com/max/1214/1*CFZTX1Ud0jOwNZMhrr4cWA.png 607w" sizes="607px" role="presentation"/></noscript></div></div></div></figure><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl pn"><div class="me s at mf"><div class="po mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_qcAV1wvucxNogDz_eh3dpQ.png" width="700" height="97" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="700" height="97" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_qcAV1wvucxNogDz_eh3dpQ(1).png" srcset="https://miro.medium.com/max/552/1*qcAV1wvucxNogDz_eh3dpQ.png 276w, https://miro.medium.com/max/1104/1*qcAV1wvucxNogDz_eh3dpQ.png 552w, https://miro.medium.com/max/1280/1*qcAV1wvucxNogDz_eh3dpQ.png 640w, https://miro.medium.com/max/1400/1*qcAV1wvucxNogDz_eh3dpQ.png 700w" sizes="700px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1400/1*qcAV1wvucxNogDz_eh3dpQ.png" width="700" height="97" srcSet="https://miro.medium.com/max/552/1*qcAV1wvucxNogDz_eh3dpQ.png 276w, https://miro.medium.com/max/1104/1*qcAV1wvucxNogDz_eh3dpQ.png 552w, https://miro.medium.com/max/1280/1*qcAV1wvucxNogDz_eh3dpQ.png 640w, https://miro.medium.com/max/1400/1*qcAV1wvucxNogDz_eh3dpQ.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="cf12" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">That makes sense: the words “<em class="ls">Clinton</em>” and “<em class="ls">GOP</em>” pointed the model in the right direction (Politics news) even if the word “<em class="ls">Stage</em>” is more common among Entertainment news.</p><h2 id="b371" class="kg kh hr bf ki kj kk is kl km kn iv ko iw kp iy kq iz kr jb ks jc kt je ku kv io" data-selectable-paragraph="">Word Embedding</h2><p id="58f4" class="kw kx hr ky b iq kz la lb it lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr gx io" data-selectable-paragraph=""><a href="https://en.wikipedia.org/wiki/Word_embedding" class="cw mp" rel="noopener nofollow"><em class="ls">Word Embedding</em></a> is the collective name for feature learning techniques where words from the vocabulary are mapped to vectors of real numbers. These vectors are calculated from the probability distribution for each word appearing before or after another. To put it another way, words of the same context usually appear together in the corpus, so they will be close in the vector space as well. For instance, let’s take the 3 sentences from the previous example:</p><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl pp"><div class="me s at mf"><div class="pq mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_u67szEvNSMqrQeitdahw_A.png" width="700" height="278" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="700" height="278" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_u67szEvNSMqrQeitdahw_A(1).png" srcset="https://miro.medium.com/max/552/1*u67szEvNSMqrQeitdahw_A.png 276w, https://miro.medium.com/max/1104/1*u67szEvNSMqrQeitdahw_A.png 552w, https://miro.medium.com/max/1280/1*u67szEvNSMqrQeitdahw_A.png 640w, https://miro.medium.com/max/1400/1*u67szEvNSMqrQeitdahw_A.png 700w" sizes="700px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1400/1*u67szEvNSMqrQeitdahw_A.png" width="700" height="278" srcSet="https://miro.medium.com/max/552/1*u67szEvNSMqrQeitdahw_A.png 276w, https://miro.medium.com/max/1104/1*u67szEvNSMqrQeitdahw_A.png 552w, https://miro.medium.com/max/1280/1*u67szEvNSMqrQeitdahw_A.png 640w, https://miro.medium.com/max/1400/1*u67szEvNSMqrQeitdahw_A.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="ox oy gm gk gl oz pa bf b bg bh es" data-selectable-paragraph="">Words embedded in 2D vector space</figcaption></figure><p id="efe4" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">In this tutorial, I’m going to use the first model of this family: Google’s <a href="https://en.wikipedia.org/wiki/Word2vec" class="cw mp" rel="noopener nofollow"><em class="ls">Word2Vec</em></a><em class="ls"> </em>(2013). Other popular Word Embedding models are Stanford’s <a href="https://en.wikipedia.org/wiki/GloVe_(machine_learning)" class="cw mp" rel="noopener nofollow"><em class="ls">GloVe</em></a><em class="ls"> </em>(2014)<em class="ls"> </em>and Facebook’s <a href="https://en.wikipedia.org/wiki/FastText" class="cw mp" rel="noopener nofollow"><em class="ls">FastText</em></a> (2016).</p><p id="b046" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph=""><strong class="ky hs">Word2Vec </strong>produces a vector space, typically of several hundred dimensions, with each unique word in the corpus such that words that share common contexts in the corpus are located close to one another in the space. That can be done using 2 different approaches: starting from a single word to predict its context (<em class="ls">Skip-gram</em>) or starting from the context to predict a word (<em class="ls">Continuous Bag-of-Words</em>).</p><p id="3453" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">In Python, you can load a pre-trained Word Embedding model from <a href="https://github.com/RaRe-Technologies/gensim-data" class="cw mp" rel="noopener nofollow"><em class="ls">genism-data</em></a><em class="ls"> </em>like this:</p><pre class="lu lv lw lx ly nz oa ob"><span id="fb2e" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph="">nlp = gensim_api.load("<strong class="oc hs">word2vec-google-news-300"</strong>)</span></pre><p id="3b3c" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">Instead of using a pre-trained model, I am going to fit my own Word2Vec on the training data corpus with <em class="ls">gensim.</em> Before fitting the model, the corpus needs to be transformed into a list of lists of n-grams. In this particular case, I’ll try to capture unigrams (“<em class="ls">york</em>”), bigrams (“<em class="ls">new york</em>”), and trigrams (“<em class="ls">new york city</em>”).</p><pre class="lu lv lw lx ly nz oa ob"><span id="eabd" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph="">corpus = dtf_train["<strong class="oc hs">text_clean</strong>"]</span><span id="c998" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs"><br>## create list of lists of unigrams</strong><br>lst_corpus = []<br>for string in corpus:<br>   lst_words = string.split()<br>   lst_grams = [" ".join(lst_words[i:i+1]) <br>               for i in range(0, len(lst_words), 1)]<br>   lst_corpus.append(lst_grams)</span><span id="ade3" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs"><br>## detect bigrams and trigrams</strong><br>bigrams_detector = gensim.models.phrases.<strong class="oc hs">Phrases</strong>(lst_corpus, <br>                 delimiter=" ".encode(), min_count=5, threshold=10)<br>bigrams_detector = gensim.models.phrases.<strong class="oc hs">Phraser</strong>(bigrams_detector)</span><span id="3024" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph="">trigrams_detector = gensim.models.phrases.<strong class="oc hs">Phrases</strong>(bigrams_detector[lst_corpus], <br>            delimiter=" ".encode(), min_count=5, threshold=10)<br>trigrams_detector = gensim.models.phrases.<strong class="oc hs">Phraser</strong>(trigrams_detector)</span></pre><p id="c423" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">When fitting the Word2Vec, you need to specify:</p><ul class=""><li id="8700" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr nk nl nm io" data-selectable-paragraph="">the target size of the word vectors, I’ll use 300;</li><li id="f753" class="kw kx hr ky b iq nn la lb it no ld le lf np lh li lj nq ll lm ln nr lp lq lr nk nl nm io" data-selectable-paragraph="">the window, or the maximum distance between the current and predicted word within a sentence, I’ll use the mean length of text in the corpus;</li><li id="e0a7" class="kw kx hr ky b iq nn la lb it no ld le lf np lh li lj nq ll lm ln nr lp lq lr nk nl nm io" data-selectable-paragraph="">the training algorithm, I’ll use skip-grams (sg=1) as in general it has better results.</li></ul><pre class="lu lv lw lx ly nz oa ob"><span id="a5f2" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph=""><strong class="oc hs">## fit w2v</strong><br>nlp = gensim.models.word2vec.<strong class="oc hs">Word2Vec</strong>(lst_corpus, size=300,   <br>            window=8, min_count=1, sg=1, iter=30)</span></pre><p id="9959" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">We have our embedding model, so we can select any word from the corpus and transform it into a vector.</p><pre class="lu lv lw lx ly nz oa ob"><span id="c003" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph="">word = "data"<br>nlp[word].shape</span></pre><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div class="gk gl pr"><div class="me s at mf"><div class="ps mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_nMBpatACacNe1SZYF39j3Q.png" width="120" height="32" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="120" height="32" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_nMBpatACacNe1SZYF39j3Q(1).png" srcset="" sizes="120px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/240/1*nMBpatACacNe1SZYF39j3Q.png" width="120" height="32" role="presentation"/></noscript></div></div></div></figure><p id="d20c" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">We can even use it to visualize a word and its context into a smaller dimensional space (2D or 3D) by applying any dimensionality reduction algorithm (i.e. <a href="https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html" class="cw mp" rel="noopener nofollow">TSNE</a>).</p><pre class="lu lv lw lx ly nz oa ob"><span id="d8d1" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph="">word = "data"<br>fig = plt.figure()</span><span id="408e" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## word embedding</strong><br>tot_words = [word] + [tupla[0] for tupla in <br>                 nlp.most_similar(word, topn=20)]<br>X = nlp[tot_words]</span><span id="a680" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## pca to reduce dimensionality from 300 to 3</strong><br>pca = manifold.<strong class="oc hs">TSNE</strong>(perplexity=40, n_components=3, init='pca')<br>X = pca.fit_transform(X)</span><span id="c7cd" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## create dtf</strong><br>dtf_ = pd.DataFrame(X, index=tot_words, columns=["x","y","z"])<br>dtf_["input"] = 0<br>dtf_["input"].iloc[0:1] = 1</span><span id="618e" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## plot 3d</strong><br>from mpl_toolkits.mplot3d import Axes3D<br>ax = fig.add_subplot(111, projection='3d')<br>ax.scatter(dtf_[dtf_["input"]==0]['x'], <br>           dtf_[dtf_["input"]==0]['y'], <br>           dtf_[dtf_["input"]==0]['z'], c="black")<br>ax.scatter(dtf_[dtf_["input"]==1]['x'], <br>           dtf_[dtf_["input"]==1]['y'], <br>           dtf_[dtf_["input"]==1]['z'], c="red")<br>ax.set(xlabel=None, ylabel=None, zlabel=None, xticklabels=[], <br>       yticklabels=[], zticklabels=[])<br>for label, row in dtf_[["x","y","z"]].iterrows():<br>    x, y, z = row<br>    ax.text(x, y, z, s=label)</span></pre><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl lt"><div class="me s at mf"><div class="mg mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_T8WWibd7u8b7gfgeG0LgAA.gif" width="700" height="342" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="700" height="342" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_T8WWibd7u8b7gfgeG0LgAA(1).gif" srcset="https://miro.medium.com/max/552/1*T8WWibd7u8b7gfgeG0LgAA.gif 276w, https://miro.medium.com/max/1104/1*T8WWibd7u8b7gfgeG0LgAA.gif 552w, https://miro.medium.com/max/1280/1*T8WWibd7u8b7gfgeG0LgAA.gif 640w, https://miro.medium.com/max/1400/1*T8WWibd7u8b7gfgeG0LgAA.gif 700w" sizes="700px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1400/1*T8WWibd7u8b7gfgeG0LgAA.gif" width="700" height="342" srcSet="https://miro.medium.com/max/552/1*T8WWibd7u8b7gfgeG0LgAA.gif 276w, https://miro.medium.com/max/1104/1*T8WWibd7u8b7gfgeG0LgAA.gif 552w, https://miro.medium.com/max/1280/1*T8WWibd7u8b7gfgeG0LgAA.gif 640w, https://miro.medium.com/max/1400/1*T8WWibd7u8b7gfgeG0LgAA.gif 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="2637" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">That’s pretty cool and all, but how can the word embedding be useful to predict the news category? Well, the word vectors can be used in a neural network as weights. This is how:</p><ul class=""><li id="6309" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr nk nl nm io" data-selectable-paragraph="">First, transform the corpus into padded sequences of word ids to get a feature matrix.</li><li id="1700" class="kw kx hr ky b iq nn la lb it no ld le lf np lh li lj nq ll lm ln nr lp lq lr nk nl nm io" data-selectable-paragraph="">Then, create an embedding matrix so that the vector of the word with id <em class="ls">N </em>is located at the <em class="ls">Nth</em> row.</li><li id="2904" class="kw kx hr ky b iq nn la lb it no ld le lf np lh li lj nq ll lm ln nr lp lq lr nk nl nm io" data-selectable-paragraph="">Finally, build a neural network with an embedding layer that weighs every word in the sequences with the corresponding vector.</li></ul><p id="7f89" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">Let’s start with the <strong class="ky hs">Feature Engineering </strong>by transforming the same preprocessed corpus (list of lists of n-grams) given to the Word2Vec into a list of sequences using <em class="ls">tensorflow/keras</em>:</p><pre class="lu lv lw lx ly nz oa ob"><span id="17e9" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph=""><strong class="oc hs">## tokenize text</strong><br>tokenizer = kprocessing.text.<strong class="oc hs">Tokenizer</strong>(lower=True, split=' ', <br>                     oov_token="NaN", <br>                     filters='!"#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n')<br>tokenizer.fit_on_texts(lst_corpus)<br>dic_vocabulary = tokenizer.word_index</span><span id="2793" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## create sequence</strong><br>lst_text2seq= tokenizer.texts_to_sequences(lst_corpus)</span><span id="495c" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## padding sequence</strong><br>X_train = kprocessing.sequence.<strong class="oc hs">pad_sequences</strong>(lst_text2seq, <br>                    maxlen=15, padding="post", truncating="post")</span></pre><p id="5ade" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">The feature matrix <em class="ls">X_train</em> has a shape of 34,265 x 15 (Number of sequences x Sequences max length). Let’s visualize it:</p><pre class="lu lv lw lx ly nz oa ob"><span id="4fcd" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph="">sns.heatmap(X_train==0, vmin=0, vmax=1, cbar=False)<br>plt.show()</span></pre><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl pt"><div class="me s at mf"><div class="pu mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_tQ588zm4i96xfEBCfiUpzQ.png" width="700" height="233" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="700" height="233" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_tQ588zm4i96xfEBCfiUpzQ(1).png" srcset="https://miro.medium.com/max/552/1*tQ588zm4i96xfEBCfiUpzQ.png 276w, https://miro.medium.com/max/1104/1*tQ588zm4i96xfEBCfiUpzQ.png 552w, https://miro.medium.com/max/1280/1*tQ588zm4i96xfEBCfiUpzQ.png 640w, https://miro.medium.com/max/1400/1*tQ588zm4i96xfEBCfiUpzQ.png 700w" sizes="700px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1400/1*tQ588zm4i96xfEBCfiUpzQ.png" width="700" height="233" srcSet="https://miro.medium.com/max/552/1*tQ588zm4i96xfEBCfiUpzQ.png 276w, https://miro.medium.com/max/1104/1*tQ588zm4i96xfEBCfiUpzQ.png 552w, https://miro.medium.com/max/1280/1*tQ588zm4i96xfEBCfiUpzQ.png 640w, https://miro.medium.com/max/1400/1*tQ588zm4i96xfEBCfiUpzQ.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="ox oy gm gk gl oz pa bf b bg bh es" data-selectable-paragraph="">Feature matrix (34,265 x 15)</figcaption></figure><p id="23eb" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">Every text in the corpus is now an id sequence with length 15. For instance, if a text had 10 tokens in it, then the sequence is composed of 10 ids + 5 0s, which is the padding element (while the id for word not in the vocabulary is 1). Let’s print how a text from the train set has been transformed into a sequence with the padding and the vocabulary.</p><pre class="lu lv lw lx ly nz oa ob"><span id="8b50" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph="">i = 0<br><br><strong class="oc hs">## list of text: ["I like this", ...]</strong><br>len_txt = len(dtf_train["text_clean"].iloc[i].split())<br>print("from: ", dtf_train["text_clean"].iloc[i], "| len:", len_txt)<br><br><strong class="oc hs">## sequence of token ids: [[1, 2, 3], ...]</strong><br>len_tokens = len(X_train[i])<br>print("to: ", X_train[i], "| len:", len(X_train[i]))<br><br><strong class="oc hs">## vocabulary: {"I":1, "like":2, "this":3, ...}</strong><br>print("check: ", dtf_train["text_clean"].iloc[i].split()[0], <br>      " -- idx in vocabulary --&gt;", <br>      dic_vocabulary[dtf_train["text_clean"].iloc[i].split()[0]])<br><br>print("vocabulary: ", dict(list(dic_vocabulary.items())[0:5]), "... (padding element, 0)")</span></pre><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl pv"><div class="me s at mf"><div class="pw mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_vhJWO3cTN2jfrg2upX2kGQ.png" width="700" height="78" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="700" height="78" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_vhJWO3cTN2jfrg2upX2kGQ(1).png" srcset="https://miro.medium.com/max/552/1*vhJWO3cTN2jfrg2upX2kGQ.png 276w, https://miro.medium.com/max/1104/1*vhJWO3cTN2jfrg2upX2kGQ.png 552w, https://miro.medium.com/max/1280/1*vhJWO3cTN2jfrg2upX2kGQ.png 640w, https://miro.medium.com/max/1400/1*vhJWO3cTN2jfrg2upX2kGQ.png 700w" sizes="700px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1400/1*vhJWO3cTN2jfrg2upX2kGQ.png" width="700" height="78" srcSet="https://miro.medium.com/max/552/1*vhJWO3cTN2jfrg2upX2kGQ.png 276w, https://miro.medium.com/max/1104/1*vhJWO3cTN2jfrg2upX2kGQ.png 552w, https://miro.medium.com/max/1280/1*vhJWO3cTN2jfrg2upX2kGQ.png 640w, https://miro.medium.com/max/1400/1*vhJWO3cTN2jfrg2upX2kGQ.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="a07a" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">Before moving on, don’t forget to do the same feature engineering on the test set as well:</p><pre class="lu lv lw lx ly nz oa ob"><span id="2ebc" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph="">corpus = dtf_test["<strong class="oc hs">text_clean</strong>"]</span><span id="220a" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs"><br>## create list of n-grams</strong><br>lst_corpus = []<br>for string in corpus:<br>    lst_words = string.split()<br>    lst_grams = [" ".join(lst_words[i:i+1]) for i in range(0, <br>                 len(lst_words), 1)]<br>    lst_corpus.append(lst_grams)<br>    </span><span id="791a" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## detect common bigrams and trigrams using the fitted detectors</strong><br>lst_corpus = list(bigrams_detector[lst_corpus])<br>lst_corpus = list(trigrams_detector[lst_corpus])<br></span><span id="3d90" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## text to sequence with the fitted tokenizer</strong><br>lst_text2seq = tokenizer.texts_to_sequences(lst_corpus)</span><span id="51fd" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><br><strong class="oc hs">## padding sequence</strong><br>X_test = kprocessing.sequence.<strong class="oc hs">pad_sequences</strong>(lst_text2seq, maxlen=15,<br>             padding="post", truncating="post")</span></pre><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl px"><div class="me s at mf"><div class="py mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_H6ZZAT-9tmu8PcaY74Un6w.png" width="700" height="234" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="700" height="234" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_H6ZZAT-9tmu8PcaY74Un6w(1).png" srcset="https://miro.medium.com/max/552/1*H6ZZAT-9tmu8PcaY74Un6w.png 276w, https://miro.medium.com/max/1104/1*H6ZZAT-9tmu8PcaY74Un6w.png 552w, https://miro.medium.com/max/1280/1*H6ZZAT-9tmu8PcaY74Un6w.png 640w, https://miro.medium.com/max/1400/1*H6ZZAT-9tmu8PcaY74Un6w.png 700w" sizes="700px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1400/1*H6ZZAT-9tmu8PcaY74Un6w.png" width="700" height="234" srcSet="https://miro.medium.com/max/552/1*H6ZZAT-9tmu8PcaY74Un6w.png 276w, https://miro.medium.com/max/1104/1*H6ZZAT-9tmu8PcaY74Un6w.png 552w, https://miro.medium.com/max/1280/1*H6ZZAT-9tmu8PcaY74Un6w.png 640w, https://miro.medium.com/max/1400/1*H6ZZAT-9tmu8PcaY74Un6w.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="ox oy gm gk gl oz pa bf b bg bh es" data-selectable-paragraph="">X_test (14,697 x 15)</figcaption></figure><p id="a0f8" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">We’ve got our <em class="ls">X_train</em> and <em class="ls">X_test</em>, now we need to create the <strong class="ky hs">matrix of embedding</strong> that will be used as a weight matrix in the neural network classifier.</p><pre class="lu lv lw lx ly nz oa ob"><span id="319f" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph=""><strong class="oc hs">## start the matrix (length of vocabulary x vector size) with all 0s</strong><br>embeddings = np.zeros((len(dic_vocabulary)+1, 300))</span><span id="b807" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph="">for word,idx in dic_vocabulary.items():<br>    <strong class="oc hs">## update the row with vector</strong><br>    try:<br>        embeddings[idx] =  nlp[word]<br>    <strong class="oc hs">## if word not in model then skip and the row stays all 0s</strong><br>    except:<br>        pass</span></pre><p id="2aa4" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">That code generates a matrix of shape 22,338 x 300 (Length of vocabulary extracted from the corpus x Vector size). It can be navigated by word id, which can be obtained from the vocabulary.</p><pre class="lu lv lw lx ly nz oa ob"><span id="d103" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph="">word = "data"</span><span id="80ad" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph="">print("dic[word]:", dic_vocabulary[word], "|idx")<br>print("embeddings[idx]:", embeddings[dic_vocabulary[word]].shape, <br>      "|vector")</span></pre><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div class="gk gl pz"><div class="me s at mf"><div class="qa mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_EWLnzZ0abpnhH1zR8jq5PQ.png" width="490" height="52" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="490" height="52" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_EWLnzZ0abpnhH1zR8jq5PQ(1).png" srcset="https://miro.medium.com/max/552/1*EWLnzZ0abpnhH1zR8jq5PQ.png 276w, https://miro.medium.com/max/980/1*EWLnzZ0abpnhH1zR8jq5PQ.png 490w" sizes="490px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/980/1*EWLnzZ0abpnhH1zR8jq5PQ.png" width="490" height="52" srcSet="https://miro.medium.com/max/552/1*EWLnzZ0abpnhH1zR8jq5PQ.png 276w, https://miro.medium.com/max/980/1*EWLnzZ0abpnhH1zR8jq5PQ.png 490w" sizes="490px" role="presentation"/></noscript></div></div></div></figure><p id="f4fc" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">It’s finally time to build a <strong class="ky hs">deep learning model</strong>. I’m going to use the embedding matrix in the first Embedding layer of the neural network that I will build and train to classify the news. Each id in the input sequence will be used as the index to access the embedding matrix. The output of this Embedding layer will be a 2D matrix with a word vector for each word id in the input sequence (Sequence length x Vector size). Let’s use the sentence “<em class="ls">I like this article</em>” as an example:</p><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl qb"><div class="me s at mf"><div class="qc mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_KvBp0xzRThA7qTXACT4A-g.png" width="700" height="417" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="700" height="417" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_KvBp0xzRThA7qTXACT4A-g(1).png" srcset="https://miro.medium.com/max/552/1*KvBp0xzRThA7qTXACT4A-g.png 276w, https://miro.medium.com/max/1104/1*KvBp0xzRThA7qTXACT4A-g.png 552w, https://miro.medium.com/max/1280/1*KvBp0xzRThA7qTXACT4A-g.png 640w, https://miro.medium.com/max/1400/1*KvBp0xzRThA7qTXACT4A-g.png 700w" sizes="700px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1400/1*KvBp0xzRThA7qTXACT4A-g.png" width="700" height="417" srcSet="https://miro.medium.com/max/552/1*KvBp0xzRThA7qTXACT4A-g.png 276w, https://miro.medium.com/max/1104/1*KvBp0xzRThA7qTXACT4A-g.png 552w, https://miro.medium.com/max/1280/1*KvBp0xzRThA7qTXACT4A-g.png 640w, https://miro.medium.com/max/1400/1*KvBp0xzRThA7qTXACT4A-g.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="e0c6" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">My neural network shall be structured as follows:</p><ul class=""><li id="e79b" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr nk nl nm io" data-selectable-paragraph="">an Embedding layer that takes the sequences as input and the word vectors as weights, just as described before.</li><li id="b13f" class="kw kx hr ky b iq nn la lb it no ld le lf np lh li lj nq ll lm ln nr lp lq lr nk nl nm io" data-selectable-paragraph="">A simple Attention layer that won’t affect the predictions but it’s going to capture the weights of each instance and allow us to build a nice explainer (it isn't necessary for the predictions, just for the explainability, so you can skip it). The Attention mechanism was presented in <a href="https://arxiv.org/abs/1409.0473" class="cw mp" rel="noopener nofollow">this paper</a> (2014) as a solution to the problem of the sequence models (i.e. LSTM) to understand what parts of a long text are actually relevant.</li><li id="1d19" class="kw kx hr ky b iq nn la lb it no ld le lf np lh li lj nq ll lm ln nr lp lq lr nk nl nm io" data-selectable-paragraph="">Two layers of Bidirectional LSTM to model the order of words in a sequence in both directions.</li><li id="e566" class="kw kx hr ky b iq nn la lb it no ld le lf np lh li lj nq ll lm ln nr lp lq lr nk nl nm io" data-selectable-paragraph="">Two final dense layers that will predict the probability of each news category.</li></ul><pre class="lu lv lw lx ly nz oa ob"><span id="3c10" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph=""><strong class="oc hs">## code attention layer</strong><br>def <strong class="oc hs">attention_layer</strong>(inputs, neurons):<br>    x = layers.<strong class="oc hs">Permute</strong>((2,1))(inputs)<br>    x = layers.<strong class="oc hs">Dense</strong>(neurons, activation="softmax")(x)<br>    x = layers.<strong class="oc hs">Permute</strong>((2,1), name="<strong class="oc hs">attention</strong>")(x)<br>    x = layers.<strong class="oc hs">multiply</strong>([inputs, x])<br>    return x</span><span id="7930" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs"><br>## input</strong><br>x_in = layers.<strong class="oc hs">Input</strong>(shape=(15,))</span><span id="d8c6" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## embedding</strong><br>x = layers.<strong class="oc hs">Embedding</strong>(input_dim=embeddings.shape[0],  <br>                     output_dim=embeddings.shape[1], <br>                     weights=[embeddings],<br>                     input_length=15, trainable=False)(x_in)</span><span id="5041" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## apply attention</strong><br>x = attention_layer(x, neurons=15)</span><span id="70c4" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## 2 layers of bidirectional lstm</strong><br>x = layers.<strong class="oc hs">Bidirectional</strong>(layers.<strong class="oc hs">LSTM</strong>(units=15, dropout=0.2, <br>                         return_sequences=True))(x)<br>x = layers.<strong class="oc hs">Bidirectional</strong>(layers.<strong class="oc hs">LSTM</strong>(units=15, dropout=0.2))(x)</span><span id="9ebc" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## final dense layers</strong><br>x = layers.<strong class="oc hs">Dense</strong>(64, activation='relu')(x)<br>y_out = layers.<strong class="oc hs">Dense</strong>(3, activation='softmax')(x)</span><span id="34a7" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## compile</strong><br>model = models.<strong class="oc hs">Model</strong>(x_in, y_out)<br>model.compile(loss='sparse_categorical_crossentropy',<br>              optimizer='adam', metrics=['accuracy'])<br><br>model.summary()</span></pre><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl qd"><div class="me s at mf"><div class="qe mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_Dfu-2hqEMaBe6YHx1C71Uw.png" width="700" height="427" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="700" height="427" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_Dfu-2hqEMaBe6YHx1C71Uw(1).png" srcset="https://miro.medium.com/max/552/1*Dfu-2hqEMaBe6YHx1C71Uw.png 276w, https://miro.medium.com/max/1104/1*Dfu-2hqEMaBe6YHx1C71Uw.png 552w, https://miro.medium.com/max/1280/1*Dfu-2hqEMaBe6YHx1C71Uw.png 640w, https://miro.medium.com/max/1400/1*Dfu-2hqEMaBe6YHx1C71Uw.png 700w" sizes="700px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1400/1*Dfu-2hqEMaBe6YHx1C71Uw.png" width="700" height="427" srcSet="https://miro.medium.com/max/552/1*Dfu-2hqEMaBe6YHx1C71Uw.png 276w, https://miro.medium.com/max/1104/1*Dfu-2hqEMaBe6YHx1C71Uw.png 552w, https://miro.medium.com/max/1280/1*Dfu-2hqEMaBe6YHx1C71Uw.png 640w, https://miro.medium.com/max/1400/1*Dfu-2hqEMaBe6YHx1C71Uw.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="9280" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">Now we can train the model and check the performance on a subset of the training set used for validation before testing it on the actual test set.</p><pre class="lu lv lw lx ly nz oa ob"><span id="523e" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph=""><strong class="oc hs">## encode y</strong><br>dic_y_mapping = {n:label for n,label in <br>                 enumerate(np.unique(y_train))}<br>inverse_dic = {v:k for k,v in dic_y_mapping.items()}<br>y_train = np.array([inverse_dic[y] for y in y_train])</span><span id="7971" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## train</strong><br>training = model.fit(x=X_train, y=y_train, batch_size=256, <br>                     epochs=10, shuffle=True, verbose=0, <br>                     validation_split=0.3)</span><span id="40ef" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## plot loss and accuracy</strong><br>metrics = [k for k in training.history.keys() if ("loss" not in k) and ("val" not in k)]<br>fig, ax = plt.subplots(nrows=1, ncols=2, sharey=True)</span><span id="be93" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph="">ax[0].set(title="Training")<br>ax11 = ax[0].twinx()<br>ax[0].plot(training.history['loss'], color='black')<br>ax[0].set_xlabel('Epochs')<br>ax[0].set_ylabel('Loss', color='black')<br>for metric in metrics:<br>    ax11.plot(training.history[metric], label=metric)<br>ax11.set_ylabel("Score", color='steelblue')<br>ax11.legend()</span><span id="19f7" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph="">ax[1].set(title="Validation")<br>ax22 = ax[1].twinx()<br>ax[1].plot(training.history['val_loss'], color='black')<br>ax[1].set_xlabel('Epochs')<br>ax[1].set_ylabel('Loss', color='black')<br>for metric in metrics:<br>     ax22.plot(training.history['val_'+metric], label=metric)<br>ax22.set_ylabel("Score", color="steelblue")<br>plt.show()</span></pre><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl qf"><div class="me s at mf"><div class="qg mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_MdYjdeOju4ez8gcAc-v8qA.png" width="700" height="166" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="700" height="166" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_MdYjdeOju4ez8gcAc-v8qA(1).png" srcset="https://miro.medium.com/max/552/1*MdYjdeOju4ez8gcAc-v8qA.png 276w, https://miro.medium.com/max/1104/1*MdYjdeOju4ez8gcAc-v8qA.png 552w, https://miro.medium.com/max/1280/1*MdYjdeOju4ez8gcAc-v8qA.png 640w, https://miro.medium.com/max/1400/1*MdYjdeOju4ez8gcAc-v8qA.png 700w" sizes="700px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1400/1*MdYjdeOju4ez8gcAc-v8qA.png" width="700" height="166" srcSet="https://miro.medium.com/max/552/1*MdYjdeOju4ez8gcAc-v8qA.png 276w, https://miro.medium.com/max/1104/1*MdYjdeOju4ez8gcAc-v8qA.png 552w, https://miro.medium.com/max/1280/1*MdYjdeOju4ez8gcAc-v8qA.png 640w, https://miro.medium.com/max/1400/1*MdYjdeOju4ez8gcAc-v8qA.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="759d" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">Nice! In some epochs, the accuracy reached 0.89. In order to complete the <strong class="ky hs">evaluation</strong> of the Word Embedding model, let’s predict the test set and compare the same metrics used before (code for metrics is the same as before).</p><pre class="lu lv lw lx ly nz oa ob"><span id="f7a4" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph=""><strong class="oc hs">## test</strong><br>predicted_prob = model.predict(X_test)<br>predicted = [dic_y_mapping[np.argmax(pred)] for pred in <br>             predicted_prob]</span></pre><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl ph"><div class="me s at mf"><div class="qh mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_a39MMTNXnDaFOKFur2Z7xQ.png" width="700" height="676" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="700" height="676" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_a39MMTNXnDaFOKFur2Z7xQ(1).png" srcset="https://miro.medium.com/max/552/1*a39MMTNXnDaFOKFur2Z7xQ.png 276w, https://miro.medium.com/max/1104/1*a39MMTNXnDaFOKFur2Z7xQ.png 552w, https://miro.medium.com/max/1280/1*a39MMTNXnDaFOKFur2Z7xQ.png 640w, https://miro.medium.com/max/1400/1*a39MMTNXnDaFOKFur2Z7xQ.png 700w" sizes="700px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1400/1*a39MMTNXnDaFOKFur2Z7xQ.png" width="700" height="676" srcSet="https://miro.medium.com/max/552/1*a39MMTNXnDaFOKFur2Z7xQ.png 276w, https://miro.medium.com/max/1104/1*a39MMTNXnDaFOKFur2Z7xQ.png 552w, https://miro.medium.com/max/1280/1*a39MMTNXnDaFOKFur2Z7xQ.png 640w, https://miro.medium.com/max/1400/1*a39MMTNXnDaFOKFur2Z7xQ.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="8636" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">The model performs as good as the previous one, in fact, it also struggles to classify Tech news.</p><p id="a333" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">But is it <strong class="ky hs">explainable </strong>as well? Yes, it is! I put an Attention layer in the neural network to extract the weights of each word and understand how much those contributed to classify an instance. So I’ll try to use Attention weights to build an explainer (similar to the one seen in the previous section):</p><pre class="lu lv lw lx ly nz oa ob"><span id="855e" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph=""><strong class="oc hs">## select observation<br></strong>i = 0<br>txt_instance = dtf_test["<strong class="oc hs">text</strong>"].iloc[i]</span><span id="6749" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## check true value and predicted value</strong><br>print("True:", y_test[i], "--&gt; Pred:", predicted[i], "| Prob:", round(np.max(predicted_prob[i]),2))</span><span id="81ae" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs"><br>## show explanation<br>### 1. preprocess input<br></strong>lst_corpus = []<br>for string in [re.sub(r'[^\w\s]','', txt_instance.lower().strip())]:<br>    lst_words = string.split()<br>    lst_grams = [" ".join(lst_words[i:i+1]) for i in range(0, <br>                 len(lst_words), 1)]<br>    lst_corpus.append(lst_grams)<br>lst_corpus = list(bigrams_detector[lst_corpus])<br>lst_corpus = list(trigrams_detector[lst_corpus])<br>X_instance = kprocessing.sequence.pad_sequences(<br>              tokenizer.texts_to_sequences(corpus), maxlen=15, <br>              padding="post", truncating="post")</span><span id="932d" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">### 2. get attention weights</strong><br>layer = [layer for layer in model.layers if "<strong class="oc hs">attention</strong>" in <br>         layer.name][0]<br>func = K.function([model.input], [layer.output])<br>weights = func(X_instance)[0]<br>weights = np.mean(weights, axis=2).flatten()</span><span id="0d81" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">### 3. rescale weights, remove null vector, map word-weight</strong><br>weights = preprocessing.MinMaxScaler(feature_range=(0,1)).fit_transform(np.array(weights).reshape(-1,1)).reshape(-1)<br>weights = [weights[n] for n,idx in enumerate(X_instance[0]) if idx <br>           != 0]<br>dic_word_weigth = {word:weights[n] for n,word in <br>                   enumerate(lst_corpus[0]) if word in <br>                   tokenizer.word_index.keys()}</span><span id="6dcb" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">### 4. barplot</strong><br>if len(dic_word_weigth) &gt; 0:<br>   dtf = pd.DataFrame.from_dict(dic_word_weigth, orient='index', <br>                                columns=["score"])<br>   dtf.sort_values(by="score", <br>           ascending=True).tail(top).plot(kind="barh", <br>           legend=False).grid(axis='x')<br>   plt.show()<br>else:<br>   print("--- No word recognized ---")</span><span id="2f30" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">### 5. produce html visualization</strong><br>text = []<br>for word in lst_corpus[0]:<br>    weight = dic_word_weigth.get(word)<br>    if weight is not None:<br>         text.append('&lt;b&gt;&lt;span style="background-color:rgba(100,149,237,' + str(weight) + ');"&gt;' + word + '&lt;/span&gt;&lt;/b&gt;')<br>    else:<br>         text.append(word)<br>text = ' '.join(text)</span><span id="ce30" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">### 6. visualize on notebook<br></strong>print("<strong class="oc hs">\033</strong>[1m"+"Text with highlighted words")<br>from IPython.core.display import display, HTML<br>display(HTML(text))</span></pre><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div class="gk gl qi"><div class="me s at mf"><div class="qj mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_21iP8FD4XOBDn3XS5LKiJA.png" width="594" height="38" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="594" height="38" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_21iP8FD4XOBDn3XS5LKiJA(1).png" srcset="https://miro.medium.com/max/552/1*21iP8FD4XOBDn3XS5LKiJA.png 276w, https://miro.medium.com/max/1104/1*21iP8FD4XOBDn3XS5LKiJA.png 552w, https://miro.medium.com/max/1188/1*21iP8FD4XOBDn3XS5LKiJA.png 594w" sizes="594px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1188/1*21iP8FD4XOBDn3XS5LKiJA.png" width="594" height="38" srcSet="https://miro.medium.com/max/552/1*21iP8FD4XOBDn3XS5LKiJA.png 276w, https://miro.medium.com/max/1104/1*21iP8FD4XOBDn3XS5LKiJA.png 552w, https://miro.medium.com/max/1188/1*21iP8FD4XOBDn3XS5LKiJA.png 594w" sizes="594px" role="presentation"/></noscript></div></div></div></figure><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl qk"><div class="me s at mf"><div class="ql mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_CrFeNLHgDzBxAmgGiXR8lg.png" width="700" height="341" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="700" height="341" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_CrFeNLHgDzBxAmgGiXR8lg(1).png" srcset="https://miro.medium.com/max/552/1*CrFeNLHgDzBxAmgGiXR8lg.png 276w, https://miro.medium.com/max/1104/1*CrFeNLHgDzBxAmgGiXR8lg.png 552w, https://miro.medium.com/max/1280/1*CrFeNLHgDzBxAmgGiXR8lg.png 640w, https://miro.medium.com/max/1400/1*CrFeNLHgDzBxAmgGiXR8lg.png 700w" sizes="700px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1400/1*CrFeNLHgDzBxAmgGiXR8lg.png" width="700" height="341" srcSet="https://miro.medium.com/max/552/1*CrFeNLHgDzBxAmgGiXR8lg.png 276w, https://miro.medium.com/max/1104/1*CrFeNLHgDzBxAmgGiXR8lg.png 552w, https://miro.medium.com/max/1280/1*CrFeNLHgDzBxAmgGiXR8lg.png 640w, https://miro.medium.com/max/1400/1*CrFeNLHgDzBxAmgGiXR8lg.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="3f7e" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">Just like before, the words “<em class="ls">clinton</em>” and “<em class="ls">gop</em>” activated the neurons of the model, but this time also “<em class="ls">high</em>” and “<em class="ls">benghazi</em>” have been considered slightly relevant for the prediction.</p><h2 id="8831" class="kg kh hr bf ki kj kk is kl km kn iv ko iw kp iy kq iz kr jb ks jc kt je ku kv io" data-selectable-paragraph="">Language Models</h2><p id="0455" class="kw kx hr ky b iq kz la lb it lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr gx io" data-selectable-paragraph=""><a href="https://en.wikipedia.org/wiki/Language_model" class="cw mp" rel="noopener nofollow">Language Models</a>, or Contextualized/Dynamic Word Embeddings<strong class="ky hs">, </strong>overcome the biggest limitation of the classic Word Embedding approach: polysemy disambiguation, a word with different meanings (e.g. “ <em class="ls">bank</em>” or “<em class="ls">stick</em>”) is identified by just one vector. One of the first popular ones was ELMO (2018), which doesn’t apply a fixed embedding but, using a bidirectional LSTM, looks at the entire sentence and then assigns an embedding to each word.</p><p id="9efe" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">Enter Transformers: a new modeling technique presented by Google’s paper <a href="https://arxiv.org/abs/1706.03762" class="cw mp" rel="noopener nofollow"><em class="ls">Attention is All You Need</em></a><em class="ls"> </em>(2017)<em class="ls"> </em>in which it was demonstrated that sequence models (like LSTM) can be totally replaced by Attention mechanisms, even obtaining better performances.</p><p id="0e55" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">Google’s <a href="https://en.wikipedia.org/wiki/BERT_(language_model)" class="cw mp" rel="noopener nofollow"><strong class="ky hs">BERT</strong></a><strong class="ky hs"> </strong>(Bidirectional Encoder Representations from Transformers, 2018) combines ELMO context embedding and several Transformers, plus it’s bidirectional (which was a big novelty for Transformers). The vector BERT assigns to a word is a function of the entire sentence, therefore, a word can have different vectors based on the contexts. Let’s try it using <em class="ls">transformers</em>:</p><pre class="lu lv lw lx ly nz oa ob"><span id="0f5f" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph="">txt = <strong class="oc hs">"bank river"</strong></span><span id="8739" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## bert tokenizer</strong><br>tokenizer = transformers.<strong class="oc hs">BertTokenizer</strong>.<strong class="oc hs">from_pretrained</strong>('bert-base-uncased', do_lower_case=True)</span><span id="b7ff" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## bert model</strong><br>nlp = transformers.<strong class="oc hs">TFBertModel</strong>.<strong class="oc hs">from_pretrained</strong>('bert-base-uncased')</span><span id="20f4" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## return hidden layer with embeddings</strong><br>input_ids = np.array(tokenizer.encode(txt))[None,:]  <br>embedding = nlp(input_ids)<strong class="oc hs"><br></strong>embedding[0][0]</span></pre><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl qm"><div class="me s at mf"><div class="qn mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_f7-l1PPDu5Q6rgUBzgz12A.png" width="700" height="129" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="700" height="129" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_f7-l1PPDu5Q6rgUBzgz12A(1).png" srcset="https://miro.medium.com/max/552/1*f7-l1PPDu5Q6rgUBzgz12A.png 276w, https://miro.medium.com/max/1104/1*f7-l1PPDu5Q6rgUBzgz12A.png 552w, https://miro.medium.com/max/1280/1*f7-l1PPDu5Q6rgUBzgz12A.png 640w, https://miro.medium.com/max/1400/1*f7-l1PPDu5Q6rgUBzgz12A.png 700w" sizes="700px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1400/1*f7-l1PPDu5Q6rgUBzgz12A.png" width="700" height="129" srcSet="https://miro.medium.com/max/552/1*f7-l1PPDu5Q6rgUBzgz12A.png 276w, https://miro.medium.com/max/1104/1*f7-l1PPDu5Q6rgUBzgz12A.png 552w, https://miro.medium.com/max/1280/1*f7-l1PPDu5Q6rgUBzgz12A.png 640w, https://miro.medium.com/max/1400/1*f7-l1PPDu5Q6rgUBzgz12A.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="cfbd" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">If we change the input text into “<em class="ls">bank money</em>”, we get this instead:</p><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl qo"><div class="me s at mf"><div class="qp mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_V2iWL5kNy9WBzIFPfoVZYQ.png" width="700" height="122" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="700" height="122" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_V2iWL5kNy9WBzIFPfoVZYQ(1).png" srcset="https://miro.medium.com/max/552/1*V2iWL5kNy9WBzIFPfoVZYQ.png 276w, https://miro.medium.com/max/1104/1*V2iWL5kNy9WBzIFPfoVZYQ.png 552w, https://miro.medium.com/max/1280/1*V2iWL5kNy9WBzIFPfoVZYQ.png 640w, https://miro.medium.com/max/1400/1*V2iWL5kNy9WBzIFPfoVZYQ.png 700w" sizes="700px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1400/1*V2iWL5kNy9WBzIFPfoVZYQ.png" width="700" height="122" srcSet="https://miro.medium.com/max/552/1*V2iWL5kNy9WBzIFPfoVZYQ.png 276w, https://miro.medium.com/max/1104/1*V2iWL5kNy9WBzIFPfoVZYQ.png 552w, https://miro.medium.com/max/1280/1*V2iWL5kNy9WBzIFPfoVZYQ.png 640w, https://miro.medium.com/max/1400/1*V2iWL5kNy9WBzIFPfoVZYQ.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="bd4c" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">In order to complete a text classification task, you can use BERT in 3 different ways:</p><ul class=""><li id="31c1" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr nk nl nm io" data-selectable-paragraph="">train it all from scratches and use it as classifier.</li><li id="32ee" class="kw kx hr ky b iq nn la lb it no ld le lf np lh li lj nq ll lm ln nr lp lq lr nk nl nm io" data-selectable-paragraph="">Extract the word embeddings and use them in an embedding layer (like I did with Word2Vec).</li><li id="4e55" class="kw kx hr ky b iq nn la lb it no ld le lf np lh li lj nq ll lm ln nr lp lq lr nk nl nm io" data-selectable-paragraph="">Fine-tuning the pre-trained model (transfer learning).</li></ul><p id="8eb7" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">I’m going with the latter and do transfer learning from a pre-trained lighter version of BERT, called <a href="https://huggingface.co/transformers/model_doc/distilbert.html" class="cw mp" rel="noopener nofollow">Distil-BERT</a> (66 million of parameters instead of 110 million!).</p><pre class="lu lv lw lx ly nz oa ob"><span id="ce0e" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph=""><strong class="oc hs">## distil-bert tokenizer</strong><br>tokenizer = transformers.<strong class="oc hs">AutoTokenizer</strong>.<strong class="oc hs">from_pretrained</strong>('distilbert-base-uncased', do_lower_case=True)</span></pre><p id="9ddd" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">As usual, before fitting the model there is some <strong class="ky hs">Feature Engineering</strong> to do, but this time it’s gonna be a little trickier. To give an illustration of what I’m going to do, let’s take as an example our beloved sentence “<em class="ls">I like this article</em>”, which has to be transformed into 3 vectors (Ids, Mask, Segment):</p><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl qq"><div class="me s at mf"><div class="qr mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_rENCe-2FhlIBIfUstVHRhA.png" width="700" height="230" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="700" height="230" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_rENCe-2FhlIBIfUstVHRhA(1).png" srcset="https://miro.medium.com/max/552/1*rENCe-2FhlIBIfUstVHRhA.png 276w, https://miro.medium.com/max/1104/1*rENCe-2FhlIBIfUstVHRhA.png 552w, https://miro.medium.com/max/1280/1*rENCe-2FhlIBIfUstVHRhA.png 640w, https://miro.medium.com/max/1400/1*rENCe-2FhlIBIfUstVHRhA.png 700w" sizes="700px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1400/1*rENCe-2FhlIBIfUstVHRhA.png" width="700" height="230" srcSet="https://miro.medium.com/max/552/1*rENCe-2FhlIBIfUstVHRhA.png 276w, https://miro.medium.com/max/1104/1*rENCe-2FhlIBIfUstVHRhA.png 552w, https://miro.medium.com/max/1280/1*rENCe-2FhlIBIfUstVHRhA.png 640w, https://miro.medium.com/max/1400/1*rENCe-2FhlIBIfUstVHRhA.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div><figcaption class="ox oy gm gk gl oz pa bf b bg bh es" data-selectable-paragraph="">Shape: 3 x Sequence length</figcaption></figure><p id="c6b3" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">First of all, we need to select the sequence max length. This time I’m gonna choose a much larger number (i.e. 50) because BERT splits unknown words into sub-tokens until it finds a known unigrams. For example, if a made-up word like “<em class="ls">zzdata</em>” is given, BERT would split it into [“<em class="ls">z</em>”, “<em class="ls">##z</em>”, “<em class="ls">##data</em>”]. Moreover, we have to insert special tokens into the input text, then generate masks and segments. Finally, put all together in a tensor to get the feature matrix that will have the shape of 3 (ids, masks, segments) x Number of documents in the corpus x Sequence length.</p><p id="37c8" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">Please note that I’m using the raw text as corpus (so far I’ve been using the <em class="ls">clean_text </em>column).</p><pre class="lu lv lw lx ly nz oa ob"><span id="19fd" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph="">corpus = dtf_train["<strong class="oc hs">text</strong>"]<br>maxlen = 50</span><span id="f2b8" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs"><br>## add special tokens</strong><br>maxqnans = np.int((maxlen-20)/2)<br>corpus_tokenized = ["[CLS] "+<br>             " ".join(tokenizer.tokenize(re.sub(r'[^\w\s]+|\n', '', <br>             str(txt).lower().strip()))[:maxqnans])+<br>             " [SEP] " for txt in corpus]<br><br><strong class="oc hs">## generate masks</strong><br>masks = [[1]*len(txt.split(" ")) + [0]*(maxlen - len(<br>           txt.split(" "))) for txt in corpus_tokenized]<br>    <br><strong class="oc hs">## padding</strong><br>txt2seq = [txt + " [PAD]"*(maxlen-len(txt.split(" "))) if len(txt.split(" ")) != maxlen else txt for txt in corpus_tokenized]<br>    <br><strong class="oc hs">## generate idx</strong><br>idx = [tokenizer.encode(seq.split(" ")) for seq in txt2seq]<br>    <br><strong class="oc hs">## generate segments</strong><br>segments = [] <br>for seq in txt2seq:<br>    temp, i = [], 0<br>    for token in seq.split(" "):<br>        temp.append(i)<br>        if token == "[SEP]":<br>             i += 1<br>    segments.append(temp)</span><span id="3481" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## feature matrix</strong><br>X_train = [np.asarray(idx, dtype='int32'), <br>           np.asarray(masks, dtype='int32'), <br>           np.asarray(segments, dtype='int32')]</span></pre><p id="5aa3" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">The feature matrix <em class="ls">X_train</em> has a shape of 3 x 34,265 x 50. We can check a random observation from the feature matrix:</p><pre class="lu lv lw lx ly nz oa ob"><span id="9e38" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph="">i = 0</span><span id="6e88" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph="">print("txt: ", dtf_train["text"].iloc[0])<br>print("tokenized:", [tokenizer.convert_ids_to_tokens(idx) for idx in X_train[0][i].tolist()])<br>print("idx: ", X_train[0][i])<br>print("mask: ", X_train[1][i])<br>print("segment: ", X_train[2][i])</span></pre><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl qs"><div class="me s at mf"><div class="qt mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_vRUkclqzuGDvmgu1VFs-5A.png" width="700" height="179" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="700" height="179" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_vRUkclqzuGDvmgu1VFs-5A(1).png" srcset="https://miro.medium.com/max/552/1*vRUkclqzuGDvmgu1VFs-5A.png 276w, https://miro.medium.com/max/1104/1*vRUkclqzuGDvmgu1VFs-5A.png 552w, https://miro.medium.com/max/1280/1*vRUkclqzuGDvmgu1VFs-5A.png 640w, https://miro.medium.com/max/1400/1*vRUkclqzuGDvmgu1VFs-5A.png 700w" sizes="700px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1400/1*vRUkclqzuGDvmgu1VFs-5A.png" width="700" height="179" srcSet="https://miro.medium.com/max/552/1*vRUkclqzuGDvmgu1VFs-5A.png 276w, https://miro.medium.com/max/1104/1*vRUkclqzuGDvmgu1VFs-5A.png 552w, https://miro.medium.com/max/1280/1*vRUkclqzuGDvmgu1VFs-5A.png 640w, https://miro.medium.com/max/1400/1*vRUkclqzuGDvmgu1VFs-5A.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="a1fc" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">You can take the same code and apply it to dtf_test[“text”] to get <em class="ls">X_test</em>.</p><p id="1d18" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">Now, I’m going to build the <strong class="ky hs">deep learning model with transfer learning</strong> from the pre-trained BERT. Basically, I’m going to summarize the output of BERT into one vector with Average Pooling and then add two final Dense layers to predict the probability of each news category.</p><p id="f6c7" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">If you want to use the original versions of BERT, here’s the code (remember to redo the feature engineering with the right tokenizer):</p><pre class="lu lv lw lx ly nz oa ob"><span id="ad59" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph=""><strong class="oc hs">## inputs</strong><br>idx = layers.<strong class="oc hs">Input</strong>((50), dtype="int32", name="input_idx")<br>masks = layers.<strong class="oc hs">Input</strong>((50), dtype="int32", name="input_masks")<br>segments = layers.Input((50), dtype="int32", name="input_segments")</span><span id="e1dc" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## pre-trained bert</strong><br>nlp = transformers.<strong class="oc hs">TFBertModel.from_pretrained</strong>("bert-base-uncased")<br>bert_out, _ = nlp([idx, masks, segments])</span><span id="454a" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## fine-tuning</strong><br>x = layers.<strong class="oc hs">GlobalAveragePooling1D</strong>()(bert_out)<br>x = layers.<strong class="oc hs">Dense</strong>(64, activation="relu")(x)<br>y_out = layers.<strong class="oc hs">Dense</strong>(len(np.unique(y_train)), <br>                     activation='softmax')(x)</span><span id="ee3d" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## compile</strong><br>model = models.<strong class="oc hs">Model</strong>([idx, masks, segments], y_out)</span><span id="aaca" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph="">for layer in model.layers[:4]:<br>    layer.trainable = False</span><span id="cd6e" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph="">model.compile(loss='sparse_categorical_crossentropy', <br>              optimizer='adam', metrics=['accuracy'])</span><span id="5e3e" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph="">model.summary()</span></pre><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl qu"><div class="me s at mf"><div class="qv mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_riJ2LlNVz_0MJvqAbYG3Bw.png" width="700" height="354" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="700" height="354" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_riJ2LlNVz_0MJvqAbYG3Bw(1).png" srcset="https://miro.medium.com/max/552/1*riJ2LlNVz_0MJvqAbYG3Bw.png 276w, https://miro.medium.com/max/1104/1*riJ2LlNVz_0MJvqAbYG3Bw.png 552w, https://miro.medium.com/max/1280/1*riJ2LlNVz_0MJvqAbYG3Bw.png 640w, https://miro.medium.com/max/1400/1*riJ2LlNVz_0MJvqAbYG3Bw.png 700w" sizes="700px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1400/1*riJ2LlNVz_0MJvqAbYG3Bw.png" width="700" height="354" srcSet="https://miro.medium.com/max/552/1*riJ2LlNVz_0MJvqAbYG3Bw.png 276w, https://miro.medium.com/max/1104/1*riJ2LlNVz_0MJvqAbYG3Bw.png 552w, https://miro.medium.com/max/1280/1*riJ2LlNVz_0MJvqAbYG3Bw.png 640w, https://miro.medium.com/max/1400/1*riJ2LlNVz_0MJvqAbYG3Bw.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="ee15" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">As I said, I’m going to use the lighter version instead, Distil-BERT:</p><pre class="lu lv lw lx ly nz oa ob"><span id="6928" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph=""><strong class="oc hs">## inputs</strong><br>idx = layers.<strong class="oc hs">Input</strong>((50), dtype="int32", name="input_idx")<br>masks = layers.<strong class="oc hs">Input</strong>((50), dtype="int32", name="input_masks")</span><span id="aff6" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## pre-trained bert with config</strong><br>config = transformers.DistilBertConfig(dropout=0.2, <br>           attention_dropout=0.2)<br>config.output_hidden_states = False</span><span id="d4a7" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph="">nlp = transformers.<strong class="oc hs">TFDistilBertModel.from_pretrained</strong>('distilbert-<br>                  base-uncased', config=config)<br>bert_out = nlp(idx, attention_mask=masks)[0]</span><span id="cc29" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## fine-tuning</strong><br>x = layers.<strong class="oc hs">GlobalAveragePooling1D</strong>()(bert_out)<br>x = layers.<strong class="oc hs">Dense</strong>(64, activation="relu")(x)<br>y_out = layers.<strong class="oc hs">Dense</strong>(len(np.unique(y_train)), <br>                     activation='softmax')(x)</span><span id="65af" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## compile</strong><br>model = models.<strong class="oc hs">Model</strong>([idx, masks], y_out)</span><span id="5a09" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph="">for layer in model.layers[:3]:<br>    layer.trainable = False</span><span id="46a9" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph="">model.compile(loss='sparse_categorical_crossentropy', <br>              optimizer='adam', metrics=['accuracy'])</span><span id="dcef" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph="">model.summary()</span></pre><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl on"><div class="me s at mf"><div class="qw mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_p5sc1h4l3DewgrHvZDrr9g.png" width="700" height="290" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="700" height="290" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_p5sc1h4l3DewgrHvZDrr9g(1).png" srcset="https://miro.medium.com/max/552/1*p5sc1h4l3DewgrHvZDrr9g.png 276w, https://miro.medium.com/max/1104/1*p5sc1h4l3DewgrHvZDrr9g.png 552w, https://miro.medium.com/max/1280/1*p5sc1h4l3DewgrHvZDrr9g.png 640w, https://miro.medium.com/max/1400/1*p5sc1h4l3DewgrHvZDrr9g.png 700w" sizes="700px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1400/1*p5sc1h4l3DewgrHvZDrr9g.png" width="700" height="290" srcSet="https://miro.medium.com/max/552/1*p5sc1h4l3DewgrHvZDrr9g.png 276w, https://miro.medium.com/max/1104/1*p5sc1h4l3DewgrHvZDrr9g.png 552w, https://miro.medium.com/max/1280/1*p5sc1h4l3DewgrHvZDrr9g.png 640w, https://miro.medium.com/max/1400/1*p5sc1h4l3DewgrHvZDrr9g.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="ed0a" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">Let’s <strong class="ky hs">train, test, evaluate</strong> this bad boy (code for evaluation is the same):</p><pre class="lu lv lw lx ly nz oa ob"><span id="6ad3" class="io kg kh hr oc b df od oe s of" data-selectable-paragraph=""><strong class="oc hs">## encode y</strong><br>dic_y_mapping = {n:label for n,label in <br>                 enumerate(np.unique(y_train))}<br>inverse_dic = {v:k for k,v in dic_y_mapping.items()}<br>y_train = np.array([inverse_dic[y] for y in y_train])</span><span id="04a7" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## train</strong><br>training = model.fit(x=X_train, y=y_train, batch_size=64, <br>                     epochs=1, shuffle=True, verbose=1, <br>                     validation_split=0.3)</span><span id="d959" class="io kg kh hr oc b df og oh oi oj ok oe s of" data-selectable-paragraph=""><strong class="oc hs">## test</strong><br>predicted_prob = model.predict(X_test)<br>predicted = [dic_y_mapping[np.argmax(pred)] for pred in <br>             predicted_prob]</span></pre><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl qx"><div class="me s at mf"><div class="qy mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_oS2xD1Y0IWPGDg8uVx2dPQ.png" width="700" height="42" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="700" height="42" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_oS2xD1Y0IWPGDg8uVx2dPQ(1).png" srcset="https://miro.medium.com/max/552/1*oS2xD1Y0IWPGDg8uVx2dPQ.png 276w, https://miro.medium.com/max/1104/1*oS2xD1Y0IWPGDg8uVx2dPQ.png 552w, https://miro.medium.com/max/1280/1*oS2xD1Y0IWPGDg8uVx2dPQ.png 640w, https://miro.medium.com/max/1400/1*oS2xD1Y0IWPGDg8uVx2dPQ.png 700w" sizes="700px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1400/1*oS2xD1Y0IWPGDg8uVx2dPQ.png" width="700" height="42" srcSet="https://miro.medium.com/max/552/1*oS2xD1Y0IWPGDg8uVx2dPQ.png 276w, https://miro.medium.com/max/1104/1*oS2xD1Y0IWPGDg8uVx2dPQ.png 552w, https://miro.medium.com/max/1280/1*oS2xD1Y0IWPGDg8uVx2dPQ.png 640w, https://miro.medium.com/max/1400/1*oS2xD1Y0IWPGDg8uVx2dPQ.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><figure class="lu lv lw lx ly hi gk gl paragraph-image"><div role="button" tabindex="0" class="hj hk at hl v hm"><div class="gk gl qz"><div class="me s at mf"><div class="ra mh s"><div class="dg lz fm fy fv ma v mb mc md"><img alt="" class="fm fy fv ma v mi mj fz acz" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_NsiKi7b0JGlCQPLpeVkftA.png" width="700" height="651" role="presentation"></div><img alt="" class="td we fm fy fv ma v c" width="700" height="651" role="presentation" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_NsiKi7b0JGlCQPLpeVkftA(1).png" srcset="https://miro.medium.com/max/552/1*NsiKi7b0JGlCQPLpeVkftA.png 276w, https://miro.medium.com/max/1104/1*NsiKi7b0JGlCQPLpeVkftA.png 552w, https://miro.medium.com/max/1280/1*NsiKi7b0JGlCQPLpeVkftA.png 640w, https://miro.medium.com/max/1400/1*NsiKi7b0JGlCQPLpeVkftA.png 700w" sizes="700px"><noscript><img alt="" class="fm fy fv ma v" src="https://miro.medium.com/max/1400/1*NsiKi7b0JGlCQPLpeVkftA.png" width="700" height="651" srcSet="https://miro.medium.com/max/552/1*NsiKi7b0JGlCQPLpeVkftA.png 276w, https://miro.medium.com/max/1104/1*NsiKi7b0JGlCQPLpeVkftA.png 552w, https://miro.medium.com/max/1280/1*NsiKi7b0JGlCQPLpeVkftA.png 640w, https://miro.medium.com/max/1400/1*NsiKi7b0JGlCQPLpeVkftA.png 700w" sizes="700px" role="presentation"/></noscript></div></div></div></div></figure><p id="1319" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">The performance of BERT is slightly better than the previous models, in fact, it can recognize more Tech news than the others.</p><h2 id="64fd" class="kg kh hr bf ki kj kk is kl km kn iv ko iw kp iy kq iz kr jb ks jc kt je ku kv io" data-selectable-paragraph="">Conclusion</h2><p id="9228" class="kw kx hr ky b iq kz la lb it lc ld le lf lg lh li lj lk ll lm ln lo lp lq lr gx io" data-selectable-paragraph="">This article has been a tutorial to demonstrate <strong class="ky hs">how to apply different NLP models to a multiclass classification use case</strong>. I compared 3 popular approaches: Bag-of-Words with Tf-Idf, Word Embedding with Word2Vec, and Language model with BERT. I went through Feature Engineering &amp; Selection, Model Design &amp; Testing, Evaluation &amp; Explainability, comparing the 3 models in each step (where possible).</p><p id="9ace" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">Please note that I haven’t covered explainability for BERT as I’m still working on that, but I will update this article as soon as I can. If you have any useful resources about that, feel free to contact me.</p></div></div></section><div class="n p cl ns nt nu" role="separator"><span class="nv jj db nw nx ny"></span><span class="nv jj db nw nx ny"></span><span class="nv jj db nw nx"></span></div><section class="gx gy gz ep ha"><div class="n p"><div class="au av aw ax ay hb ba v"><blockquote class="rb rc rd"><p id="3822" class="kw kx ls ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">This article is part of the series <strong class="ky hs">NLP with Python</strong>, see also:</p></blockquote><div class="hd he hf hg hh mq"><a target="_blank" rel="noopener" href="https://towardsdatascience.com/text-analysis-feature-engineering-with-nlp-502d6ea9225d"><div class="dj n z"><div class="mr n ao p ms mt"><h2 class="bf hs df bh mb mu mv mw mx my mz hq io">Text Analysis &amp; Feature Engineering with NLP</h2><div class="na s"><h3 class="bf b df bh mb mu mv mw mx my mz es">Language Detection, Text Cleaning, Length, Sentiment, Named-Entity Recognition, N-grams Frequency, Word Vectors, Topic…</h3></div><div class="nb s"><p class="bf b nc bh mb mu mv mw mx my mz es">towardsdatascience.com</p></div></div><div class="nd s"><div class="re s nf ng nh nd ni hn mq"></div></div></div></a></div><div class="hd he hf hg hh mq"><a target="_blank" rel="noopener" href="https://towardsdatascience.com/text-classification-with-no-model-training-935fe0e42180"><div class="dj n z"><div class="mr n ao p ms mt"><h2 class="bf hs df bh mb mu mv mw mx my mz hq io">BERT for Text Classification with NO model training</h2><div class="na s"><h3 class="bf b df bh mb mu mv mw mx my mz es">Use BERT, Word Embedding, and Vector Similarity when you don’t have a labeled training set</h3></div><div class="nb s"><p class="bf b nc bh mb mu mv mw mx my mz es">towardsdatascience.com</p></div></div><div class="nd s"><div class="rf s nf ng nh nd ni hn mq"></div></div></div></a></div><div class="hd he hf hg hh mq"><a target="_blank" rel="noopener" href="https://towardsdatascience.com/surpass-excel-vlookup-with-python-and-nlp-ab20d56c4a1a"><div class="dj n z"><div class="mr n ao p ms mt"><h2 class="bf hs df bh mb mu mv mw mx my mz hq io">String Matching: Surpass Excel VLOOKUP with Python &amp; NLP</h2><div class="na s"><h3 class="bf b df bh mb mu mv mw mx my mz es">Build a String Matching App for all the Excel lovers (and haters)</h3></div><div class="nb s"><p class="bf b nc bh mb mu mv mw mx my mz es">towardsdatascience.com</p></div></div><div class="nd s"><div class="rg s nf ng nh nd ni hn mq"></div></div></div></a></div><p id="f2df" class="kw kx hr ky b iq mk la lb it ml ld le lf mm lh li lj mn ll lm ln mo lp lq lr gx io" data-selectable-paragraph="">Connect: <a href="https://www.linkedin.com/in/mauro-di-pietro-56a1366b/" class="cw mp" rel="noopener nofollow">LinkedIn</a> | <a href="https://www.instagram.com/maurodp09/" class="cw mp" rel="noopener nofollow">Instagram</a> | <a href="https://twitter.com/maurodp90?lang=en" class="cw mp" rel="noopener nofollow">Twitter</a> | <a href="https://github.com/mdipietro09" class="cw mp" rel="noopener nofollow">GitHub</a></p></div></div></section></div></article><div class="dg gw fw ro v wq fy rm rq" data-test-id="post-sidebar"><div class="n p"><div class="au av aw ax ay az ba v"><div class="rr n ao"><div class="gw"><div><div class="rs s"><div class="rt s"><a href="https://mdipietro09.medium.com/?source=post_sidebar--------------------------post_sidebar-----------" class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" rel="noopener"><h2 class="bf ki df bh hq io gx">Mauro Di Pietro</h2></a></div><div class="ru s"><p class="bf b bg bh es">Italian, Data Scientist, Financial Analyst, Good Reader, Bad Writer</p></div><div class="rv n"><button class="bf b bg bh ew ex ey ez fa fb fc bs dv dw fd fe ea eb ec ed db ee">Follow</button><div class="cz s"><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="132" aria-labelledby="132"><div class="s"><button class="bf b nc bh ew bq ey ez fa fb fc bs dv dw fd fe ea eb ec ed db ee"><svg width="38" height="38" viewBox="0 0 38 38" fill="none" class="yb wh wi"><rect x="26.25" y="9.25" width="0.5" height="6.5" rx="0.25" stroke-width="0.5"></rect><rect x="29.75" y="12.25" width="0.5" height="6.5" rx="0.25" transform="rotate(90 29.75 12.25)" stroke-width="0.5"></rect><path d="M19.5 12.5h-7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1v-5" stroke-linecap="round"></path><path d="M11.5 14.5L19 20l4-3" stroke-linecap="round"></path></svg></button></div></div></div></div></div></div><div class="as sf s at"><div class="zs cl s"><span class="zp tz"><span class="bf b nc bh es">Mauro Di Pietro Follows</span></span><ul class="na"><li class="zq zr"><div class="zt s"><a href="https://matildaswinney.medium.com/?source=blogrolls_sidebar-----41ff868d1794--------------------------------" rel="noopener"><img alt="Matilda Swinney" class="s jj zu zv" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_lIGqnwySdowQoBURCgNCCg@2x.jpeg" width="20" height="20"></a></div><section class="gx"><span class="zy tz s"><a href="https://matildaswinney.medium.com/?source=blogrolls_sidebar-----41ff868d1794--------------------------------" class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" rel="noopener"><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="75" aria-labelledby="75"><h4 class="bf b nc bh mb zw mv mw uu my mz es zx">Matilda Swinney</h4></div></div></a></span></section></li><li class="zq zr"><div class="zt s"><a href="https://medium.com/@emma.austin.writer?source=blogrolls_sidebar-----41ff868d1794--------------------------------" rel="noopener"><img alt="Emma Austin" class="s jj zu zv" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_oHMlMZZZ4QNBDGsvaDs4Lg@2x.jpeg" width="20" height="20"></a></div><section class="gx"><span class="zy tz s"><a href="https://medium.com/@emma.austin.writer?source=blogrolls_sidebar-----41ff868d1794--------------------------------" class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" rel="noopener"><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="76" aria-labelledby="76"><h4 class="bf b nc bh mb zw mv mw uu my mz es zx">Emma Austin</h4></div></div></a></span></section></li><li class="zq zr"><div class="zt s"><a href="https://medium.com/@ashdeleonlopez?source=blogrolls_sidebar-----41ff868d1794--------------------------------" rel="noopener"><img alt="Ashley de Leon Lopez" class="s jj zu zv" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/2_dZvNde9zkJlPo-I8e-YrVg.jpeg" width="20" height="20"></a></div><section class="gx"><span class="zy tz s"><a href="https://medium.com/@ashdeleonlopez?source=blogrolls_sidebar-----41ff868d1794--------------------------------" class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" rel="noopener"><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="77" aria-labelledby="77"><h4 class="bf b nc bh mb zw mv mw uu my mz es zx">Ashley de Leon Lopez</h4></div></div></a></span></section></li><li class="zq zr"><div class="zt s"><a href="https://medium.com/@purplepen118?source=blogrolls_sidebar-----41ff868d1794--------------------------------" rel="noopener"><img alt="Molly Frances" class="s jj zu zv" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_qeZf_411edohhz7lUeYXvw.jpeg" width="20" height="20"></a></div><section class="gx"><span class="zy tz s"><a href="https://medium.com/@purplepen118?source=blogrolls_sidebar-----41ff868d1794--------------------------------" class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" rel="noopener"><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="78" aria-labelledby="78"><h4 class="bf b nc bh mb zw mv mw uu my mz es zx">Molly Frances</h4></div></div></a></span></section></li><li class="zq zr"><div class="zt s"><a href="https://medium.com/@claudiostamile?source=blogrolls_sidebar-----41ff868d1794--------------------------------" rel="noopener"><img alt="Claudio Stamile" class="s jj zu zv" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/0_ZvQYcfOZ9jT_2miy" width="20" height="20"></a></div><section class="gx"><span class="zy tz s"><a href="https://medium.com/@claudiostamile?source=blogrolls_sidebar-----41ff868d1794--------------------------------" class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" rel="noopener"><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="79" aria-labelledby="79"><h4 class="bf b nc bh mb zw mv mw uu my mz es zx">Claudio Stamile</h4></div></div></a></span></section></li></ul><p class="bf b nc bh es"><a href="https://towardsdatascience.com/@mdipietro09/following?source=blogrolls_sidebar-----41ff868d1794--------------------------------" class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" rel="noopener">See all (28)</a></p></div></div><div class="se sf v n o bc sg"><div class="ny n"><div class="n o bc"><div class="s at sh si sj sk sl"><div class=""><button class="bq sm sn so fq sp sq sr r ss st"><svg width="29" height="29" aria-label="clap"><g fill-rule="evenodd"><path d="M13.74 1l.76 2.97.76-2.97zM16.82 4.78l1.84-2.56-1.43-.47zM10.38 2.22l1.84 2.56-.41-3.03zM22.38 22.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M9.1 22.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L6.1 15.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L6.4 11.26l-1.18-1.18a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L11.96 14a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L8.43 9.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L20.63 15c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM13 6.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 23 23.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div><div class="s su sv sw sx sy sz ta"><div class="tb"><p class="bf b bg bh es"><button class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj">1.5K </button></p></div></div></div></div><div class="tc ny s"><button class="fq sn bq"><div class="n o bc"><div class="n o"><svg width="25" height="25" aria-label="responses" class="jm td fq st"><path d="M19.07 21.12a6.33 6.33 0 0 1-3.53-1.1 7.8 7.8 0 0 1-.7-.52c-.77.21-1.57.32-2.38.32-4.67 0-8.46-3.5-8.46-7.8C4 7.7 7.79 4.2 12.46 4.2c4.66 0 8.46 3.5 8.46 7.8 0 2.06-.85 3.99-2.4 5.45a6.28 6.28 0 0 0 1.14 2.59c.15.21.17.48.06.7a.69.69 0 0 1-.62.38h-.03zm0-1v.5l.03-.5h-.03zm-3.92-1.64l.21.2a6.09 6.09 0 0 0 3.24 1.54 7.14 7.14 0 0 1-.83-1.84 5.15 5.15 0 0 1-.16-.75 2.4 2.4 0 0 1-.02-.29v-.23l.18-.15a6.6 6.6 0 0 0 2.3-4.96c0-3.82-3.4-6.93-7.6-6.93-4.19 0-7.6 3.11-7.6 6.93 0 3.83 3.41 6.94 7.6 6.94.83 0 1.64-.12 2.41-.35l.28-.08z" fill-rule="evenodd"></path></svg><p class="bf b bg bh es"><span class="te td">17</span></p></div></div></button></div><div class="tf s"><div class="db v" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="cw cx bl bm bn bo bp bq br bs"><svg width="25" height="25" viewBox="0 0 25 25" fill="none" class="kc"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="#292929"></path></svg></button></div></div></div></div></div></div></div></div></div><div class="dg gw rh fw ri rj rk rl rm rn"></div><div><div class="tg hi n ao p"><div class="n p"><div class="au av aw ax ay hb ba v"><div class="n ci"></div><div class="n o ci"></div><div class="xo xp na xq ue xr"><div class="xs s"><h2 class="bf ki kj is kl km iv ko iw iy kq iz jb ks jc je ku io">Sign up for The Variable</h2></div><div class="xt s"><h3 class="bf b nc bh io">By Towards Data Science</h3></div><div class="nu xb s"><p class="bf b xu ug xv uj xw um xx up xy us io">Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss.&nbsp;<a href="https://medium.com/towards-data-science/newsletters/the-variable?source=newsletter_v3_promo--------------------------newsletter_v3_promo-----------" class="cw dm bl bm bn bo bp bq br bs bv fi fj mp" rel="noopener">Take a look.</a></p></div><div class="n ci"><div class="xz s ya"><button class="bf b df ej ew yc ey ez fa fb fc bs dv dw fd fe ea eb ec ed db ee"><div class="ka s"><span class="yb" aria-hidden="true"><svg width="38" height="38" viewBox="0 0 38 38" fill="none"><rect x="26.25" y="9.25" width="0.5" height="6.5" rx="0.25" stroke-width="0.5"></rect><rect x="29.75" y="12.25" width="0.5" height="6.5" rx="0.25" transform="rotate(90 29.75 12.25)" stroke-width="0.5"></rect><path d="M19.5 12.5h-7a1 1 0 0 0-1 1v11a1 1 0 0 0 1 1h13a1 1 0 0 0 1-1v-5" stroke-linecap="round"></path><path d="M11.5 14.5L19 20l4-3" stroke-linecap="round"></path></svg></span>Get this newsletter</div></button></div><div class="yd ye s"><p class="bf b nc bh io">Emails will be sent to assansanogo@gmail.com.<span><a href="https://medium.com/m/signin?operation=login&amp;redirect=https%3A%2F%2Ftowardsdatascience.com%2Ftext-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794&amp;collection=Towards%20Data%20Science&amp;collectionId=7f60cf5620c9&amp;newsletterV3=The%20Variable&amp;newsletterV3Id=d6fe9076899&amp;source=newsletter_v3_promo--------------------------newsletter_v3_promo-----------" class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" rel="noopener"><button class="cw dm bl bm bn bo bp bq br bs bv fi fj mp s" target="_blank">Not you?</button></a></span></p></div></div></div><div class="th ti s"><div class="n bz jz"><div class="n o bc"><div class="tj s"><span class="s tk tl tm e d"><div class="n o bc"><div class="s at sh si sj sk sl"><div class=""><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="2" aria-labelledby="2"><button class="bq sm sn so fq sp sq sr r ss st"><svg width="25" height="25" viewBox="0 0 25 25" aria-label="clap"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div></div></div><div class="s su sv sw sx tn to tp"><div class="at tq tb"><p class="bf b bg bh io"><button class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj">1.5K<span class="s h g f tr ts">&nbsp;</span></button><span class="s h g f tr ts"></span></p></div></div></div></span><span class="s h g f tr ts"><div class="n o bc"><div class="s at sh si sj sk sl"><div class=""><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="3" aria-labelledby="3"><button class="bq sm sn so fq sp sq sr r ss st"><svg width="25" height="25" viewBox="0 0 25 25" aria-label="clap"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div></div></div><div class="s su sv sw sx tn to tp"><div class="tb"><p class="bf b bg bh io"><button class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj">1.5K </button></p></div></div></div></span></div><div class="tt s"><button class="fq sn bq"><div class="n o bc"><div class="n o"><svg width="29" height="29" aria-label="responses" class="jm td fq st tu"><path d="M21.27 20.06a9.04 9.04 0 0 0 2.75-6.68C24.02 8.21 19.67 4 14.1 4S4 8.21 4 13.38c0 5.18 4.53 9.39 10.1 9.39 1 0 2-.14 2.95-.41.28.25.6.49.92.7a7.46 7.46 0 0 0 4.19 1.3c.27 0 .5-.13.6-.35a.63.63 0 0 0-.05-.65 8.08 8.08 0 0 1-1.29-2.58 5.42 5.42 0 0 1-.15-.75zm-3.85 1.32l-.08-.28-.4.12a9.72 9.72 0 0 1-2.84.43c-4.96 0-9-3.71-9-8.27 0-4.55 4.04-8.26 9-8.26 4.95 0 8.77 3.71 8.77 8.27 0 2.25-.75 4.35-2.5 5.92l-.24.21v.32a5.59 5.59 0 0 0 .21 1.29c.19.7.49 1.4.89 2.08a6.43 6.43 0 0 1-2.67-1.06c-.34-.22-.88-.48-1.16-.74z"></path></svg><p class="bf b bg bh io"><span class="tv tw td">17</span></p></div></div></button></div></div><div class="n o"><div class="ka s"><div class="db" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="4" aria-labelledby="4"><button class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post"><svg width="25" height="25" class="r"><g fill-rule="evenodd"><path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01"></path><path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2"></path></g></svg></button></div></div></div></div><div class="ka s z"><div class="db v" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="cw cx bl bm bn bo bp bq br bs"><svg width="25" height="25" viewBox="0 0 25 25" fill="none" class="kc"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="#292929"></path></svg></button></div></div><div class="db" aria-hidden="false" aria-describedby="creatorActionOverflowMenu" aria-labelledby="creatorActionOverflowMenu"><div class="db" aria-hidden="false" aria-describedby="removeFromPublicationPopover" aria-labelledby="removeFromPublicationPopover"><div class="kd s z"><button class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" aria-controls="creatorActionOverflowMenu" aria-expanded="false" aria-label="More options"><svg class="r ke kf" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div><div class="tx ti s"><ul class="bq br"><li class="db ty kb tz"><a href="https://towardsdatascience.com/tagged/data-science" class="bf b nc ua es ub uc ee s oa">Data Science</a></li><li class="db ty kb tz"><a href="https://towardsdatascience.com/tagged/artificial-intelligence" class="bf b nc ua es ub uc ee s oa">Artificial Intelligence</a></li><li class="db ty kb tz"><a href="https://towardsdatascience.com/tagged/machine-learning" class="bf b nc ua es ub uc ee s oa">Machine Learning</a></li><li class="db ty kb tz"><a href="https://towardsdatascience.com/tagged/programming" class="bf b nc ua es ub uc ee s oa">Programming</a></li><li class="db ty kb tz"><a href="https://towardsdatascience.com/tagged/nlp" class="bf b nc ua es ub uc ee s oa">NLP</a></li></ul></div></div></div><div><div class="n p"><div class="au av aw ax ay hb ba v"></div></div><div class="s jz"><div class="ud se s ue"><div class="n p"><div class="au av aw ax ay hb ba v"><div class="n o bz"><h2 class="bf ki uf ug uh kl ui uj uk ko ul um un kq uo up uq ks ur us ut ku mb mv mw uu my mz io"><a href="https://towardsdatascience.com/?source=follow_footer-------------------------------------" class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" rel="noopener">More from Towards Data Science</a></h2><div class="db" aria-hidden="false" aria-describedby="collectionFollowPopover" aria-labelledby="collectionFollowPopover"><button class="bf b bg bh ew ex ey ez fa fb fc bs dv dw fd fe ea eb ec ed db ee" aria-controls="collectionFollowPopover" aria-expanded="false"><div class="n bc">Follow</div></button></div></div><div class="uv uw s"><p class="bf b bg bh es">Your home for data science. A Medium publication sharing concepts, ideas and codes.</p></div></div></div></div></div><div class="ux s ue jz"><div class="n p"><div class="uy uz va vb vc hn ba v"><span><div class="gk gl aer s js jg ju jh aes aet aeu aev aew aex"><div class="n p"><div class="au av aw ax ay hb ba v"><div class="v ma"><div class="s"><div class="o n"><div></div><div class="v n ci"><div class="n"><div style="flex: 1 1 0%;"><span class="bf b bg bh io"><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="109" aria-labelledby="109"><a href="https://medium.com/@divyanaidu?source=follow_footer---------0----------------------------" class="" rel="noopener"><p class="bf b bg bh ge">Divya Naidu</p></a></div></div></span></div></div><span class="bf b bg bh es"><a class="" rel="noopener" href="https://towardsdatascience.com/a-b-testing-the-case-study-1030a94f3c4b?source=follow_footer---------0----------------------------"><p class="bf b bg bh es"><span class="jp">·</span>Jul 18, 2020<svg class="jm jn jo" width="15" height="15" viewBox="0 0 15 15" aria-label="Member only content"><path d="M7.44 2.32c.03-.1.09-.1.12 0l1.2 3.53a.29.29 0 0 0 .26.2h3.88c.11 0 .13.04.04.1L9.8 8.33a.27.27 0 0 0-.1.29l1.2 3.53c.03.1-.01.13-.1.07l-3.14-2.18a.3.3 0 0 0-.32 0L4.2 12.22c-.1.06-.14.03-.1-.07l1.2-3.53a.27.27 0 0 0-.1-.3L2.06 6.16c-.1-.06-.07-.12.03-.12h3.89a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></p></a></span></div></div><div class="nb s"><span class="s"></span><div><div class="fm fv wo gu gv gw"></div><section class="gx gy gz ep ha"><figure class="cl hi rt uv paragraph-image"><a href="https://towardsdatascience.com/a-b-testing-the-case-study-1030a94f3c4b?source=follow_footer---------0----------------------------"><div class="aey"><img alt="" class="v hn ho" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_XGcTq8M5cAQANdvcvWP3xA.jpeg" width="1000" height="751" role="presentation"></div></a><figcaption class="ox oy gm gk gl oz pa bf b bg bh es" data-selectable-paragraph="">Image by Author</figcaption></figure><div class=""><h1 id="e73c" class="hp hq hr bf hs aez afa la kl afb afc ld ko afd afe aff afg afh afi afj afk afl afm afn afo in io"><a class="cw ee" rel="noopener" href="https://towardsdatascience.com/a-b-testing-the-case-study-1030a94f3c4b?source=follow_footer---------0----------------------------">A/B Testing: The Case Study!</a></h1></div><p id="2e06" class="kw kx hr ky b iq afp la lb it afq ld le lf lg lh li lj lk ll lm ln lo lp lq lr gx io" data-selectable-paragraph="">Analyzing the test results using Python.</p><p id="e971" class="kw kx hr ky b iq afp la lb it afq ld le lf lg lh li lj lk ll lm ln lo lp lq lr gx io afr" data-selectable-paragraph="">In theory, there is no difference between theory and practice, but<br>practice, there is.</p><p id="48e4" class="kw kx hr ky b iq afp la lb it afq ld le lf lg lh li lj lk ll lm ln lo lp lq lr gx io" data-selectable-paragraph="">My previous blog gives a basic idea of what exactly is <a class="cw mp" rel="noopener" href="https://towardsdatascience.com/a-b-testing-the-basics-86d6d98525c9?source=friends_link&amp;sk=86434b44e90841eb1a30e7e7cc2760eb">A/B testing</a>. From the positioning of images on pages, to the checkout process, we are staunch advocates of A/B Testing. Knowledge of a concept is not enough, implementation of this knowledge gives the best exposure of learning.</p><p id="66ec" class="kw kx hr ky b iq afp la lb it afq ld le lf lg lh li lj lk ll lm ln lo lp lq lr gx io" data-selectable-paragraph="">This blog is a walkthrough of an A/B testing case study that I have worked on as part of <div class="db aie"><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="124" aria-labelledby="124"><a href="https://medium.com/u/2929690a28fb?source=follow_footer---------0----------------------------" class="afs ge ee" target="_blank" rel="noopener">Udacity</a></div></div></div>’s Data Analyst NanoDegree Program. …</p></section></div></div></div><div><a class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj s" rel="noopener" href="https://towardsdatascience.com/a-b-testing-the-case-study-1030a94f3c4b?readmore=1&amp;source=follow_footer---------0----------------------------"><div class="aft s"><p class="bf b bg bh ge">Read more <span> · 6 min read</span></p></div></a></div></div></div></div><div class="afu s"></div><div class="s"></div><div class="fu s afv"><div class="s"><div class="n p"><div class="au av aw ax ay hb ba v"><div class="n o bz"><div class="n o"><div class="n o bc"><div class="s at jt jv sj sk sl"><div class=""><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="90" aria-labelledby="90"><button class="bq sm sn so fq sp sq vh r ss st"><svg width="25" height="25" viewBox="0 0 25 25" aria-label="clap"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div></div></div><div class="s su sv sw sx sy sz ta"><div class="tb"><p class="bf b bg bh io"><button class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj">226 </button></p></div></div></div><div class="afw n o afx afy afz aga agb"><a class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" rel="noopener" href="https://towardsdatascience.com/a-b-testing-the-case-study-1030a94f3c4b?responsesOpen=true&amp;source=follow_footer---------0----------------------------"><div class="tw s"><button class="fq sn bq"><div class="n o bc"><div class="n o"><svg width="29" height="29" aria-label="responses" class="jm td fq st"><path d="M21.27 20.06a9.04 9.04 0 0 0 2.75-6.68C24.02 8.21 19.67 4 14.1 4S4 8.21 4 13.38c0 5.18 4.53 9.39 10.1 9.39 1 0 2-.14 2.95-.41.28.25.6.49.92.7a7.46 7.46 0 0 0 4.19 1.3c.27 0 .5-.13.6-.35a.63.63 0 0 0-.05-.65 8.08 8.08 0 0 1-1.29-2.58 5.42 5.42 0 0 1-.15-.75zm-3.85 1.32l-.08-.28-.4.12a9.72 9.72 0 0 1-2.84.43c-4.96 0-9-3.71-9-8.27 0-4.55 4.04-8.26 9-8.26 4.95 0 8.77 3.71 8.77 8.27 0 2.25-.75 4.35-2.5 5.92l-.24.21v.32a5.59 5.59 0 0 0 .21 1.29c.19.7.49 1.4.89 2.08a6.43 6.43 0 0 1-2.67-1.06c-.34-.22-.88-.48-1.16-.74z"></path></svg></div></div></button></div></a></div></div><div class="n o agc"><div class="kb s"><div class="db" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="91" aria-labelledby="91"><button class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post"><svg width="25" height="25" class="r"><g fill-rule="evenodd"><path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01"></path><path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2"></path></g></svg></button></div></div></div></div><div class="kb s"><div class="db v" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="cw cx bl bm bn bo bp bq br bs"><svg width="25" height="25" viewBox="0 0 25 25" fill="none" class="kc"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="#292929"></path></svg></button></div></div><div class="kd s"><div class="db" aria-hidden="false" aria-describedby="truncatedPostCardReaderMenu" aria-labelledby="truncatedPostCardReaderMenu"><button class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" aria-controls="truncatedPostCardReaderMenu" aria-expanded="false" aria-label="More options"><svg class="r ke kf" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div><div class="n p"><div class="au av aw ax ay hb ba v"><hr class="sn aep aeq br"></div></div></span><span><div class="ae" id="post-start-writing-unit-li"><div class="nz gp s oy"><div class="n p"><div class="uy uz va vb vc hn ba v"><span class="ae" id="post-start-writing-text-li"><div class="agd age agf agg agh agi agj agk ts agl"><p class="bf b df ej io">Post a quick thought or a long story. It's easy and free.</p></div></span><span class="ae" id="post-start-writing-link-li"><div class="ya agd agm agf agn agh ago agj agp ts"><a href="https://medium.com/new-story?source=post_page_footer_cta_write-------------------------------------" class="bf b bg bh ew ex ey agq agr ags abw bs dv agt agu agv ea eb ec ed db ee" rel="noopener">Write on Medium</a></div></span></div></div></div></div><div class="gk gl aer s"><div class="n p"><div class="au av aw ax ay hb ba v"><hr class="sn aep aeq br"><div class="v ma"><div class="agw s agx"><div class="o n"><div></div><div class="v n ci"><div class="n"><div style="flex: 1 1 0%;"><span class="bf b bg bh io"><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="112" aria-labelledby="112"><a href="https://francoisstamant.medium.com/?source=follow_footer---------1----------------------------" class="" rel="noopener"><p class="bf b bg bh ge">François St-Amant</p></a></div></div></span></div></div><span class="bf b bg bh es"><a class="" rel="noopener" href="https://towardsdatascience.com/building-a-brain-tumor-classification-app-e9a0eb9f068?source=follow_footer---------1----------------------------"><p class="bf b bg bh es"><span class="jp">·</span>Jul 18, 2020<svg class="jm jn jo" width="15" height="15" viewBox="0 0 15 15" aria-label="Member only content"><path d="M7.44 2.32c.03-.1.09-.1.12 0l1.2 3.53a.29.29 0 0 0 .26.2h3.88c.11 0 .13.04.04.1L9.8 8.33a.27.27 0 0 0-.1.29l1.2 3.53c.03.1-.01.13-.1.07l-3.14-2.18a.3.3 0 0 0-.32 0L4.2 12.22c-.1.06-.14.03-.1-.07l1.2-3.53a.27.27 0 0 0-.1-.3L2.06 6.16c-.1-.06-.07-.12.03-.12h3.89a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></p></a></span></div></div><div class="nb s"><span class="s"></span><div><div class="fm fv wo gu gv gw"></div><section class="gx gy gz ep ha"><div class=""><h1 id="9a85" class="hp hq hr bf hs aez afa la kl afb afc ld ko afd afe aff afg afh afi afj afk afl afm afn afo in io"><a class="cw ee" rel="noopener" href="https://towardsdatascience.com/building-a-brain-tumor-classification-app-e9a0eb9f068?source=follow_footer---------1----------------------------">Building a Brain Tumor Classification App</a></h1></div><div class=""><h2 id="de35" class="ip hq hr bf b agy agz jf iq is it iv iw iy iz jb jc je es">A Real Machine Learning App from scratch with Dash</h2></div><figure class="cl hi gk gl paragraph-image"><a href="https://towardsdatascience.com/building-a-brain-tumor-classification-app-e9a0eb9f068?source=follow_footer---------1----------------------------"><div class="aha gk gl"><img alt="" class="v hn ho" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/0_wOTKLCPTWUoOg81r" width="700" height="464" role="presentation"></div></a><figcaption class="ox oy gm gk gl oz pa bf b bg bh es" data-selectable-paragraph="">Source: <a href="https://unsplash.com/photos/rmWtVQN5RzU" class="cw mp" rel="noopener nofollow">https://unsplash.com/photos/rmWtVQN5RzU</a></figcaption></figure><p id="baac" class="kw kx hr ky b iq afp la lb it afq ld le lf lg lh li lj lk ll lm ln lo lp lq lr gx io" data-selectable-paragraph="">In the following article, I will present a machine learning app I created from scratch.</p><h2 id="e8f7" class="kg kh hr bf ki uf ahb ug kl ui ahc uj ko ul ahd um kq uo ahe up ks ur ahf us ku kv io" data-selectable-paragraph="">1. The goal</h2><p id="de74" class="kw kx hr ky b iq kz la lb it lc ld le lf ahg lh li lj ahh ll lm ln ahi lp lq lr gx io" data-selectable-paragraph="">What I wanted to build was an app that would take as input a brain MRI image. From there, the app would return a prediction, saying if there is or not a tumor present on the image.</p><p id="7adb" class="kw kx hr ky b iq afp la lb it afq ld le lf lg lh li lj lk ll lm ln lo lp lq lr gx io" data-selectable-paragraph="">I found the idea interesting because the app could be used by anyone to determine the presence (or not) of a brain tumor. No need for coding skills or knowledge about brains.</p><p id="0d39" class="kw kx hr ky b iq afp la lb it afq ld le lf lg lh li lj lk ll lm ln lo lp lq lr gx io" data-selectable-paragraph="">To achieve this goal, three steps needed to be achieved, i.e. the creation of a model…</p></section></div></div></div><div><a class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj s" rel="noopener" href="https://towardsdatascience.com/building-a-brain-tumor-classification-app-e9a0eb9f068?readmore=1&amp;source=follow_footer---------1----------------------------"><div class="aft s"><p class="bf b bg bh ge">Read more <span> · 4 min read</span></p></div></a></div></div></div></div><div class="afu s"></div><div class="s"></div><div class="fu s afv"><div class="s"><div class="n p"><div class="au av aw ax ay hb ba v"><div class="n o bz"><div class="n o"><div class="n o bc"><div class="s at jt jv sj sk sl"><div class=""><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="92" aria-labelledby="92"><button class="bq sm sn so fq sp sq vh r ss st"><svg width="25" height="25" viewBox="0 0 25 25" aria-label="clap"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div></div></div><div class="s su sv sw sx sy sz ta"><div class="tb"><p class="bf b bg bh io"><button class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj">62 </button></p></div></div></div><div class="afw n o afx afy afz aga agb"><a class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" rel="noopener" href="https://towardsdatascience.com/building-a-brain-tumor-classification-app-e9a0eb9f068?responsesOpen=true&amp;source=follow_footer---------1----------------------------"><div class="tw s"><button class="fq sn bq"><div class="n o bc"><div class="n o"><svg width="29" height="29" aria-label="responses" class="jm td fq st"><path d="M21.27 20.06a9.04 9.04 0 0 0 2.75-6.68C24.02 8.21 19.67 4 14.1 4S4 8.21 4 13.38c0 5.18 4.53 9.39 10.1 9.39 1 0 2-.14 2.95-.41.28.25.6.49.92.7a7.46 7.46 0 0 0 4.19 1.3c.27 0 .5-.13.6-.35a.63.63 0 0 0-.05-.65 8.08 8.08 0 0 1-1.29-2.58 5.42 5.42 0 0 1-.15-.75zm-3.85 1.32l-.08-.28-.4.12a9.72 9.72 0 0 1-2.84.43c-4.96 0-9-3.71-9-8.27 0-4.55 4.04-8.26 9-8.26 4.95 0 8.77 3.71 8.77 8.27 0 2.25-.75 4.35-2.5 5.92l-.24.21v.32a5.59 5.59 0 0 0 .21 1.29c.19.7.49 1.4.89 2.08a6.43 6.43 0 0 1-2.67-1.06c-.34-.22-.88-.48-1.16-.74z"></path></svg><p class="bf b bg bh io"><span class="tv td">1</span></p></div></div></button></div></a></div></div><div class="n o agc"><div class="kb s"><div class="db" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="93" aria-labelledby="93"><button class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post"><svg width="25" height="25" class="r"><g fill-rule="evenodd"><path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01"></path><path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2"></path></g></svg></button></div></div></div></div><div class="kb s"><div class="db v" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="cw cx bl bm bn bo bp bq br bs"><svg width="25" height="25" viewBox="0 0 25 25" fill="none" class="kc"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="#292929"></path></svg></button></div></div><div class="kd s"><div class="db" aria-hidden="false" aria-describedby="truncatedPostCardReaderMenu" aria-labelledby="truncatedPostCardReaderMenu"><button class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" aria-controls="truncatedPostCardReaderMenu" aria-expanded="false" aria-label="More options"><svg class="r ke kf" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div></span><span><div class="gk gl aer s js ahj ju ahk aes ahl aeu ahm aew ahn"><div class="n p"><div class="au av aw ax ay hb ba v"><hr class="sn aep aeq br"><div class="v ma"><div class="agw s agx"><div class="o n"><div></div><div class="v n ci"><div class="n"><div style="flex: 1 1 0%;"><span class="bf b bg bh io"><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="115" aria-labelledby="115"><a href="https://medium.com/@ranpelta?source=follow_footer---------2----------------------------" class="" rel="noopener"><p class="bf b bg bh ge">Ran Pelta</p></a></div></div></span></div></div><span class="bf b bg bh es"><a class="" rel="noopener" href="https://towardsdatascience.com/how-to-convert-pandas-dataframe-to-keras-rnn-and-back-to-pandas-for-multivariate-regression-dcc34c991df9?source=follow_footer---------2----------------------------"><p class="bf b bg bh es"><span class="jp">·</span>Jul 18, 2020<svg class="jm jn jo" width="15" height="15" viewBox="0 0 15 15" aria-label="Member only content"><path d="M7.44 2.32c.03-.1.09-.1.12 0l1.2 3.53a.29.29 0 0 0 .26.2h3.88c.11 0 .13.04.04.1L9.8 8.33a.27.27 0 0 0-.1.29l1.2 3.53c.03.1-.01.13-.1.07l-3.14-2.18a.3.3 0 0 0-.32 0L4.2 12.22c-.1.06-.14.03-.1-.07l1.2-3.53a.27.27 0 0 0-.1-.3L2.06 6.16c-.1-.06-.07-.12.03-.12h3.89a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></p></a></span></div></div><div class="nb s"><span class="s"></span><div><div class="fm fv wo gu gv gw"></div><section class="gx gy gz ep ha"><div class=""><h1 id="567c" class="hp hq hr bf hs aez afa la kl afb afc ld ko afd afe aff afg afh afi afj afk afl afm afn afo in io"><a class="cw ee" rel="noopener" href="https://towardsdatascience.com/how-to-convert-pandas-dataframe-to-keras-rnn-and-back-to-pandas-for-multivariate-regression-dcc34c991df9?source=follow_footer---------2----------------------------">How to Convert Pandas Dataframe to Keras RNN and Back to Pandas for Multivariate Regression Problems</a></h1></div><div class=""><h2 id="ce9a" class="ip hq hr bf b agy agz jf iq is it iv iw iy iz jb jc je es">This post provides a straightforward Python code that takes data in Pandas dataframe and outputs predictions in the same format using Keras RNN LSTM model.</h2></div><p id="18b3" class="kw kx hr ky b iq afp la lb it afq ld le lf lg lh li lj lk ll lm ln lo lp lq lr gx io" data-selectable-paragraph="">The problem I encountered was rather common (I think): Taking data in a <code class="mf aho ahp ahq oc b">pandas</code> <code class="mf aho ahp ahq oc b">dataframe</code> format and making predictions using a time series regression model with <code class="mf aho ahp ahq oc b">keras</code> RNN where I have more than one independent <code class="mf aho ahp ahq oc b">X</code> (AKA features or predictors) and one dependent <code class="mf aho ahp ahq oc b">y</code>. To be more precise, the problem was not to build the model, rather to convert the data from a <code class="mf aho ahp ahq oc b">pandas</code> <code class="mf aho ahp ahq oc b">dataframe</code> format to a format that an RNN model (in <code class="mf aho ahp ahq oc b">keras</code>) requires and obtaining predictions from the <code class="mf aho ahp ahq oc b">keras</code> model back as a <code class="mf aho ahp ahq oc b">pandas</code> <code class="mf aho ahp ahq oc b">dataframe</code>.</p><p id="d3c4" class="kw kx hr ky b iq afp la lb it afq ld le lf lg lh li lj lk ll lm ln lo lp lq lr gx io" data-selectable-paragraph="">It felt that wherever I looked for a solution I got explanations…<code class="mf aho ahp ahq oc b"></code><code class="mf aho ahp ahq oc b"></code></p></section></div></div></div><div><a class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj s" rel="noopener" href="https://towardsdatascience.com/how-to-convert-pandas-dataframe-to-keras-rnn-and-back-to-pandas-for-multivariate-regression-dcc34c991df9?readmore=1&amp;source=follow_footer---------2----------------------------"><div class="aft s"><p class="bf b bg bh ge">Read more <span> · 7 min read</span></p></div></a></div></div></div></div><div class="afu s"></div><div class="s"></div><div class="fu s afv"><div class="s"><div class="n p"><div class="au av aw ax ay hb ba v"><div class="n o bz"><div class="n o"><div class="n o bc"><div class="s at jt jv sj sk sl"><div class=""><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="94" aria-labelledby="94"><button class="bq sm sn so fq sp sq vh r ss st"><svg width="25" height="25" viewBox="0 0 25 25" aria-label="clap"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div></div></div><div class="s su sv sw sx sy sz ta"><div class="tb"><p class="bf b bg bh io"><button class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj">16 </button></p></div></div></div><div class="afw n o afx afy afz aga agb"><a class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" rel="noopener" href="https://towardsdatascience.com/how-to-convert-pandas-dataframe-to-keras-rnn-and-back-to-pandas-for-multivariate-regression-dcc34c991df9?responsesOpen=true&amp;source=follow_footer---------2----------------------------"><div class="tw s"><button class="fq sn bq"><div class="n o bc"><div class="n o"><svg width="29" height="29" aria-label="responses" class="jm td fq st"><path d="M21.27 20.06a9.04 9.04 0 0 0 2.75-6.68C24.02 8.21 19.67 4 14.1 4S4 8.21 4 13.38c0 5.18 4.53 9.39 10.1 9.39 1 0 2-.14 2.95-.41.28.25.6.49.92.7a7.46 7.46 0 0 0 4.19 1.3c.27 0 .5-.13.6-.35a.63.63 0 0 0-.05-.65 8.08 8.08 0 0 1-1.29-2.58 5.42 5.42 0 0 1-.15-.75zm-3.85 1.32l-.08-.28-.4.12a9.72 9.72 0 0 1-2.84.43c-4.96 0-9-3.71-9-8.27 0-4.55 4.04-8.26 9-8.26 4.95 0 8.77 3.71 8.77 8.27 0 2.25-.75 4.35-2.5 5.92l-.24.21v.32a5.59 5.59 0 0 0 .21 1.29c.19.7.49 1.4.89 2.08a6.43 6.43 0 0 1-2.67-1.06c-.34-.22-.88-.48-1.16-.74z"></path></svg><p class="bf b bg bh io"><span class="tv td">3</span></p></div></div></button></div></a></div></div><div class="n o agc"><div class="kb s"><div class="db" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="95" aria-labelledby="95"><button class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post"><svg width="25" height="25" class="r"><g fill-rule="evenodd"><path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01"></path><path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2"></path></g></svg></button></div></div></div></div><div class="kb s"><div class="db v" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="cw cx bl bm bn bo bp bq br bs"><svg width="25" height="25" viewBox="0 0 25 25" fill="none" class="kc"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="#292929"></path></svg></button></div></div><div class="kd s"><div class="db" aria-hidden="false" aria-describedby="truncatedPostCardReaderMenu" aria-labelledby="truncatedPostCardReaderMenu"><button class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" aria-controls="truncatedPostCardReaderMenu" aria-expanded="false" aria-label="More options"><svg class="r ke kf" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div></span><span><div class="gk gl aer s js ahj ju ahk aes ahl aeu ahm aew ahn"><div class="n p"><div class="au av aw ax ay hb ba v"><hr class="sn aep aeq br"><div class="v ma"><div class="agw s agx"><div class="o n"><div></div><div class="v n ci"><div class="n"><div style="flex: 1 1 0%;"><span class="bf b bg bh io"><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="118" aria-labelledby="118"><a href="https://medium.com/@ankitgoel16?source=follow_footer---------3----------------------------" class="" rel="noopener"><p class="bf b bg bh ge">Ankit Goel</p></a></div></div></span></div></div><span class="bf b bg bh es"><a class="" rel="noopener" href="https://towardsdatascience.com/are-you-solving-ml-clustering-problems-using-k-means-68fb4efa5469?source=follow_footer---------3----------------------------"><p class="bf b bg bh es"><span class="jp">·</span>Jul 18, 2020<svg class="jm jn jo" width="15" height="15" viewBox="0 0 15 15" aria-label="Member only content"><path d="M7.44 2.32c.03-.1.09-.1.12 0l1.2 3.53a.29.29 0 0 0 .26.2h3.88c.11 0 .13.04.04.1L9.8 8.33a.27.27 0 0 0-.1.29l1.2 3.53c.03.1-.01.13-.1.07l-3.14-2.18a.3.3 0 0 0-.32 0L4.2 12.22c-.1.06-.14.03-.1-.07l1.2-3.53a.27.27 0 0 0-.1-.3L2.06 6.16c-.1-.06-.07-.12.03-.12h3.89a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></p></a></span></div></div><div class="nb s"><span class="s"></span><div><div class="fm fv wo gu gv gw"></div><section class="gx gy gz ep ha"><div class=""><h1 id="9b51" class="hp hq hr bf hs aez afa la kl afb afc ld ko afd afe aff afg afh afi afj afk afl afm afn afo in io"><a class="cw ee" rel="noopener" href="https://towardsdatascience.com/are-you-solving-ml-clustering-problems-using-k-means-68fb4efa5469?source=follow_footer---------3----------------------------">Are you solving ML Clustering problems using K-Means?</a></h1></div><div class=""><h2 id="8df2" class="ip hq hr bf b agy agz jf iq is it iv iw iy iz jb jc je es">One-liner to plot Elbow curve, Silhouette curve, Intercluster distances, and learn about Scikit-Learn tips that can improve your model.</h2></div><figure class="cl hi gk gl paragraph-image"><a href="https://towardsdatascience.com/are-you-solving-ml-clustering-problems-using-k-means-68fb4efa5469?source=follow_footer---------3----------------------------"><div class="aha gk gl"><img alt="Different Methods to find optimal value of K in K-Means." class="v hn ho" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_Piw5jgcG8Lj6FYXKkeVKmQ.png" width="700" height="438"></div></a><figcaption class="ox oy gm gk gl oz pa bf b bg bh es" data-selectable-paragraph="">Source: Image by Author</figcaption></figure><p id="07c3" class="kw kx hr ky b iq afp la lb it afq ld le lf lg lh li lj lk ll lm ln lo lp lq lr gx io" data-selectable-paragraph="">K-Means is the most used clustering algorithm in unsupervised Machine Learning problems and it is really useful to find similar data points and to determine the structure of the data. In this article, I assume that you have a basic understanding of K-Means and will focus more on how you can-</p><ul class=""><li id="6b45" class="ahr ahs hr ky b iq afp it afq aht ahu ahv ahw ahx ahy lr nk nl nm ahz io" data-selectable-paragraph="">Find the <strong class="ky hs">value of K</strong> (number of clusters) using different methods like the <em class="ls">Elbow curve, Sillhouette curve, and Intercluster distances.</em></li><li id="8957" class="ahr ahs hr ky b iq nn it no aht aia ahv aib ahx aic lr nk nl nm ahz io" data-selectable-paragraph="">A great visualization library <strong class="ky hs">YellowBrick </strong>which can help you to plot these curves with just <strong class="ky hs">1 line of code</strong>.</li><li id="0d6e" class="ahr ahs hr ky b iq nn it no aht aia ahv aib ahx aic lr nk nl nm ahz io" data-selectable-paragraph="">Different Scikit-Learn tips to improve your K-Means model.</li></ul><blockquote class="rb rc be"><p id="788f" class="kw kx ls ky b iq afp la lb it afq ld le lf lg lh li lj lk ll lm ln lo lp lq lr gx io" data-selectable-paragraph="">If you…<div class="db aie"><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="126" aria-labelledby="126"><a href="https://medium.com/u/793eeb87d3ee?source=follow_footer---------3----------------------------" class="afs ge ee" target="_blank" rel="noopener"></a></div></div></div><a class="cw mp" rel="noopener" href="https://towardsdatascience.com/k-means-clustering-algorithm-applications-evaluation-methods-and-drawbacks-aa03e644b48a"></a><div class="db aie"><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="131" aria-labelledby="131"><a href="https://medium.com/u/50e1cd76b649?source=follow_footer---------3----------------------------" class="afs ge ee" target="_blank" rel="noopener"></a></div></div></div><a class="cw mp" rel="noopener" href="https://towardsdatascience.com/k-means-clustering-from-a-to-z-f6242a314e9a"></a><a href="https://scikit-learn.org/stable/modules/clustering.html#clustering" class="cw mp" rel="noopener nofollow"></a></p></blockquote></section></div></div></div><div><a class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj s" rel="noopener" href="https://towardsdatascience.com/are-you-solving-ml-clustering-problems-using-k-means-68fb4efa5469?readmore=1&amp;source=follow_footer---------3----------------------------"><div class="aft s"><p class="bf b bg bh ge">Read more <span> · 8 min read</span></p></div></a></div></div></div></div><div class="afu s"></div><div class="s"></div><div class="fu s afv"><div class="s"><div class="n p"><div class="au av aw ax ay hb ba v"><div class="n o bz"><div class="n o"><div class="n o bc"><div class="s at jt jv sj sk sl"><div class=""><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="96" aria-labelledby="96"><button class="bq sm sn so fq sp sq vh r ss st"><svg width="25" height="25" viewBox="0 0 25 25" aria-label="clap"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div></div></div><div class="s su sv sw sx sy sz ta"><div class="tb"><p class="bf b bg bh io"><button class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj">65 </button></p></div></div></div><div class="afw n o afx afy afz aga agb"><a class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" rel="noopener" href="https://towardsdatascience.com/are-you-solving-ml-clustering-problems-using-k-means-68fb4efa5469?responsesOpen=true&amp;source=follow_footer---------3----------------------------"><div class="tw s"><button class="fq sn bq"><div class="n o bc"><div class="n o"><svg width="29" height="29" aria-label="responses" class="jm td fq st"><path d="M21.27 20.06a9.04 9.04 0 0 0 2.75-6.68C24.02 8.21 19.67 4 14.1 4S4 8.21 4 13.38c0 5.18 4.53 9.39 10.1 9.39 1 0 2-.14 2.95-.41.28.25.6.49.92.7a7.46 7.46 0 0 0 4.19 1.3c.27 0 .5-.13.6-.35a.63.63 0 0 0-.05-.65 8.08 8.08 0 0 1-1.29-2.58 5.42 5.42 0 0 1-.15-.75zm-3.85 1.32l-.08-.28-.4.12a9.72 9.72 0 0 1-2.84.43c-4.96 0-9-3.71-9-8.27 0-4.55 4.04-8.26 9-8.26 4.95 0 8.77 3.71 8.77 8.27 0 2.25-.75 4.35-2.5 5.92l-.24.21v.32a5.59 5.59 0 0 0 .21 1.29c.19.7.49 1.4.89 2.08a6.43 6.43 0 0 1-2.67-1.06c-.34-.22-.88-.48-1.16-.74z"></path></svg><p class="bf b bg bh io"><span class="tv td">1</span></p></div></div></button></div></a></div></div><div class="n o agc"><div class="kb s"><div class="db" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="97" aria-labelledby="97"><button class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post"><svg width="25" height="25" class="r"><g fill-rule="evenodd"><path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01"></path><path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2"></path></g></svg></button></div></div></div></div><div class="kb s"><div class="db v" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="cw cx bl bm bn bo bp bq br bs"><svg width="25" height="25" viewBox="0 0 25 25" fill="none" class="kc"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="#292929"></path></svg></button></div></div><div class="kd s"><div class="db" aria-hidden="false" aria-describedby="truncatedPostCardReaderMenu" aria-labelledby="truncatedPostCardReaderMenu"><button class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" aria-controls="truncatedPostCardReaderMenu" aria-expanded="false" aria-label="More options"><svg class="r ke kf" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div></span><span><div class="gk gl aer s js ahj ju ahk aes ahl aeu ahm aew ahn"><div class="n p"><div class="au av aw ax ay hb ba v"><hr class="sn aep aeq br"><div class="v ma"><div class="agw s agx"><div class="o n"><div></div><div class="v n ci"><div class="n"><div style="flex: 1 1 0%;"><span class="bf b bg bh io"><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="121" aria-labelledby="121"><a href="https://medium.com/@rmcharan?source=follow_footer---------4----------------------------" class="" rel="noopener"><p class="bf b bg bh ge">Ravi Charan</p></a></div></div></span></div></div><span class="bf b bg bh es"><a class="" rel="noopener" href="https://towardsdatascience.com/visualizing-time-series-survival-data-36029652a393?source=follow_footer---------4----------------------------"><p class="bf b bg bh es"><span class="jp">·</span>Jul 18, 2020<svg class="jm jn jo" width="15" height="15" viewBox="0 0 15 15" aria-label="Member only content"><path d="M7.44 2.32c.03-.1.09-.1.12 0l1.2 3.53a.29.29 0 0 0 .26.2h3.88c.11 0 .13.04.04.1L9.8 8.33a.27.27 0 0 0-.1.29l1.2 3.53c.03.1-.01.13-.1.07l-3.14-2.18a.3.3 0 0 0-.32 0L4.2 12.22c-.1.06-.14.03-.1-.07l1.2-3.53a.27.27 0 0 0-.1-.3L2.06 6.16c-.1-.06-.07-.12.03-.12h3.89a.29.29 0 0 0 .26-.19l1.2-3.52z"></path></svg></p></a></span></div></div><div class="nb s"><span class="s"></span><div><div class="fm fv wo gu gv gw"></div><section class="gx gy gz ep ha"><figure class="cl hi v paragraph-image"><a href="https://towardsdatascience.com/visualizing-time-series-survival-data-36029652a393?source=follow_footer---------4----------------------------"><div class="aid"><img alt="" class="v hn ho" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/0_6wkEntAbI5s6rJMh" width="2400" height="1800" role="presentation"></div></a><figcaption class="ox oy gm gk gl oz pa bf b bg bh es" data-selectable-paragraph="">Photo by <a href="https://unsplash.com/@gerandeklerk?utm_source=medium&amp;utm_medium=referral" class="cw mp" rel="noopener nofollow">Geran de Klerk</a> on <a href="https://unsplash.com/?utm_source=medium&amp;utm_medium=referral" class="cw mp" rel="noopener nofollow">Unsplash</a></figcaption></figure><div class=""><h1 id="9eba" class="hp hq hr bf hs aez afa la kl afb afc ld ko afd afe aff afg afh afi afj afk afl afm afn afo in io"><a class="cw ee" rel="noopener" href="https://towardsdatascience.com/visualizing-time-series-survival-data-36029652a393?source=follow_footer---------4----------------------------">Visualizing Time Series Survival Data</a></h1></div><div class=""><h2 id="2e4b" class="ip hq hr bf b agy agz jf iq is it iv iw iy iz jb jc je es">The Kaplan–Meier Curve</h2></div><p id="ed52" class="kw kx hr ky b iq afp la lb it afq ld le lf lg lh li lj lk ll lm ln lo lp lq lr gx io" data-selectable-paragraph="">Let’s imagine you have data on how long subjects in your study “survived.” Survival could be literal (as in a clinical trial) or figurative (if you are studying customer retention, when people stop reading an article, or when a machine breaks down). In order to visualize the data, we’d like to plot a survival curve, called a Kaplan–Meier curve like the one below.</p></section></div></div></div><div><a class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj s" rel="noopener" href="https://towardsdatascience.com/visualizing-time-series-survival-data-36029652a393?readmore=1&amp;source=follow_footer---------4----------------------------"><div class="aft s"><p class="bf b bg bh ge">Read more <span> · 9 min read</span></p></div></a></div></div></div></div><div class="afu s"></div><div class="s"></div><div class="fu s afv"><div class="s"><div class="n p"><div class="au av aw ax ay hb ba v"><div class="n o bz"><div class="n o"><div class="n o bc"><div class="s at jt jv sj sk sl"><div class=""><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="98" aria-labelledby="98"><button class="bq sm sn so fq sp sq vh r ss st"><svg width="25" height="25" viewBox="0 0 25 25" aria-label="clap"><g fill-rule="evenodd"><path d="M11.74 0l.76 2.97.76-2.97zM14.81 3.78l1.84-2.56-1.42-.47zM8.38 1.22l1.84 2.56L9.8.75zM20.38 21.62a5.11 5.11 0 0 1-3.16 1.61l.49-.45c2.88-2.89 3.45-5.98 1.69-9.21l-1.1-1.94-.96-2.02c-.31-.67-.23-1.18.25-1.55a.84.84 0 0 1 .66-.16c.34.05.66.28.88.6l2.85 5.02c1.18 1.97 1.38 5.12-1.6 8.1M7.1 21.1l-5.02-5.02a1 1 0 0 1 .7-1.7 1 1 0 0 1 .72.3l2.6 2.6a.44.44 0 0 0 .63-.62L4.1 14.04l-1.75-1.75a1 1 0 1 1 1.41-1.41l4.15 4.15a.44.44 0 0 0 .63 0 .44.44 0 0 0 0-.62L4.4 10.26 3.22 9.08a1 1 0 0 1 0-1.4 1.02 1.02 0 0 1 1.41 0l1.18 1.16L9.96 13a.44.44 0 0 0 .62 0 .44.44 0 0 0 0-.63L6.43 8.22a.99.99 0 0 1-.3-.7.99.99 0 0 1 .3-.7 1 1 0 0 1 1.41 0l7 6.98a.44.44 0 0 0 .7-.5l-1.35-2.85c-.31-.68-.23-1.19.25-1.56a.85.85 0 0 1 .66-.16c.34.06.66.28.88.6L18.63 14c1.57 2.88 1.07 5.54-1.55 8.16a5.62 5.62 0 0 1-5.06 1.65 9.35 9.35 0 0 1-4.93-2.72zM11 5.98l2.56 2.56c-.5.6-.56 1.41-.15 2.28l.26.56-4.25-4.25a.98.98 0 0 1-.12-.45 1 1 0 0 1 .29-.7 1.02 1.02 0 0 1 1.41 0zm8.89 2.06c-.38-.56-.9-.92-1.49-1.01a1.74 1.74 0 0 0-1.34.33c-.38.29-.61.65-.71 1.06a2.1 2.1 0 0 0-1.1-.56 1.78 1.78 0 0 0-.99.13l-2.64-2.64a1.88 1.88 0 0 0-2.65 0 1.86 1.86 0 0 0-.48.85 1.89 1.89 0 0 0-2.67-.01 1.87 1.87 0 0 0-.5.9c-.76-.75-2-.75-2.7-.04a1.88 1.88 0 0 0 0 2.66c-.3.12-.61.29-.87.55a1.88 1.88 0 0 0 0 2.66l.62.62a1.88 1.88 0 0 0-.9 3.16l5.01 5.02c1.6 1.6 3.52 2.64 5.4 2.96a7.16 7.16 0 0 0 1.18.1c1.03 0 2-.25 2.9-.7A5.9 5.9 0 0 0 21 22.24c3.34-3.34 3.08-6.93 1.74-9.17l-2.87-5.04z"></path></g></svg></button></div></div></div></div><div class="s su sv sw sx sy sz ta"><div class="tb"><p class="bf b bg bh io"><button class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj">123 </button></p></div></div></div><div class="afw n o afx afy afz aga agb"><a class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" rel="noopener" href="https://towardsdatascience.com/visualizing-time-series-survival-data-36029652a393?responsesOpen=true&amp;source=follow_footer---------4----------------------------"><div class="tw s"><button class="fq sn bq"><div class="n o bc"><div class="n o"><svg width="29" height="29" aria-label="responses" class="jm td fq st"><path d="M21.27 20.06a9.04 9.04 0 0 0 2.75-6.68C24.02 8.21 19.67 4 14.1 4S4 8.21 4 13.38c0 5.18 4.53 9.39 10.1 9.39 1 0 2-.14 2.95-.41.28.25.6.49.92.7a7.46 7.46 0 0 0 4.19 1.3c.27 0 .5-.13.6-.35a.63.63 0 0 0-.05-.65 8.08 8.08 0 0 1-1.29-2.58 5.42 5.42 0 0 1-.15-.75zm-3.85 1.32l-.08-.28-.4.12a9.72 9.72 0 0 1-2.84.43c-4.96 0-9-3.71-9-8.27 0-4.55 4.04-8.26 9-8.26 4.95 0 8.77 3.71 8.77 8.27 0 2.25-.75 4.35-2.5 5.92l-.24.21v.32a5.59 5.59 0 0 0 .21 1.29c.19.7.49 1.4.89 2.08a6.43 6.43 0 0 1-2.67-1.06c-.34-.22-.88-.48-1.16-.74z"></path></svg><p class="bf b bg bh io"><span class="tv td">1</span></p></div></div></button></div></a></div></div><div class="n o agc"><div class="kb s"><div class="db" aria-hidden="false" aria-describedby="postFooterSocialMenu" aria-labelledby="postFooterSocialMenu"><div><div class="db" role="tooltip" aria-hidden="false" aria-describedby="99" aria-labelledby="99"><button class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" aria-controls="postFooterSocialMenu" aria-expanded="false" aria-label="Share Post"><svg width="25" height="25" class="r"><g fill-rule="evenodd"><path d="M15.6 5a.42.42 0 0 0 .17-.3.42.42 0 0 0-.12-.33l-2.8-2.79a.5.5 0 0 0-.7 0l-2.8 2.8a.4.4 0 0 0-.1.32c0 .12.07.23.16.3h.02a.45.45 0 0 0 .57-.04l2-2V10c0 .28.23.5.5.5s.5-.22.5-.5V2.93l2.02 2.02c.08.07.18.12.3.13.11.01.21-.02.3-.08v.01"></path><path d="M18 7h-1.5a.5.5 0 0 0 0 1h1.6c.5 0 .9.4.9.9v10.2c0 .5-.4.9-.9.9H6.9a.9.9 0 0 1-.9-.9V8.9c0-.5.4-.9.9-.9h1.6a.5.5 0 0 0 .35-.15A.5.5 0 0 0 9 7.5a.5.5 0 0 0-.15-.35A.5.5 0 0 0 8.5 7H7a2 2 0 0 0-2 2v10c0 1.1.9 2 2 2h11a2 2 0 0 0 2-2V9a2 2 0 0 0-2-2"></path></g></svg></button></div></div></div></div><div class="kb s"><div class="db v" aria-hidden="false"><button aria-controls="addToCatalogBookmarkButton" aria-expanded="false" aria-label="Add to list bookmark button" class="cw cx bl bm bn bo bp bq br bs"><svg width="25" height="25" viewBox="0 0 25 25" fill="none" class="kc"><path d="M18 2.5a.5.5 0 0 1 1 0V5h2.5a.5.5 0 0 1 0 1H19v2.5a.5.5 0 1 1-1 0V6h-2.5a.5.5 0 0 1 0-1H18V2.5zM7 7a1 1 0 0 1 1-1h3.5a.5.5 0 0 0 0-1H8a2 2 0 0 0-2 2v14a.5.5 0 0 0 .8.4l5.7-4.4 5.7 4.4a.5.5 0 0 0 .8-.4v-8.5a.5.5 0 0 0-1 0v7.48l-5.2-4a.5.5 0 0 0-.6 0l-5.2 4V7z" fill="#292929"></path></svg></button></div></div><div class="kd s"><div class="db" aria-hidden="false" aria-describedby="truncatedPostCardReaderMenu" aria-labelledby="truncatedPostCardReaderMenu"><button class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj" aria-controls="truncatedPostCardReaderMenu" aria-expanded="false" aria-label="More options"><svg class="r ke kf" width="25" height="25"><path d="M5 12.5c0 .55.2 1.02.59 1.41.39.4.86.59 1.41.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41A1.93 1.93 0 0 0 7 10.5c-.55 0-1.02.2-1.41.59-.4.39-.59.86-.59 1.41zm5.62 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.42.59.55 0 1.02-.2 1.41-.59.4-.39.59-.86.59-1.41 0-.55-.2-1.02-.59-1.41a1.93 1.93 0 0 0-1.41-.59c-.55 0-1.03.2-1.42.59-.39.39-.58.86-.58 1.41zm5.6 0c0 .55.2 1.02.58 1.41.4.4.87.59 1.43.59.56 0 1.03-.2 1.42-.59.39-.39.58-.86.58-1.41 0-.55-.2-1.02-.58-1.41a1.93 1.93 0 0 0-1.42-.59c-.56 0-1.04.2-1.43.59-.39.39-.58.86-.58 1.41z" fill-rule="evenodd"></path></svg></button></div></div></div></div></div></div></div></div></div><div class="n p"><div class="au av aw ax ay hb ba v"><hr class="sn aep aeq br"></div></div></span><div class="vd tg s"><div class="hn s oy"><a href="https://towardsdatascience.com/?source=follow_footer-------------------------------------" class="bf b bg bh ew ex ey ez fa fb fc bs dv dw fd fe ea eb ec ed db ee" rel="noopener">Read more from <!-- -->Towards Data Science</a></div></div></div></div></div><div class="s gn jz"><div class="n p"><div class="au av aw ax ay az ba v"></div></div></div></div></div></div><div class="wt s wu wv"><div class="n p"><div class="au av aw ax ay az ba v"><div class="n ao"><div class="n bz cd"><a href="https://medium.com/?source=post_page-----41ff868d1794--------------------------------" class="cw dm bl bm bn bo bp bq br bs ww wx bv wy wz" rel="noopener" aria-label="Go to homepage"><svg viewBox="0 0 3940 610" class="ey xa"><path d="M594.79 308.2c0 163.76-131.85 296.52-294.5 296.52S5.8 472 5.8 308.2 137.65 11.69 300.29 11.69s294.5 132.75 294.5 296.51M917.86 308.2c0 154.16-65.93 279.12-147.25 279.12s-147.25-125-147.25-279.12S689.29 29.08 770.61 29.08s147.25 125 147.25 279.12M1050 308.2c0 138.12-23.19 250.08-51.79 250.08s-51.79-112-51.79-250.08 23.19-250.08 51.8-250.08S1050 170.09 1050 308.2M1862.77 37.4l.82-.18v-6.35h-167.48l-155.51 365.5-155.51-365.5h-180.48v6.35l.81.18c30.57 6.9 46.09 17.19 46.09 54.3v434.45c0 37.11-15.58 47.4-46.15 54.3l-.81.18V587H1327v-6.35l-.81-.18c-30.57-6.9-46.09-17.19-46.09-54.3V116.9L1479.87 587h11.33l205.59-483.21V536.9c-2.62 29.31-18 38.36-45.68 44.61l-.82.19v6.3h213.3v-6.3l-.82-.19c-27.71-6.25-43.46-15.3-46.08-44.61l-.14-445.2h.14c0-37.11 15.52-47.4 46.08-54.3m97.43 287.8c3.49-78.06 31.52-134.4 78.56-135.37 14.51.24 26.68 5 36.14 14.16 20.1 19.51 29.55 60.28 28.09 121.21zm-2.11 22h250v-1.05c-.71-59.69-18-106.12-51.34-138-28.82-27.55-71.49-42.71-116.31-42.71h-1c-23.26 0-51.79 5.64-72.09 15.86-23.11 10.7-43.49 26.7-60.45 47.7-27.3 33.83-43.84 79.55-47.86 130.93-.13 1.54-.24 3.08-.35 4.62s-.18 2.92-.25 4.39a332.64 332.64 0 0 0-.36 21.69C1860.79 507 1923.65 600 2035.3 600c98 0 155.07-71.64 169.3-167.8l-7.19-2.53c-25 51.68-69.9 83-121 79.18-69.76-5.22-123.2-75.95-118.35-161.63m532.69 157.68c-8.2 19.45-25.31 30.15-48.24 30.15s-43.89-15.74-58.78-44.34c-16-30.7-24.42-74.1-24.42-125.51 0-107 33.28-176.21 84.79-176.21 21.57 0 38.55 10.7 46.65 29.37zm165.84 76.28c-30.57-7.23-46.09-18-46.09-57V5.28L2424.77 60v6.7l1.14-.09c25.62-2.07 43 1.47 53.09 10.79 7.9 7.3 11.75 18.5 11.75 34.26v71.14c-18.31-11.69-40.09-17.38-66.52-17.38-53.6 0-102.59 22.57-137.92 63.56-36.83 42.72-56.3 101.1-56.3 168.81C2230 518.72 2289.53 600 2378.13 600c51.83 0 93.53-28.4 112.62-76.3V588h166.65v-6.66zm159.29-505.33c0-37.76-28.47-66.24-66.24-66.24-37.59 0-67 29.1-67 66.24s29.44 66.24 67 66.24c37.77 0 66.24-28.48 66.24-66.24m43.84 505.33c-30.57-7.23-46.09-18-46.09-57h-.13V166.65l-166.66 47.85v6.5l1 .09c36.06 3.21 45.93 15.63 45.93 57.77V588h166.8v-6.66zm427.05 0c-30.57-7.23-46.09-18-46.09-57V166.65L3082 212.92v6.52l.94.1c29.48 3.1 38 16.23 38 58.56v226c-9.83 19.45-28.27 31-50.61 31.78-36.23 0-56.18-24.47-56.18-68.9V166.66l-166.66 47.85V221l1 .09c36.06 3.2 45.94 15.62 45.94 57.77v191.27a214.48 214.48 0 0 0 3.47 39.82l3 13.05c14.11 50.56 51.08 77 109 77 49.06 0 92.06-30.37 111-77.89v66h166.66v-6.66zM3934.2 588v-6.67l-.81-.19c-33.17-7.65-46.09-22.07-46.09-51.43v-243.2c0-75.83-42.59-121.09-113.93-121.09-52 0-95.85 30.05-112.73 76.86-13.41-49.6-52-76.86-109.06-76.86-50.12 0-89.4 26.45-106.25 71.13v-69.87l-166.66 45.89v6.54l1 .09c35.63 3.16 45.93 15.94 45.93 57V588h155.5v-6.66l-.82-.2c-26.46-6.22-35-17.56-35-46.66V255.72c7-16.35 21.11-35.72 49-35.72 34.64 0 52.2 24 52.2 71.28V588h155.54v-6.66l-.82-.2c-26.46-6.22-35-17.56-35-46.66v-248a160.45 160.45 0 0 0-2.2-27.68c7.42-17.77 22.34-38.8 51.37-38.8 35.13 0 52.2 23.31 52.2 71.28V588z"></path></svg></a><div class="xb xc n bz xd"><p class="bf b df ej xe"><a href="https://medium.com/about?autoplay=1&amp;source=post_page-----41ff868d1794--------------------------------" class="cw dm bl bm bn bo bp bq br bs xf bv wy wz" rel="noopener">About</a></p><p class="bf b df ej xe"><a href="https://medium.com/new-story?source=post_page-----41ff868d1794--------------------------------" class="cw dm bl bm bn bo bp bq br bs xf bv wy wz" rel="noopener">Write</a></p><p class="bf b df ej xe"><a href="https://help.medium.com/hc/en-us?source=post_page-----41ff868d1794--------------------------------" class="cw dm bl bm bn bo bp bq br bs xf bv wy wz" rel="noopener">Help</a></p><p class="bf b df ej xe"><a href="https://policy.medium.com/medium-terms-of-service-9db0094a1e0f?source=post_page-----41ff868d1794--------------------------------" class="cw dm bl bm bn bo bp bq br bs xf bv wy wz" rel="noopener">Legal</a></p></div></div><div class="aq xg xh xi cb"><p class="bf b df ej xj">Get the Medium app</p></div><div class="aq xk xl cb xm"><div class="xn s"><a href="https://itunes.apple.com/app/medium-everyones-stories/id828256236?pt=698524&amp;mt=8&amp;ct=post_page&amp;source=post_page-----41ff868d1794--------------------------------" class="cw dm bl bm bn bo bp bq br bs ww wx bv wy wz" rel="noopener nofollow"><img alt="A button that says &#39;Download on the App Store&#39;, and if clicked it will lead you to the iOS App store" class="" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_Crl55Tm6yDNMoucPo1tvDg.png" width="135" height="41"></a></div><div class="s"><a href="https://play.google.com/store/apps/details?id=com.medium.reader&amp;source=post_page-----41ff868d1794--------------------------------" class="cw dm bl bm bn bo bp bq br bs ww wx bv wy wz" rel="noopener nofollow"><img alt="A button that says &#39;Get it on, Google Play&#39;, and if clicked it will lead you to the Google Play store" class="" src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1_W_RAPQ62h0em559zluJLdQ.png" width="135" height="41"></a></div></div></div></div></div></div></div></div></div><script>window.__BUILD_ID__="main-20210805-042950-549a828ac2"</script><script>window.__GRAPHQL_URI__ = "https://towardsdatascience.com/_/graphql"</script><script>window.__PRELOADED_STATE__ = {"algolia":{"queries":{}},"auroraPage":{"isAuroraPageEnabled":true},"bookReader":{"assets":{},"reader":{"currentAsset":null,"currentGFI":null,"settingsPanelIsOpen":false,"settings":{"fontFamily":"CHARTER","fontScale":"M","publisherStyling":true,"textAlignment":"start","theme":"White","lineSpacing":0,"wordSpacing":0,"letterSpacing":0},"internalNavCounter":0,"currentSelection":null}},"cache":{"experimentGroupSet":true,"reason":"This request is not using the cache middleware worker","group":"disabled","tags":["group-edgeCachePosts","post-41ff868d1794","user-44a176cd070a","collection-7f60cf5620c9"],"serverVariantState":"","middlewareEnabled":false,"cacheStatus":"DYNAMIC"},"client":{"hydrated":false,"isUs":false,"isNativeMedium":false,"isSafariMobile":false,"isSafari":false,"routingEntity":{"type":"COLLECTION","id":"7f60cf5620c9","explicit":true}},"debug":{"requestId":"32f3a385-f3ac-4e66-b443-fe207c6b0ea0","hybridDevServices":[],"showBookReaderDebugger":false,"originalSpanCarrier":{"ot-tracer-spanid":"1741360371cc4fe0","ot-tracer-traceid":"7866f201849e1371","ot-tracer-sampled":"true"}},"multiVote":{"clapsPerPost":{}},"navigation":{"branch":{"show":null,"hasRendered":null,"blockedByCTA":false},"hideGoogleOneTap":false,"hasRenderedGoogleOneTap":null,"hasRenderedAlternateUserBanner":null,"currentLocation":"https:\u002F\u002Ftowardsdatascience.com\u002Ftext-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794","host":"towardsdatascience.com","hostname":"towardsdatascience.com","referrer":"https:\u002F\u002Fwww.google.com\u002F","hasSetReferrer":false,"susiModal":{"step":null,"operation":"register"},"postRead":false},"tracing":{},"config":{"nodeEnv":"production","version":"main-20210805-042950-549a828ac2","isTaggedVersion":false,"isMediumDotApp":false,"target":"production","productName":"Medium","publicUrl":"https:\u002F\u002Fcdn-client.medium.com\u002Flite","authDomain":"medium.com","authGoogleClientId":"216296035834-k1k6qe060s2tp2a2jam4ljdcms00sttg.apps.googleusercontent.com","favicon":"production","glyphUrl":"https:\u002F\u002Fglyph.medium.com","branchKey":"key_live_ofxXr2qTrrU9NqURK8ZwEhknBxiI6KBm","lightStep":{"name":"lite-web","host":"lightstep.medium.systems","token":"ce5be895bef60919541332990ac9fef2","appVersion":"main-20210805-042950-549a828ac2","disableClientReporting":false},"algolia":{"appId":"MQ57UUUQZ2","apiKeySearch":"394474ced050e3911ae2249ecc774921","indexPrefix":"medium_","host":"-dsn.algolia.net"},"recaptchaKey":"6Lfc37IUAAAAAKGGtC6rLS13R1Hrw_BqADfS1LRk","recaptcha3Key":"6Lf8R9wUAAAAABMI_85Wb8melS7Zj6ziuf99Yot5","datadog":{"applicationId":"6702d87d-a7e0-42fe-bbcb-95b469547ea0","clientToken":"pub853ea8d17ad6821d9f8f11861d23dfed","rumToken":"pubf9cc52896502b9413b68ba36fc0c7162","context":{"deployment":{"target":"production","tag":"main-20210805-042950-549a828ac2","commit":"549a828ac2763cc5a0a35d9d9669f47abe91faf8"}},"datacenter":"us"},"googleAnalyticsCode":"UA-24232453-2","googlePay":{"apiVersion":"2","apiVersionMinor":"0","merchantId":"BCR2DN6TV7EMTGBM","merchantName":"Medium"},"signInWallCustomDomainCollectionIds":["3a8144eabfe3","336d898217ee","61061eb0c96b","138adf9c44c","819cc2aaeee0"],"mediumOwnedAndOperatedCollectionIds":["8a9336e5bb4","b7e45b22fec3","193b68bd4fba","8d6b8a439e32","54c98c43354d","3f6ecf56618","d944778ce714","92d2092dc598","ae2a65f35510","1285ba81cada","544c7006046e","fc8964313712","40187e704f1c","88d9857e584e","7b6769f2748b","bcc38c8f6edf","cef6983b292","cb8577c9149e","444d13b52878","713d7dbc99b0","ef8e90590e66","191186aaafa0","55760f21cdc5","9dc80918cc93","bdc4052bbdba","8ccfed20cbb2"],"tierOneDomains":["medium.com","thebolditalic.com","arcdigital.media","towardsdatascience.com","uxdesign.cc","codeburst.io","psiloveyou.xyz","writingcooperative.com","entrepreneurshandbook.co","prototypr.io","betterhumans.coach.me","theascent.pub"],"topicsToFollow":["d61cf867d93f","8a146bc21b28","1eca0103fff3","4d562ee63426","aef1078a3ef5","e15e46793f8d","6158eb913466","55f1c20aba7a","3d18b94f6858","4861fee224fd","63c6f1f93ee","1d98b3a9a871","decb52b64abf","ae5d4995e225","830cded25262"],"topicToTagMappings":{"accessibility":"accessibility","addiction":"addiction","android-development":"android-development","art":"art","artificial-intelligence":"artificial-intelligence","astrology":"astrology","basic-income":"basic-income","beauty":"beauty","biotech":"biotech","blockchain":"blockchain","books":"books","business":"business","cannabis":"cannabis","cities":"cities","climate-change":"climate-change","comics":"comics","coronavirus":"coronavirus","creativity":"creativity","cryptocurrency":"cryptocurrency","culture":"culture","cybersecurity":"cybersecurity","data-science":"data-science","design":"design","digital-life":"digital-life","disability":"disability","economy":"economy","education":"education","equality":"equality","family":"family","feminism":"feminism","fiction":"fiction","film":"film","fitness":"fitness","food":"food","freelancing":"freelancing","future":"future","gadgets":"gadgets","gaming":"gaming","gun-control":"gun-control","health":"health","history":"history","humor":"humor","immigration":"immigration","ios-development":"ios-development","javascript":"javascript","justice":"justice","language":"language","leadership":"leadership","lgbtqia":"lgbtqia","lifestyle":"lifestyle","machine-learning":"machine-learning","makers":"makers","marketing":"marketing","math":"math","media":"media","mental-health":"mental-health","mindfulness":"mindfulness","money":"money","music":"music","neuroscience":"neuroscience","nonfiction":"nonfiction","outdoors":"outdoors","parenting":"parenting","pets":"pets","philosophy":"philosophy","photography":"photography","podcasts":"podcast","poetry":"poetry","politics":"politics","popular":"popular","privacy":"privacy","product-management":"product-management","productivity":"productivity","programming":"programming","psychedelics":"psychedelics","psychology":"psychology","race":"race","relationships":"relationships","religion":"religion","remote-work":"remote-work","san-francisco":"san-francisco","science":"science","self":"self","self-driving-cars":"self-driving-cars","sexuality":"sexuality","social-media":"social-media","society":"society","software-engineering":"software-engineering","space":"space","spirituality":"spirituality","sports":"sports","startups":"startup","style":"style","technology":"technology","transportation":"transportation","travel":"travel","true-crime":"true-crime","tv":"tv","ux":"ux","venture-capital":"venture-capital","visual-design":"visual-design","work":"work","world":"world","writing":"writing"},"defaultImages":{"avatar":{"imageId":"1*dmbNkD5D-u45r44go_cf0g.png","height":150,"width":150},"orgLogo":{"imageId":"1*OMF3fSqH8t4xBJ9-6oZDZw.png","height":106,"width":545},"postLogo":{"imageId":"1*kFrc4tBFM_tCis-2Ic87WA.png","height":810,"width":1440},"postPreviewImage":{"imageId":"1*hn4v1tCaJy7cWMyb0bpNpQ.png","height":386,"width":579}},"collectionStructuredData":{"8d6b8a439e32":{"name":"Elemental","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F980\u002F1*9ygdqoKprhwuTVKUM0DLPA@2x.png","width":980,"height":159}}},"3f6ecf56618":{"name":"Forge","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F596\u002F1*uULpIlImcO5TDuBZ6lm7Lg@2x.png","width":596,"height":183}}},"ae2a65f35510":{"name":"GEN","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F264\u002F1*RdVZMdvfV3YiZTw6mX7yWA.png","width":264,"height":140}}},"88d9857e584e":{"name":"LEVEL","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*JqYMhNX6KNNb2UlqGqO2WQ.png","width":540,"height":108}}},"7b6769f2748b":{"name":"Marker","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fcdn-images-1.medium.com\u002Fmax\u002F383\u002F1*haCUs0wF6TgOOvfoY-jEoQ@2x.png","width":383,"height":92}}},"444d13b52878":{"name":"OneZero","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*cw32fIqCbRWzwJaoQw6BUg.png","width":540,"height":123}}},"8ccfed20cbb2":{"name":"Zora","data":{"@type":"NewsMediaOrganization","ethicsPolicy":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Farticles\u002F360043290473","logo":{"@type":"ImageObject","url":"https:\u002F\u002Fmiro.medium.com\u002Fmax\u002F540\u002F1*tZUQqRcCCZDXjjiZ4bDvgQ.png","width":540,"height":106}}}},"embeddedPostIds":{"coronavirus":"cd3010f9d81f"},"sharedCdcMessaging":{"COVID_APPLICABLE_TAG_SLUGS":[],"COVID_APPLICABLE_TOPIC_NAMES":[],"COVID_APPLICABLE_TOPIC_NAMES_FOR_TOPIC_PAGE":[],"COVID_MESSAGES":{"tierA":{"text":"For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":66,"end":73,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"tierB":{"text":"Anyone can publish on Medium per our Policies, but we don’t fact-check every story. For more info about the coronavirus, see cdc.gov.","markups":[{"start":37,"end":45,"href":"https:\u002F\u002Fhelp.medium.com\u002Fhc\u002Fen-us\u002Fcategories\u002F201931128-Policies-Safety"},{"start":125,"end":132,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"paywall":{"text":"This article has been made free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":56,"end":70,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":138,"end":145,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]},"unbound":{"text":"This article is free for everyone, thanks to Medium Members. For more information on the novel coronavirus and Covid-19, visit cdc.gov.","markups":[{"start":45,"end":59,"href":"https:\u002F\u002Fmedium.com\u002Fmembership"},{"start":127,"end":134,"href":"https:\u002F\u002Fwww.cdc.gov\u002Fcoronavirus\u002F2019-nCoV"}]}},"COVID_BANNER_POST_ID_OVERRIDE_WHITELIST":["3b31a67bff4a"]},"sharedVoteMessaging":{"TAGS":["politics","election-2020","government","us-politics","election","2020-presidential-race","trump","donald-trump","democrats","republicans","congress","republican-party","democratic-party","biden","joe-biden","maga"],"TOPICS":["politics","election"],"MESSAGE":{"text":"Find out more about the U.S. election results here.","markups":[{"start":46,"end":50,"href":"https:\u002F\u002Fcookpolitical.com\u002F2020-national-popular-vote-tracker"}]},"EXCLUDE_POSTS":["397ef29e3ca5"]},"embedPostRules":[],"recircOptions":{"v1":{"limit":3},"v2":{"limit":8}},"braintreeClientKey":"production_zjkj96jm_m56f8fqpf7ngnrd4","paypalClientId":"AXj1G4fotC2GE8KzWX9mSxCH1wmPE3nJglf4Z2ig_amnhvlMVX87otaq58niAg9iuLktVNF_1WCMnN7v","stripePublishableKey":"pk_live_7FReX44VnNIInZwrIIx6ghjl"},"session":{"xsrf":"882e2576f63f"}}</script><script>window.__APOLLO_STATE__ = {"ROOT_QUERY":{"__typename":"Query","variantFlags":[{"__typename":"VariantFlag","name":"allow_access","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"allow_signup","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"allow_test_auth","valueType":{"__typename":"VariantFlagString","value":"disallow"}},{"__typename":"VariantFlag","name":"android_enable_lock_responses","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"assign_default_topic_to_posts","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"author_fair_distribution_non_qp","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"available_annual_plan","valueType":{"__typename":"VariantFlagString","value":"2c754bcc2995"}},{"__typename":"VariantFlag","name":"available_monthly_plan","valueType":{"__typename":"VariantFlagString","value":"60e220181034"}},{"__typename":"VariantFlag","name":"bane_add_user","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"bane_verify_domain","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"branch_seo_metadata","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"browsable_stream_config_bucket","valueType":{"__typename":"VariantFlagString","value":"curated-topics"}},{"__typename":"VariantFlag","name":"coronavirus_topic_recirc","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"covid_19_cdc_banner","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"default_seo_post_titles","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_android_subscription_activity_carousel","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_ios_subscription_activity_carousel","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_mobile_featured_chunk","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"disable_post_recommended_from_friends_provider","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_android_local_currency","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_annual_renewal_reminder_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_app_flirty_thirty","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_parse_expires_at","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_sign_in","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_apple_webhook_renewal_failure","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_about_page_routing","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_general_admission","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_nav","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_profile_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_pub_follower_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_recirc","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_sticky_nav","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_aurora_tag_page_routing","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_author_autotier","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_author_cards","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_author_cards_byline","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_auto_follow_on_subscribe","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_automated_mission_control_triggers","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_apple_pay","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_client","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_google_pay","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_integration","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_paypal","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_trial_membership","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_braintree_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_branch_io","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_branch_text_me_the_app","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_branding","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_branding_fonts","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_cleansweep_double_writes","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_confirm_sign_in","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_creator_welcome_email","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_cta_meter","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_custom_domain_v2_settings","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_daily_read_digest_promo","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_digest_feature_logging","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_digest_generation_pipeline","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_digest_tagline","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_drm_provider","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_earn_redirect","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ed_picks_first_lo_hp2","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_edit_alt_text","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_email_sign_in_captcha","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_embedding_based_diversification","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_end_of_post_cleanup","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_evhead_com_to_ev_medium_com_redirect","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_expanded_feature_chunk_pool","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_filter_by_resend_rules","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_filter_expire_processor","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_fix_collection_follow_counts","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_fix_follow_counts","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_footer_app_buttons","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_global_susi_modal","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_one_tap","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_webhook","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_webhook_subscription_cancelled","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_webhook_subscription_expired","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_webhook_subscription_in_grace_period","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_webhook_subscription_on_hold","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_google_webhook_subscription_renewed","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_highlander_member_digest","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_hightower_user_bonus","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_hot_topics","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_hybrid_fdh_non_popular_sources","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_iceland_forced_android","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_iceland_forced_ios","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_inline_expansion","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ios_iceland_3","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_json_logs_trained_ranker","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_kbfd_rex","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_kbfd_rex_app_highlights","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_li_homepage_write_cta","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lists","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_continue_this_thread","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_homepage","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_homepage_feed","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_notifications","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_pub_homepage_for_selected_domains","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_response_markup","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_server_upstream_deadlines","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_stories","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_subscribe_to_writer_cta","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_topics","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_topics_edge_cache","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lite_unread_notification_count_mutation","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_lock_responses","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_login_code_flow","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_marketing_emails","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_media_resource_try_catch","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_medium2_kbfd","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_medium_dot_com","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_membership_remove_section_a","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_miro_on_kubernetes","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mission_control","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ml_rank_modules","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ml_rank_rex_anno","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mobile_web_editor_redirect_route","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_mute","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_collaborative_filtering_data","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_member_welcome_email_enhancement","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_new_three_dot_menu","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_newsletter_lo_flow_custom_domains","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_parsely","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_patronus_on_kubernetes","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_popularity_feature","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_page_nav_stickiness_removal","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_page_write_cta","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_post_settings_screen","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_pp_tax_status_clarity","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_primary_topic_for_mobile","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_profile_design_reminder","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_profile_page_seo_titles","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_publish_to_email_for_publication_posts","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rank_v2_usage","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_receipt_notes","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_recirc_reboot","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_responses_all","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_responses_edit_and_delete","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_responses_moderation","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rex_follow_feed_cache","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rito_upstream_deadlines","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_robometric_scanner","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_rtr_channel","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_save_to_medium","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_signup_friction","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_starspace","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_starspace_ranker_starspace","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_subscriber_stats","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_subscription_promos_for_all_users","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tag_feed_urls","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tick_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_ticks_digest_promo","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tipalti_onboarding","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_top_posts_in_fdh_hybrid","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_trending_posts_diversification","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_tribute_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_triton_predictions","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_trumpland_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_twitter_auth_suggestions","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_user_settings_subdomain","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_writer_subscription_landing_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"enable_writer_subscription_to_all_users","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"glyph_font_set","valueType":{"__typename":"VariantFlagString","value":"m2-unbound"}},{"__typename":"VariantFlag","name":"google_sign_in_android","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_generic_home_modules","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_home_post_menu","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_enable_lock_responses","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_iceland_nux","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"ios_pub_follow_email_opt_in","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"is_not_medium_subscriber","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"kill_fastrak","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"kill_stripe_express","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"limit_post_referrers","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"limit_user_follows","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"make_nav_sticky","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"new_transition_page","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"post_edge_cache_enabled","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"post_edge_cache_enabled_moc","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"posts_under_quota_fair_distribution","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"provider_for_credit_card_form","valueType":{"__typename":"VariantFlagString","value":"BRAINTREE"}},{"__typename":"VariantFlag","name":"pub_sidebar","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"redefine_average_post_reading_time","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"remove_post_post_similarity","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"retrained_ranker","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"sign_up_with_email_button","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"signin_services","valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"}},{"__typename":"VariantFlag","name":"signup_services","valueType":{"__typename":"VariantFlagString","value":"twitter,facebook,google,email,google-fastidv,google-one-tap,apple"}},{"__typename":"VariantFlag","name":"single_partition_key","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"skip_sign_in_recaptcha","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"suppress_apple_missing_expires_date_alert","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"unhide_mobile_ctas","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"use_new_admin_topic_backend","valueType":{"__typename":"VariantFlagBoolean","value":true}},{"__typename":"VariantFlag","name":"xgboost_auto_suspend","valueType":{"__typename":"VariantFlagBoolean","value":true}}],"viewer":{"__ref":"User:ad0cdc98db0b"},"meterPost({\"postId\":\"41ff868d1794\",\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\",\"sk\":null,\"source\":null}})":{"__ref":"MeteringInfo:{}"},"postResult({\"id\":\"41ff868d1794\"})":{"__ref":"Post:41ff868d1794"}},"User:ad0cdc98db0b":{"id":"ad0cdc98db0b","__typename":"User","username":"assansanogo","name":"Assan Sanogo","imageId":"0*7GZZawY-4v3DYqIS.","mediumMemberAt":0,"hasPastMemberships":false,"isPartnerProgramEnrolled":false,"email":"assansanogo@gmail.com","unverifiedEmail":"","createdAt":1492771192520,"isAuroraVisible":true,"isEligibleToViewNewResponses":true,"isMembershipTrialEligible":true,"isSuspended":false,"styleEditorOnboardingVersionSeen":0,"allowEmailAddressSharingEditorWriter":false,"hasSubdomain":false,"dismissableFlags":[],"hasWebMembershipTrialEnabled":false,"twitterScreenName":"","geolocation":{"__typename":"Geolocation","country":"FR"},"atsQualifiedAt":0,"readingList":{"__typename":"UserReadingList","counts":{"__typename":"UserReadingListCount","saved":0,"archived":0}}},"MeteringInfo:{}":{"__typename":"MeteringInfo","postIds":["fe0c5dec3ddb","41ff868d1794"],"maxUnlockCount":3,"unlocksRemaining":1},"User:44a176cd070a":{"id":"44a176cd070a","__typename":"User","customStyleSheet":null,"isSuspended":false,"name":"Mauro Di Pietro","bio":"Italian, Data Scientist, Financial Analyst, Good Reader, Bad Writer","imageId":"2*vdOG1aNO6cMj0IYIQuKxDQ.jpeg","hasCompletedProfile":false,"username":"mdipietro09","isAuroraVisible":true,"mediumMemberAt":0,"socialStats":{"__typename":"SocialStats","followerCount":844,"followingCount":16,"collectionFollowingCount":12},"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","domain":"mdipietro09.medium.com","status":"ACTIVE","isSubdomain":true}},"hasSubdomain":true,"viewerEdge":{"__ref":"UserViewerEdge:userId:44a176cd070a-viewerId:ad0cdc98db0b"},"bookAuthor":null,"isPartnerProgramEnrolled":true,"viewerIsUser":false,"newsletterV3":null,"homepagePostsConnection({\"paging\":{\"limit\":1}})":{"__typename":"PostConnection","posts":[{"__ref":"Post:95891008b25f"}]},"allowNotes":true,"twitterScreenName":"maurodp90","followedCollections":12,"atsQualifiedAt":1612205495421},"ImageMetadata:1*ChFMdf--f5jbm-AYv6VdYA@2x.png":{"id":"1*ChFMdf--f5jbm-AYv6VdYA@2x.png","__typename":"ImageMetadata"},"CustomStyleSheet:63d23b36fcaa":{"id":"63d23b36fcaa","__typename":"CustomStyleSheet","global":{"__typename":"GlobalStyles","colorPalette":{"__typename":"StyleSheetColorPalette","primary":{"__typename":"ColorValue","colorPalette":{"__typename":"ColorPalette","highlightSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FFEDF4FC","point":0},{"__typename":"ColorPoint","color":"#FFE9F2FD","point":0.1},{"__typename":"ColorPoint","color":"#FFE6F1FD","point":0.2},{"__typename":"ColorPoint","color":"#FFE2EFFD","point":0.3},{"__typename":"ColorPoint","color":"#FFDFEEFD","point":0.4},{"__typename":"ColorPoint","color":"#FFDBECFE","point":0.5},{"__typename":"ColorPoint","color":"#FFD7EBFE","point":0.6},{"__typename":"ColorPoint","color":"#FFD4E9FE","point":0.7},{"__typename":"ColorPoint","color":"#FFD0E7FF","point":0.8},{"__typename":"ColorPoint","color":"#FFCCE6FF","point":0.9},{"__typename":"ColorPoint","color":"#FFC8E4FF","point":1}]},"defaultBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FF668AAA","point":0},{"__typename":"ColorPoint","color":"#FF61809D","point":0.1},{"__typename":"ColorPoint","color":"#FF5A7690","point":0.2},{"__typename":"ColorPoint","color":"#FF546C83","point":0.3},{"__typename":"ColorPoint","color":"#FF4D6275","point":0.4},{"__typename":"ColorPoint","color":"#FF455768","point":0.5},{"__typename":"ColorPoint","color":"#FF3D4C5A","point":0.6},{"__typename":"ColorPoint","color":"#FF34414C","point":0.7},{"__typename":"ColorPoint","color":"#FF2B353E","point":0.8},{"__typename":"ColorPoint","color":"#FF21282F","point":0.9},{"__typename":"ColorPoint","color":"#FF161B1F","point":1}]},"tintBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FF355876","colorPoints":[{"__typename":"ColorPoint","color":"#FF355876","point":0},{"__typename":"ColorPoint","color":"#FF4D6C88","point":0.1},{"__typename":"ColorPoint","color":"#FF637F99","point":0.2},{"__typename":"ColorPoint","color":"#FF7791A8","point":0.3},{"__typename":"ColorPoint","color":"#FF8CA2B7","point":0.4},{"__typename":"ColorPoint","color":"#FF9FB3C6","point":0.5},{"__typename":"ColorPoint","color":"#FFB2C3D4","point":0.6},{"__typename":"ColorPoint","color":"#FFC5D2E1","point":0.7},{"__typename":"ColorPoint","color":"#FFD7E2EE","point":0.8},{"__typename":"ColorPoint","color":"#FFE9F1FA","point":0.9},{"__typename":"ColorPoint","color":"#FFFBFFFF","point":1}]}}},"background":null},"fonts":{"__typename":"StyleSheetFonts","font1":{"__typename":"StyleSheetFont","name":"SANS_SERIF_1"},"font2":{"__typename":"StyleSheetFont","name":"SANS_SERIF_1"},"font3":{"__typename":"StyleSheetFont","name":"SERIF_2"}}},"header":{"__typename":"HeaderStyles","backgroundColor":{"__typename":"ColorValue","colorPalette":{"__typename":"ColorPalette","tintBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FF355876","colorPoints":[{"__typename":"ColorPoint","color":"#FF355876","point":0},{"__typename":"ColorPoint","color":"#FF4D6C88","point":0.1},{"__typename":"ColorPoint","color":"#FF637F99","point":0.2},{"__typename":"ColorPoint","color":"#FF7791A8","point":0.3},{"__typename":"ColorPoint","color":"#FF8CA2B7","point":0.4},{"__typename":"ColorPoint","color":"#FF9FB3C6","point":0.5},{"__typename":"ColorPoint","color":"#FFB2C3D4","point":0.6},{"__typename":"ColorPoint","color":"#FFC5D2E1","point":0.7},{"__typename":"ColorPoint","color":"#FFD7E2EE","point":0.8},{"__typename":"ColorPoint","color":"#FFE9F1FA","point":0.9},{"__typename":"ColorPoint","color":"#FFFBFFFF","point":1}]}},"rgb":"355876","alpha":"99"},"postBackgroundColor":null,"backgroundImage":{"__ref":"ImageMetadata:1*sfUruIusLq6tbpLx0sDYZQ.png"},"headerScale":"HEADER_SCALE_MEDIUM","horizontalAlignment":"CENTER","backgroundImageDisplayMode":"IMAGE_DISPLAY_MODE_FILL","backgroundImageVerticalAlignment":"END","backgroundColorDisplayMode":"COLOR_DISPLAY_MODE_SOLID","secondaryBackgroundColor":null,"nameColor":null,"nameTreatment":"NAME_TREATMENT_LOGO","postNameTreatment":"NAME_TREATMENT_LOGO","logoImage":{"__ref":"ImageMetadata:1*AGyTPCaRzVqL77kFwUwHKg.png"},"logoScale":"HEADER_SCALE_LARGE","taglineColor":{"__typename":"ColorValue","rgb":"ffffff","alpha":"ff"},"taglineTreatment":"TAGLINE_TREATMENT_HEADER"},"navigation":{"__typename":"HeaderNavigation","navItems":[{"__typename":"HeaderNavigationItem","name":"Editors' Picks","href":null,"type":"NAV_TYPE_TAG","tags":[{"__ref":"Tag:editors-pick"}],"tagSlugs":["editors-pick"]},{"__typename":"HeaderNavigationItem","name":"Features","href":null,"type":"NAV_TYPE_TAG","tags":[{"__ref":"Tag:tds-features"}],"tagSlugs":["tds-features"]},{"__typename":"HeaderNavigationItem","name":"Deep Dives","href":null,"type":"NAV_TYPE_TAG","tags":[{"__ref":"Tag:deep-dives"}],"tagSlugs":["deep-dives"]},{"__typename":"HeaderNavigationItem","name":"Grow","href":"https:\u002F\u002Ftowardsdatascience.com\u002Fhow-to-get-the-most-out-of-towards-data-science-3bf37f75a345","type":"NAV_TYPE_LINK","tags":[],"tagSlugs":[]},{"__typename":"HeaderNavigationItem","name":"Contribute","href":"https:\u002F\u002Ftowardsdatascience.com\u002Fquestions-96667b06af5","type":"NAV_TYPE_LINK","tags":[],"tagSlugs":[]}]},"postBody":null,"blogroll":{"__typename":"BlogrollConfiguration","visibility":"BLOGROLL_VISIBILITY_SIDEBAR"}},"ImageMetadata:1*sfUruIusLq6tbpLx0sDYZQ.png":{"id":"1*sfUruIusLq6tbpLx0sDYZQ.png","__typename":"ImageMetadata","originalWidth":1401},"CollectionViewerEdge:collectionId:7f60cf5620c9-viewerId:ad0cdc98db0b":{"id":"collectionId:7f60cf5620c9-viewerId:ad0cdc98db0b","__typename":"CollectionViewerEdge","isEditor":false},"ImageMetadata:1*mG6i4Bh_LgixUYXJgQpYsg@2x.png":{"id":"1*mG6i4Bh_LgixUYXJgQpYsg@2x.png","__typename":"ImageMetadata","originalWidth":337,"originalHeight":122},"User:7e12c71dfa81":{"id":"7e12c71dfa81","__typename":"User","atsQualifiedAt":1612205680542},"ImageMetadata:1*eLxNtw6hQ4-3HrHda5BCCw.png":{"id":"1*eLxNtw6hQ4-3HrHda5BCCw.png","__typename":"ImageMetadata"},"NewsletterV3:d6fe9076899":{"id":"d6fe9076899","__typename":"NewsletterV3","slug":"the-variable","showPromo":true,"name":"The Variable","description":"Every Thursday, the Variable delivers the very best of Towards Data Science: from hands-on tutorials and cutting-edge research to original features you don't want to miss.","promoHeadline":"","promoBody":"","collection":{"__ref":"Collection:7f60cf5620c9"},"type":"NEWSLETTER_TYPE_COLLECTION","user":{"__ref":"User:895063a310f4"}},"Collection:7f60cf5620c9":{"id":"7f60cf5620c9","__typename":"Collection","domain":"towardsdatascience.com","googleAnalyticsId":null,"slug":"towards-data-science","colorBehavior":"ACCENT_COLOR_AND_FILL_BACKGROUND","isAuroraVisible":true,"favicon":{"__ref":"ImageMetadata:1*ChFMdf--f5jbm-AYv6VdYA@2x.png"},"name":"Towards Data Science","colorPalette":{"__typename":"ColorPalette","highlightSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FFEDF4FC","point":0},{"__typename":"ColorPoint","color":"#FFE9F2FD","point":0.1},{"__typename":"ColorPoint","color":"#FFE6F1FD","point":0.2},{"__typename":"ColorPoint","color":"#FFE2EFFD","point":0.3},{"__typename":"ColorPoint","color":"#FFDFEEFD","point":0.4},{"__typename":"ColorPoint","color":"#FFDBECFE","point":0.5},{"__typename":"ColorPoint","color":"#FFD7EBFE","point":0.6},{"__typename":"ColorPoint","color":"#FFD4E9FE","point":0.7},{"__typename":"ColorPoint","color":"#FFD0E7FF","point":0.8},{"__typename":"ColorPoint","color":"#FFCCE6FF","point":0.9},{"__typename":"ColorPoint","color":"#FFC8E4FF","point":1}]},"defaultBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FFFFFFFF","colorPoints":[{"__typename":"ColorPoint","color":"#FF668AAA","point":0},{"__typename":"ColorPoint","color":"#FF61809D","point":0.1},{"__typename":"ColorPoint","color":"#FF5A7690","point":0.2},{"__typename":"ColorPoint","color":"#FF546C83","point":0.3},{"__typename":"ColorPoint","color":"#FF4D6275","point":0.4},{"__typename":"ColorPoint","color":"#FF455768","point":0.5},{"__typename":"ColorPoint","color":"#FF3D4C5A","point":0.6},{"__typename":"ColorPoint","color":"#FF34414C","point":0.7},{"__typename":"ColorPoint","color":"#FF2B353E","point":0.8},{"__typename":"ColorPoint","color":"#FF21282F","point":0.9},{"__typename":"ColorPoint","color":"#FF161B1F","point":1}]},"tintBackgroundSpectrum":{"__typename":"ColorSpectrum","backgroundColor":"#FF355876","colorPoints":[{"__typename":"ColorPoint","color":"#FF355876","point":0},{"__typename":"ColorPoint","color":"#FF4D6C88","point":0.1},{"__typename":"ColorPoint","color":"#FF637F99","point":0.2},{"__typename":"ColorPoint","color":"#FF7791A8","point":0.3},{"__typename":"ColorPoint","color":"#FF8CA2B7","point":0.4},{"__typename":"ColorPoint","color":"#FF9FB3C6","point":0.5},{"__typename":"ColorPoint","color":"#FFB2C3D4","point":0.6},{"__typename":"ColorPoint","color":"#FFC5D2E1","point":0.7},{"__typename":"ColorPoint","color":"#FFD7E2EE","point":0.8},{"__typename":"ColorPoint","color":"#FFE9F1FA","point":0.9},{"__typename":"ColorPoint","color":"#FFFBFFFF","point":1}]}},"customStyleSheet":{"__ref":"CustomStyleSheet:63d23b36fcaa"},"tagline":"A Medium publication sharing concepts, ideas and codes.","isAuroraEligible":true,"viewerEdge":{"__ref":"CollectionViewerEdge:collectionId:7f60cf5620c9-viewerId:ad0cdc98db0b"},"logo":{"__ref":"ImageMetadata:1*mG6i4Bh_LgixUYXJgQpYsg@2x.png"},"navItems":[{"__typename":"NavItem","title":"Data Science","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fdata-science\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"Machine Learning","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fmachine-learning\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"Programming","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fprogramming\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"Visualization","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fdata-visualization\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"Video","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fvideo\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"★","url":"https:\u002F\u002Ftowardsdatascience.com\u002Feditors-picks\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"About","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fabout-us\u002Fhome","type":"TOPIC_PAGE"},{"__typename":"NavItem","title":"Contribute","url":"https:\u002F\u002Ftowardsdatascience.com\u002Fcontribute\u002Fhome","type":"EXTERNAL_LINK_NAV_ITEM"}],"creator":{"__ref":"User:7e12c71dfa81"},"subscriberCount":566626,"avatar":{"__ref":"ImageMetadata:1*eLxNtw6hQ4-3HrHda5BCCw.png"},"newsletterV3":{"__ref":"NewsletterV3:d6fe9076899"},"canToggleEmail":true,"description":"Your home for data science. A Medium publication sharing concepts, ideas and codes.","ampEnabled":false,"twitterUsername":"TDataScience","facebookPageId":null,"customDomainState":{"__typename":"CustomDomainState","live":{"__typename":"CustomDomain","status":"ACTIVE","isSubdomain":false}},"ptsQualifiedAt":1616092952992},"UserViewerEdge:userId:44a176cd070a-viewerId:ad0cdc98db0b":{"id":"userId:44a176cd070a-viewerId:ad0cdc98db0b","__typename":"UserViewerEdge","createdAt":0,"lastPostCreatedAt":0,"isAllowEdsEnabled":false,"isFollowing":false,"isUser":false},"Post:95891008b25f":{"id":"95891008b25f","__typename":"Post"},"ImageMetadata:1*AGyTPCaRzVqL77kFwUwHKg.png":{"id":"1*AGyTPCaRzVqL77kFwUwHKg.png","__typename":"ImageMetadata","originalWidth":1376,"originalHeight":429},"Tag:editors-pick":{"id":"editors-pick","__typename":"Tag","normalizedTagSlug":"editors-pick"},"Tag:tds-features":{"id":"tds-features","__typename":"Tag","normalizedTagSlug":"tds-features"},"Tag:deep-dives":{"id":"deep-dives","__typename":"Tag","normalizedTagSlug":"deep-dives"},"Topic:1eca0103fff3":{"id":"1eca0103fff3","__typename":"Topic","name":"Machine Learning","slug":"machine-learning"},"Paragraph:3be3dbf9c8b2_0":{"id":"3be3dbf9c8b2_0","__typename":"Paragraph","name":"f836","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*LgqxDMP5qD1HE_uM33zZrg.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_1":{"id":"3be3dbf9c8b2_1","__typename":"Paragraph","name":"7d8a","text":"Text Classification with NLP: Tf-Idf vs Word2Vec vs BERT","type":"H3","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_2":{"id":"3be3dbf9c8b2_2","__typename":"Paragraph","name":"bd0f","text":"Preprocessing, Model Design, Evaluation, Explainability for Bag-of-Words, Word Embedding, Language models","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_3":{"id":"3be3dbf9c8b2_3","__typename":"Paragraph","name":"8f03","text":"Summary","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_4":{"id":"3be3dbf9c8b2_4","__typename":"Paragraph","name":"84c0","text":"In this article, using NLP and Python, I will explain 3 different strategies for text multiclass classification: the old-fashioned Bag-of-Words (with Tf-Idf ), the famous Word Embedding (with Word2Vec), and the cutting edge Language models (with BERT).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":131,"end":144,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":158,"end":160,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":171,"end":187,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":223,"end":239,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_5":{"id":"3be3dbf9c8b2_5","__typename":"Paragraph","name":"bb1b","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*T8WWibd7u8b7gfgeG0LgAA.gif"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_6":{"id":"3be3dbf9c8b2_6","__typename":"Paragraph","name":"d857","text":"NLP (Natural Language Processing) is the field of artificial intelligence that studies the interactions between computers and human languages, in particular how to program computers to process and analyze large amounts of natural language data. NLP is often applied for classifying text data. Text classification is the problem of assigning categories to text data according to its content.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":33,"type":"A","href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FNatural_language_processing","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":0,"end":33,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":293,"end":312,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_7":{"id":"3be3dbf9c8b2_7","__typename":"Paragraph","name":"deee","text":"There are different techniques to extract information from raw text data and use it to train a classification model. This tutorial compares the old school approach of Bag-of-Words (used with a simple machine learning algorithm), the popular Word Embedding model (used with a deep learning neural network), and the state of the art Language models (used with transfer learning from attention-based transformers) that have completely revolutionized the NLP landscape.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":167,"end":179,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":241,"end":255,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":331,"end":346,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_8":{"id":"3be3dbf9c8b2_8","__typename":"Paragraph","name":"d290","text":"I will present some useful Python code that can be easily applied in other similar cases (just copy, paste, run) and walk through every line of code with comments so that you can replicate this example (link to the full code below).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_9":{"id":"3be3dbf9c8b2_9","__typename":"Paragraph","name":"db7e","text":"mdipietro09\u002FDataScience_ArtificialIntelligence_Utils\nPermalink Dismiss GitHub is home to over 50 million developers working together to host and review code, manage…github.com","type":"MIXTAPE_EMBED","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":{"__typename":"MixtapeMetadata","href":"https:\u002F\u002Fgithub.com\u002Fmdipietro09\u002FDataScience_ArtificialIntelligence_Utils\u002Fblob\u002Fmaster\u002Fnatural_language_processing\u002Fexample_text_classification.ipynb","thumbnailImageId":"0*720BmLEupzuQgjj7"},"markups":[{"__typename":"Markup","start":0,"end":175,"type":"A","href":"https:\u002F\u002Fgithub.com\u002Fmdipietro09\u002FDataScience_ArtificialIntelligence_Utils\u002Fblob\u002Fmaster\u002Fnatural_language_processing\u002Fexample_text_classification.ipynb","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":0,"end":52,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":53,"end":165,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_10":{"id":"3be3dbf9c8b2_10","__typename":"Paragraph","name":"27b4","text":"I will use the “News category dataset” in which you are provided with news headlines from the year 2012 to 2018 obtained from HuffPost and you are asked to classify them with the right category, therefore this is a multiclass classification problem (link below).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":16,"end":37,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":126,"end":135,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_11":{"id":"3be3dbf9c8b2_11","__typename":"Paragraph","name":"4467","text":"News Category Dataset\nIdentify the type of news based on headlines and short descriptionswww.kaggle.com","type":"MIXTAPE_EMBED","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":{"__typename":"MixtapeMetadata","href":"https:\u002F\u002Fwww.kaggle.com\u002Frmisra\u002Fnews-category-dataset","thumbnailImageId":"0*PXqNW2Dynndqsmt1"},"markups":[{"__typename":"Markup","start":0,"end":103,"type":"A","href":"https:\u002F\u002Fwww.kaggle.com\u002Frmisra\u002Fnews-category-dataset","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":0,"end":21,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":22,"end":89,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_12":{"id":"3be3dbf9c8b2_12","__typename":"Paragraph","name":"841e","text":"In particular, I will go through:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_13":{"id":"3be3dbf9c8b2_13","__typename":"Paragraph","name":"4920","text":"Setup: import packages, read data, Preprocessing, Partitioning.","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_14":{"id":"3be3dbf9c8b2_14","__typename":"Paragraph","name":"e2a3","text":"Bag-of-Words: Feature Engineering & Feature Selection & Machine Learning with scikit-learn, Testing & Evaluation, Explainability with lime.","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":78,"end":90,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":134,"end":138,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_15":{"id":"3be3dbf9c8b2_15","__typename":"Paragraph","name":"4c08","text":"Word Embedding: Fitting a Word2Vec with gensim, Feature Engineering & Deep Learning with tensorflow\u002Fkeras, Testing & Evaluation, Explainability with the Attention mechanism.","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":40,"end":46,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":89,"end":105,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_16":{"id":"3be3dbf9c8b2_16","__typename":"Paragraph","name":"afd8","text":"Language Models: Feature Engineering with transformers, Transfer Learning from pre-trained BERT with transformers and tensorflow\u002Fkeras, Testing & Evaluation.","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":42,"end":54,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":101,"end":114,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":117,"end":136,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_17":{"id":"3be3dbf9c8b2_17","__typename":"Paragraph","name":"ec62","text":"Setup","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_18":{"id":"3be3dbf9c8b2_18","__typename":"Paragraph","name":"eafa","text":"First of all, I need to import the following libraries:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_19":{"id":"3be3dbf9c8b2_19","__typename":"Paragraph","name":"a49d","text":"## for data\nimport json\nimport pandas as pd\nimport numpy as np","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":12,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":19,"end":24,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":31,"end":38,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":51,"end":57,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_20":{"id":"3be3dbf9c8b2_20","__typename":"Paragraph","name":"9c21","text":"## for plotting\nimport matplotlib.pyplot as plt\nimport seaborn as sns","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":15,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":23,"end":33,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":55,"end":63,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_21":{"id":"3be3dbf9c8b2_21","__typename":"Paragraph","name":"a39f","text":"## for processing\nimport re\nimport nltk","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":18,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":25,"end":27,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":35,"end":39,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_22":{"id":"3be3dbf9c8b2_22","__typename":"Paragraph","name":"10de","text":"## for bag-of-words\nfrom sklearn import feature_extraction, model_selection, naive_bayes, pipeline, manifold, preprocessing","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":19,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":25,"end":33,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_23":{"id":"3be3dbf9c8b2_23","__typename":"Paragraph","name":"2f48","text":"## for explainer\nfrom lime import lime_text","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":16,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":22,"end":27,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_24":{"id":"3be3dbf9c8b2_24","__typename":"Paragraph","name":"80df","text":"## for word embedding\nimport gensim\nimport gensim.downloader as gensim_api","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":21,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":29,"end":36,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_25":{"id":"3be3dbf9c8b2_25","__typename":"Paragraph","name":"7b35","text":"## for deep learning\nfrom tensorflow.keras import models, layers, preprocessing as kprocessing\nfrom tensorflow.keras import backend as K","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":20,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":26,"end":36,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_26":{"id":"3be3dbf9c8b2_26","__typename":"Paragraph","name":"73bd","text":"## for bert language model\nimport transformers","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":26,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":34,"end":46,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_27":{"id":"3be3dbf9c8b2_27","__typename":"Paragraph","name":"9c06","text":"The dataset is contained into a json file, so I will first read it into a list of dictionaries with json and then transform it into a pandas Dataframe.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":100,"end":105,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":134,"end":141,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_28":{"id":"3be3dbf9c8b2_28","__typename":"Paragraph","name":"7f3b","text":"lst_dics = []\nwith open('data.json', mode='r', errors='ignore') as json_file:\n    for dic in json_file:\n        lst_dics.append( json.loads(dic) )","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":19,"end":23,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":133,"end":139,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_29":{"id":"3be3dbf9c8b2_29","__typename":"Paragraph","name":"4125","text":"## print the first one\nlst_dics[0]","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":22,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_30":{"id":"3be3dbf9c8b2_30","__typename":"Paragraph","name":"77b7","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*N7xAYy2MBRJHKMBXnxMi0A.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_31":{"id":"3be3dbf9c8b2_31","__typename":"Paragraph","name":"6721","text":"The original dataset contains over 30 categories, but for the purposes of this tutorial, I will work with a subset of 3: Entertainment, Politics, and Tech.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_32":{"id":"3be3dbf9c8b2_32","__typename":"Paragraph","name":"b876","text":"## create dtf\ndtf = pd.DataFrame(lst_dics)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":13,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_33":{"id":"3be3dbf9c8b2_33","__typename":"Paragraph","name":"09fd","text":"## filter categories\ndtf = dtf[ dtf[\"category\"].isin(['ENTERTAINMENT','POLITICS','TECH']) ][[\"category\",\"headline\"]]","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":20,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":55,"end":68,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":71,"end":79,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":82,"end":86,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_34":{"id":"3be3dbf9c8b2_34","__typename":"Paragraph","name":"0cbe","text":"## rename columns\ndtf = dtf.rename(columns={\"category\":\"y\", \"headline\":\"text\"})","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":17,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":56,"end":57,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":72,"end":76,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_35":{"id":"3be3dbf9c8b2_35","__typename":"Paragraph","name":"da4f","text":"## print 5 random rows\ndtf.sample(5)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":22,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_36":{"id":"3be3dbf9c8b2_36","__typename":"Paragraph","name":"4ac0","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*iurA976CkC9i1Yi1L6hIIw.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_37":{"id":"3be3dbf9c8b2_37","__typename":"Paragraph","name":"c391","text":"In order to understand the composition of the dataset, I am going to look into the univariate distribution of the target by showing labels frequency with a bar plot.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":83,"end":106,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_38":{"id":"3be3dbf9c8b2_38","__typename":"Paragraph","name":"4dac","text":"fig, ax = plt.subplots()\nfig.suptitle(\"y\", fontsize=12)\ndtf[\"y\"].reset_index().groupby(\"y\").count().sort_values(by= \n       \"index\").plot(kind=\"barh\", legend=False, \n        ax=ax).grid(axis='x')\nplt.show()","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":38,"end":41,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":60,"end":63,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":87,"end":90,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_39":{"id":"3be3dbf9c8b2_39","__typename":"Paragraph","name":"0705","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*b7hN7kENZzF4wsck1ne0QA.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_40":{"id":"3be3dbf9c8b2_40","__typename":"Paragraph","name":"a1d5","text":"The dataset is imbalanced: the proportion of Tech news is really small compared to the others, this will make for models to recognize Tech news rather tough.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_41":{"id":"3be3dbf9c8b2_41","__typename":"Paragraph","name":"8362","text":"Before explaining and building the models, I am going to give an example of preprocessing by cleaning text, removing stop words, and applying lemmatization. I will write a function and apply it to the whole data set.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_42":{"id":"3be3dbf9c8b2_42","__typename":"Paragraph","name":"8b11","text":"'''\nPreprocess a string.\n:parameter\n    :param text: string - name of column containing text\n    :param lst_stopwords: list - list of stopwords to remove\n    :param flg_stemm: bool - whether stemming is to be applied\n    :param flg_lemm: bool - whether lemmitisation is to be applied\n:return\n    cleaned text\n'''\ndef utils_preprocess_text(text, flg_stemm=False, flg_lemm=True, lst_stopwords=None):\n    ## clean (convert to lowercase and remove punctuations and   \n    characters and then strip)\n    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())\n            \n    ## Tokenize (convert from string to list)\n    lst_text = text.split()","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":312,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":317,"end":338,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":402,"end":494,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":573,"end":614,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_43":{"id":"3be3dbf9c8b2_43","__typename":"Paragraph","name":"c171","text":"    ## remove Stopwords\n    if lst_stopwords is not None:\n        lst_text = [word for word in lst_text if word not in \n                    lst_stopwords]\n                \n    ## Stemming (remove -ing, -ly, ...)\n    if flg_stemm == True:\n        ps = nltk.stem.porter.PorterStemmer()\n        lst_text = [ps.stem(word) for word in lst_text]\n                \n    ## Lemmatisation (convert the word into root word)\n    if flg_lemm == True:\n        lem = nltk.stem.wordnet.WordNetLemmatizer()\n        lst_text = [lem.lemmatize(word) for word in lst_text]\n            \n    ## back to string from list\n    text = \" \".join(lst_text)\n    return text","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":23,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":176,"end":211,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":361,"end":411,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":568,"end":595,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_44":{"id":"3be3dbf9c8b2_44","__typename":"Paragraph","name":"3136","text":"That function removes a set of words from the corpus if given. I can create a list of generic stop words for the English vocabulary with nltk (we could edit this list by adding or removing words).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":137,"end":142,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_45":{"id":"3be3dbf9c8b2_45","__typename":"Paragraph","name":"72e4","text":"lst_stopwords = nltk.corpus.stopwords.words(\"english\")\nlst_stopwords","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":16,"end":20,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":45,"end":52,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_46":{"id":"3be3dbf9c8b2_46","__typename":"Paragraph","name":"4aa6","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*k1fsJU_S0_WPZku6gg-qOQ.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_47":{"id":"3be3dbf9c8b2_47","__typename":"Paragraph","name":"482f","text":"Now I shall apply the function I wrote on the whole dataset and store the result in a new column named “text_clean” so that you can choose to work with the raw corpus or the preprocessed text.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":104,"end":114,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_48":{"id":"3be3dbf9c8b2_48","__typename":"Paragraph","name":"8fe2","text":"dtf[\"text_clean\"] = dtf[\"text\"].apply(lambda x: \n          utils_preprocess_text(x, flg_stemm=False, flg_lemm=True, \n          lst_stopwords=lst_stopwords))","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":5,"end":15,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":59,"end":80,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":101,"end":114,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":127,"end":154,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_49":{"id":"3be3dbf9c8b2_49","__typename":"Paragraph","name":"4fdd","text":"dtf.head()","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_50":{"id":"3be3dbf9c8b2_50","__typename":"Paragraph","name":"7809","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*t-R6djtHnK4cBVqFrjfLgA.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_51":{"id":"3be3dbf9c8b2_51","__typename":"Paragraph","name":"d7f8","text":"If you are interested in a deeper text analysis and preprocessing, you can check this article. With this in mind, I am going to partition the dataset into training set (70%) and test set (30%) in order to evaluate the models performance.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":81,"end":93,"type":"A","href":"https:\u002F\u002Ftowardsdatascience.com\u002Ftext-analysis-feature-engineering-with-nlp-502d6ea9225d","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_52":{"id":"3be3dbf9c8b2_52","__typename":"Paragraph","name":"00f2","text":"## split dataset\ndtf_train, dtf_test = model_selection.train_test_split(dtf, test_size=0.3)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":16,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":55,"end":71,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_53":{"id":"3be3dbf9c8b2_53","__typename":"Paragraph","name":"4b93","text":"## get target\ny_train = dtf_train[\"y\"].values\ny_test = dtf_test[\"y\"].values","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":13,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":34,"end":37,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":64,"end":67,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_54":{"id":"3be3dbf9c8b2_54","__typename":"Paragraph","name":"e70d","text":"Let’s get started, shall we?","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_55":{"id":"3be3dbf9c8b2_55","__typename":"Paragraph","name":"a81a","text":"Bag-of-Words","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_56":{"id":"3be3dbf9c8b2_56","__typename":"Paragraph","name":"96c7","text":"The Bag-of-Words model is simple: it builds a vocabulary from a corpus of documents and counts how many times the words appear in each document. To put it another way, each word in the vocabulary becomes a feature and a document is represented by a vector with the same length of the vocabulary (a “bag of words”). For instance, let’s take 3 sentences and represent them with this approach:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":4,"end":16,"type":"A","href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FBag-of-words_model","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":4,"end":16,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_57":{"id":"3be3dbf9c8b2_57","__typename":"Paragraph","name":"f222","text":"Feature matrix shape: Number of documents x Length of vocabulary","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*m1O25pvl8R5DlkhuJjRrDw.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":22,"end":41,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":44,"end":64,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":22,"end":42,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":43,"end":64,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_58":{"id":"3be3dbf9c8b2_58","__typename":"Paragraph","name":"f309","text":"As you can imagine, this approach causes a significant dimensionality problem: the more documents you have the larger is the vocabulary, so the feature matrix will be a huge sparse matrix. Therefore, the Bag-of-Words model is usually preceded by an important preprocessing (word cleaning, stop words removal, stemming\u002Flemmatization) aimed to reduce the dimensionality problem.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_59":{"id":"3be3dbf9c8b2_59","__typename":"Paragraph","name":"1375","text":"Terms frequency is not necessarily the best representation for text. In fact, you can find in the corpus common words with the highest frequency but little predictive power over the target variable. To address this problem there is an advanced variant of the Bag-of-Words that, instead of simple counting, uses the term frequency–inverse document frequency (or Tf–Idf). Basically, the value of a word increases proportionally to count, but it is inversely proportional to the frequency of the word in the corpus.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":362,"end":367,"type":"A","href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FTf%E2%80%93idf","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":315,"end":357,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":368,"end":370,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_60":{"id":"3be3dbf9c8b2_60","__typename":"Paragraph","name":"3524","text":"Let’s start with the Feature Engineering, the process to create features by extracting information from the data. I am going to use the Tf-Idf vectorizer with a limit of 10,000 words (so the length of my vocabulary will be 10k), capturing unigrams (i.e. “new” and “york”) and bigrams (i.e. “new york”). I will provide the code for the classic count vectorizer as well:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":21,"end":42,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":255,"end":258,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":265,"end":269,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":291,"end":299,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_61":{"id":"3be3dbf9c8b2_61","__typename":"Paragraph","name":"98fe","text":"## Count (classic BoW)\nvectorizer = feature_extraction.text.CountVectorizer(max_features=10000, ngram_range=(1,2))\n\n## Tf-Idf (advanced variant of BoW)\nvectorizer = feature_extraction.text.TfidfVectorizer(max_features=10000, ngram_range=(1,2))","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":22,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":60,"end":75,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":116,"end":151,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":189,"end":204,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":0,"end":22,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":23,"end":96,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":116,"end":151,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_62":{"id":"3be3dbf9c8b2_62","__typename":"Paragraph","name":"187b","text":"Now I will use the vectorizer on the preprocessed corpus of the train set to extract a vocabulary and create the feature matrix.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_63":{"id":"3be3dbf9c8b2_63","__typename":"Paragraph","name":"1f61","text":"corpus = dtf_train[\"text_clean\"]","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":20,"end":30,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_64":{"id":"3be3dbf9c8b2_64","__typename":"Paragraph","name":"eb41","text":"vectorizer.fit(corpus)\nX_train = vectorizer.transform(corpus)\ndic_vocabulary = vectorizer.vocabulary_","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_65":{"id":"3be3dbf9c8b2_65","__typename":"Paragraph","name":"0d8b","text":"The feature matrix X_train has a shape of 34,265 (Number of documents in training) x 10,000 (Length of vocabulary) and it’s pretty sparse:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":19,"end":27,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_66":{"id":"3be3dbf9c8b2_66","__typename":"Paragraph","name":"12f1","text":"sns.heatmap(X_train.todense()[:,np.random.randint(0,X.shape[1],100)]==0, vmin=0, vmax=1, cbar=False).set_title('Sparse Matrix Sample')","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":4,"end":11,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_67":{"id":"3be3dbf9c8b2_67","__typename":"Paragraph","name":"9e98","text":"Random sample from the feature matrix (non-zero values in black)","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*CpZ9fxPY5iSEzgdyS021_Q.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_68":{"id":"3be3dbf9c8b2_68","__typename":"Paragraph","name":"9df7","text":"In order to know the position of a certain word, we can look it up in the vocabulary:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_69":{"id":"3be3dbf9c8b2_69","__typename":"Paragraph","name":"b534","text":"word = \"new york\"","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_70":{"id":"3be3dbf9c8b2_70","__typename":"Paragraph","name":"d66b","text":"dic_vocabulary[word]","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_71":{"id":"3be3dbf9c8b2_71","__typename":"Paragraph","name":"931e","text":"If the word exists in the vocabulary, this command prints a number N, meaning that the Nth feature of the matrix is that word.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":67,"end":68,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":87,"end":88,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_72":{"id":"3be3dbf9c8b2_72","__typename":"Paragraph","name":"18fd","text":"In order to drop some columns and reduce the matrix dimensionality, we can carry out some Feature Selection, the process of selecting a subset of relevant variables. I will proceed as follows:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":90,"end":107,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_73":{"id":"3be3dbf9c8b2_73","__typename":"Paragraph","name":"d5ae","text":"treat each category as binary (for example, the “Tech” category is 1 for the Tech news and 0 for the others);","type":"OLI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_74":{"id":"3be3dbf9c8b2_74","__typename":"Paragraph","name":"6d94","text":"perform a Chi-Square test to determine whether a feature and the (binary) target are independent;","type":"OLI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":10,"end":25,"type":"A","href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FChi-squared_test","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_75":{"id":"3be3dbf9c8b2_75","__typename":"Paragraph","name":"31b3","text":"keep only the features with a certain p-value from the Chi-Square test.","type":"OLI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_76":{"id":"3be3dbf9c8b2_76","__typename":"Paragraph","name":"3c2d","text":"y = dtf_train[\"y\"]\nX_names = vectorizer.get_feature_names()\np_value_limit = 0.95","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":15,"end":16,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_77":{"id":"3be3dbf9c8b2_77","__typename":"Paragraph","name":"70ae","text":"dtf_features = pd.DataFrame()\nfor cat in np.unique(y):\n    chi2, p = feature_selection.chi2(X_train, y==cat)\n    dtf_features = dtf_features.append(pd.DataFrame(\n                   {\"feature\":X_names, \"score\":1-p, \"y\":cat}))\n    dtf_features = dtf_features.sort_values([\"y\",\"score\"], \n                    ascending=[True,False])\n    dtf_features = dtf_features[dtf_features[\"score\"]\u003Ep_value_limit]","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":87,"end":91,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_78":{"id":"3be3dbf9c8b2_78","__typename":"Paragraph","name":"4aaa","text":"X_names = dtf_features[\"feature\"].unique().tolist()","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_79":{"id":"3be3dbf9c8b2_79","__typename":"Paragraph","name":"cf35","text":"I reduced the number of features from 10,000 to 3,152 by keeping the most statistically relevant ones. Let’s print some:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_80":{"id":"3be3dbf9c8b2_80","__typename":"Paragraph","name":"9c3d","text":"for cat in np.unique(y):\n   print(\"# {}:\".format(cat))\n   print(\"  . selected features:\",\n         len(dtf_features[dtf_features[\"y\"]==cat]))\n   print(\"  . top features:\", \",\".join(\ndtf_features[dtf_features[\"y\"]==cat][\"feature\"].values[:10]))\n   print(\" \")","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_81":{"id":"3be3dbf9c8b2_81","__typename":"Paragraph","name":"511b","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*Fo0EjcD4Ibo2Jz1y6eNL0A.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_82":{"id":"3be3dbf9c8b2_82","__typename":"Paragraph","name":"1761","text":"We can refit the vectorizer on the corpus by giving this new set of words as input. That will produce a smaller feature matrix and a shorter vocabulary.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_83":{"id":"3be3dbf9c8b2_83","__typename":"Paragraph","name":"9a1f","text":"vectorizer = feature_extraction.text.TfidfVectorizer(vocabulary=X_names)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":37,"end":52,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_84":{"id":"3be3dbf9c8b2_84","__typename":"Paragraph","name":"123e","text":"vectorizer.fit(corpus)\nX_train = vectorizer.transform(corpus)\ndic_vocabulary = vectorizer.vocabulary_","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_85":{"id":"3be3dbf9c8b2_85","__typename":"Paragraph","name":"d6e5","text":"The new feature matrix X_train has a shape of is 34,265 (Number of documents in training) x 3,152 (Length of the given vocabulary). Let’s see if the matrix is less sparse:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":23,"end":30,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_86":{"id":"3be3dbf9c8b2_86","__typename":"Paragraph","name":"b7b4","text":"Random sample from the new feature matrix (non-zero values in black)","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*O8lMt_obkHbMXuOSg1bTRA.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_87":{"id":"3be3dbf9c8b2_87","__typename":"Paragraph","name":"3a83","text":"It’s time to train a machine learning model and test it. I recommend using a Naive Bayes algorithm: a probabilistic classifier that makes use of Bayes’ Theorem, a rule that uses probability to make predictions based on prior knowledge of conditions that might be related. This algorithm is the most suitable for such large dataset as it considers each feature independently, calculates the probability of each category, and then predicts the category with the highest probability.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":145,"end":159,"type":"A","href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FBayes%27_theorem","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":21,"end":43,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_88":{"id":"3be3dbf9c8b2_88","__typename":"Paragraph","name":"a2de","text":"classifier = naive_bayes.MultinomialNB()","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":25,"end":38,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_89":{"id":"3be3dbf9c8b2_89","__typename":"Paragraph","name":"4428","text":"I’m going to train this classifier on the feature matrix and then test it on the transformed test set. To that end, I need to build a scikit-learn pipeline: a sequential application of a list of transformations and a final estimator. Putting the Tf-Idf vectorizer and the Naive Bayes classifier in a pipeline allows us to transform and predict test data in just one step.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":134,"end":146,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_90":{"id":"3be3dbf9c8b2_90","__typename":"Paragraph","name":"374d","text":"## pipeline\nmodel = pipeline.Pipeline([(\"vectorizer\", vectorizer),  \n                           (\"classifier\", classifier)])","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":11,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":29,"end":37,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":41,"end":51,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":98,"end":108,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_91":{"id":"3be3dbf9c8b2_91","__typename":"Paragraph","name":"8518","text":"## train classifier\nmodel[\"classifier\"].fit(X_train, y_train)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":20,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_92":{"id":"3be3dbf9c8b2_92","__typename":"Paragraph","name":"f0ee","text":"## test\nX_test = dtf_test[\"text_clean\"].values\npredicted = model.predict(X_test)\npredicted_prob = model.predict_proba(X_test)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":8,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_93":{"id":"3be3dbf9c8b2_93","__typename":"Paragraph","name":"8016","text":"We can now evaluate the performance of the Bag-of-Words model, I will use the following metrics:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":11,"end":35,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_94":{"id":"3be3dbf9c8b2_94","__typename":"Paragraph","name":"97f9","text":"Accuracy: the fraction of predictions the model got right.","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_95":{"id":"3be3dbf9c8b2_95","__typename":"Paragraph","name":"1828","text":"Confusion Matrix: a summary table that breaks down the number of correct and incorrect predictions by each class.","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_96":{"id":"3be3dbf9c8b2_96","__typename":"Paragraph","name":"e253","text":"ROC: a plot that illustrates the true positive rate against the false positive rate at various threshold settings. The area under the curve (AUC) indicates the probability that the classifier will rank a randomly chosen positive observation higher than a randomly chosen negative one.","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_97":{"id":"3be3dbf9c8b2_97","__typename":"Paragraph","name":"c5f5","text":"Precision: the fraction of relevant instances among the retrieved instances.","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_98":{"id":"3be3dbf9c8b2_98","__typename":"Paragraph","name":"7682","text":"Recall: the fraction of the total amount of relevant instances that were actually retrieved.","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_99":{"id":"3be3dbf9c8b2_99","__typename":"Paragraph","name":"4c56","text":"classes = np.unique(y_test)\ny_test_array = pd.get_dummies(y_test, drop_first=False).values\n    ","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_100":{"id":"3be3dbf9c8b2_100","__typename":"Paragraph","name":"52c1","text":"## Accuracy, Precision, Recall\naccuracy = metrics.accuracy_score(y_test, predicted)\nauc = metrics.roc_auc_score(y_test, predicted_prob, \n                            multi_class=\"ovr\")\nprint(\"Accuracy:\",  round(accuracy,2))\nprint(\"Auc:\", round(auc,2))\nprint(\"Detail:\")\nprint(metrics.classification_report(y_test, predicted))\n    \n## Plot confusion matrix\ncm = metrics.confusion_matrix(y_test, predicted)\nfig, ax = plt.subplots()\nsns.heatmap(cm, annot=True, fmt='d', ax=ax, cmap=plt.cm.Blues, \n            cbar=False)\nax.set(xlabel=\"Pred\", ylabel=\"True\", xticklabels=classes, \n       yticklabels=classes, title=\"Confusion matrix\")\nplt.yticks(rotation=0)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":30,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":329,"end":353,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_101":{"id":"3be3dbf9c8b2_101","__typename":"Paragraph","name":"4b72","text":"\nfig, ax = plt.subplots(nrows=1, ncols=2)\n## Plot roc\nfor i in range(len(classes)):\n    fpr, tpr, thresholds = metrics.roc_curve(y_test_array[:,i],  \n                           predicted_prob[:,i])\n    ax[0].plot(fpr, tpr, lw=3, \n              label='{0} (area={1:0.2f})'.format(classes[i], \n                              metrics.auc(fpr, tpr))\n               )\nax[0].plot([0,1], [0,1], color='navy', lw=3, linestyle='--')\nax[0].set(xlim=[-0.05,1.0], ylim=[0.0,1.05], \n          xlabel='False Positive Rate', \n          ylabel=\"True Positive Rate (Recall)\", \n          title=\"Receiver operating characteristic\")\nax[0].legend(loc=\"lower right\")\nax[0].grid(True)\n    \n## Plot precision-recall curve\nfor i in range(len(classes)):\n    precision, recall, thresholds = metrics.precision_recall_curve(\n                 y_test_array[:,i], predicted_prob[:,i])\n    ax[1].plot(recall, precision, lw=3, \n               label='{0} (area={1:0.2f})'.format(classes[i], \n                                  metrics.auc(recall, precision))\n              )\nax[1].set(xlim=[0.0,1.05], ylim=[0.0,1.05], xlabel='Recall', \n          ylabel=\"Precision\", title=\"Precision-Recall curve\")\nax[1].legend(loc=\"best\")\nax[1].grid(True)\nplt.show()","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":42,"end":53,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":666,"end":697,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_102":{"id":"3be3dbf9c8b2_102","__typename":"Paragraph","name":"ff87","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*iPL_8iJOuTJ_mrLvftwUEw.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_103":{"id":"3be3dbf9c8b2_103","__typename":"Paragraph","name":"7722","text":"The BoW model got 85% of the test set right (Accuracy is 0.85), but struggles to recognize Tech news (only 252 predicted correctly).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_104":{"id":"3be3dbf9c8b2_104","__typename":"Paragraph","name":"9d9a","text":"Let’s try to understand why the model classifies news with a certain category and assess the explainability of these predictions. The lime package can help us to build an explainer. To give an illustration, I will take a random observation from the test set and see what the model predicts and why.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":93,"end":108,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":134,"end":139,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_105":{"id":"3be3dbf9c8b2_105","__typename":"Paragraph","name":"31a1","text":"## select observation\ni = 0\ntxt_instance = dtf_test[\"text\"].iloc[i]","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":22,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":53,"end":57,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_106":{"id":"3be3dbf9c8b2_106","__typename":"Paragraph","name":"346e","text":"## check true value and predicted value\nprint(\"True:\", y_test[i], \"--\u003E Pred:\", predicted[i], \"| Prob:\", round(np.max(predicted_prob[i]),2))","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":39,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_107":{"id":"3be3dbf9c8b2_107","__typename":"Paragraph","name":"9694","text":"## show explanation\nexplainer = lime_text.LimeTextExplainer(class_names=\n             np.unique(y_train))\nexplained = explainer.explain_instance(txt_instance, \n             model.predict_proba, num_features=3)\nexplained.show_in_notebook(text=txt_instance, predict_proba=False)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":19,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":42,"end":59,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_108":{"id":"3be3dbf9c8b2_108","__typename":"Paragraph","name":"6a52","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*CFZTX1Ud0jOwNZMhrr4cWA.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_109":{"id":"3be3dbf9c8b2_109","__typename":"Paragraph","name":"20dd","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*qcAV1wvucxNogDz_eh3dpQ.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_110":{"id":"3be3dbf9c8b2_110","__typename":"Paragraph","name":"cf12","text":"That makes sense: the words “Clinton” and “GOP” pointed the model in the right direction (Politics news) even if the word “Stage” is more common among Entertainment news.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":29,"end":36,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":43,"end":46,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":123,"end":128,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_111":{"id":"3be3dbf9c8b2_111","__typename":"Paragraph","name":"b371","text":"Word Embedding","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_112":{"id":"3be3dbf9c8b2_112","__typename":"Paragraph","name":"58f4","text":"Word Embedding is the collective name for feature learning techniques where words from the vocabulary are mapped to vectors of real numbers. These vectors are calculated from the probability distribution for each word appearing before or after another. To put it another way, words of the same context usually appear together in the corpus, so they will be close in the vector space as well. For instance, let’s take the 3 sentences from the previous example:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":14,"type":"A","href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FWord_embedding","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":0,"end":14,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_113":{"id":"3be3dbf9c8b2_113","__typename":"Paragraph","name":"8752","text":"Words embedded in 2D vector space","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*u67szEvNSMqrQeitdahw_A.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_114":{"id":"3be3dbf9c8b2_114","__typename":"Paragraph","name":"efe4","text":"In this tutorial, I’m going to use the first model of this family: Google’s Word2Vec (2013). Other popular Word Embedding models are Stanford’s GloVe (2014) and Facebook’s FastText (2016).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":76,"end":84,"type":"A","href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FWord2vec","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":144,"end":149,"type":"A","href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FGloVe_(machine_learning)","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":172,"end":180,"type":"A","href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FFastText","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":76,"end":85,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":144,"end":150,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":156,"end":157,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":172,"end":180,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_115":{"id":"3be3dbf9c8b2_115","__typename":"Paragraph","name":"b046","text":"Word2Vec produces a vector space, typically of several hundred dimensions, with each unique word in the corpus such that words that share common contexts in the corpus are located close to one another in the space. That can be done using 2 different approaches: starting from a single word to predict its context (Skip-gram) or starting from the context to predict a word (Continuous Bag-of-Words).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":9,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":314,"end":323,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":373,"end":396,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_116":{"id":"3be3dbf9c8b2_116","__typename":"Paragraph","name":"3453","text":"In Python, you can load a pre-trained Word Embedding model from genism-data like this:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":64,"end":75,"type":"A","href":"https:\u002F\u002Fgithub.com\u002FRaRe-Technologies\u002Fgensim-data","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":64,"end":76,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_117":{"id":"3be3dbf9c8b2_117","__typename":"Paragraph","name":"fb2e","text":"nlp = gensim_api.load(\"word2vec-google-news-300\")","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":23,"end":48,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_118":{"id":"3be3dbf9c8b2_118","__typename":"Paragraph","name":"3b3c","text":"Instead of using a pre-trained model, I am going to fit my own Word2Vec on the training data corpus with gensim. Before fitting the model, the corpus needs to be transformed into a list of lists of n-grams. In this particular case, I’ll try to capture unigrams (“york”), bigrams (“new york”), and trigrams (“new york city”).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":105,"end":112,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":263,"end":267,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":281,"end":289,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":308,"end":321,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_119":{"id":"3be3dbf9c8b2_119","__typename":"Paragraph","name":"eabd","text":"corpus = dtf_train[\"text_clean\"]","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":20,"end":30,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_120":{"id":"3be3dbf9c8b2_120","__typename":"Paragraph","name":"c998","text":"\n## create list of lists of unigrams\nlst_corpus = []\nfor string in corpus:\n   lst_words = string.split()\n   lst_grams = [\" \".join(lst_words[i:i+1]) \n               for i in range(0, len(lst_words), 1)]\n   lst_corpus.append(lst_grams)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":36,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_121":{"id":"3be3dbf9c8b2_121","__typename":"Paragraph","name":"ade3","text":"\n## detect bigrams and trigrams\nbigrams_detector = gensim.models.phrases.Phrases(lst_corpus, \n                 delimiter=\" \".encode(), min_count=5, threshold=10)\nbigrams_detector = gensim.models.phrases.Phraser(bigrams_detector)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":31,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":73,"end":80,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":203,"end":210,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_122":{"id":"3be3dbf9c8b2_122","__typename":"Paragraph","name":"3024","text":"trigrams_detector = gensim.models.phrases.Phrases(bigrams_detector[lst_corpus], \n            delimiter=\" \".encode(), min_count=5, threshold=10)\ntrigrams_detector = gensim.models.phrases.Phraser(trigrams_detector)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":42,"end":49,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":186,"end":193,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_123":{"id":"3be3dbf9c8b2_123","__typename":"Paragraph","name":"c423","text":"When fitting the Word2Vec, you need to specify:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_124":{"id":"3be3dbf9c8b2_124","__typename":"Paragraph","name":"8700","text":"the target size of the word vectors, I’ll use 300;","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_125":{"id":"3be3dbf9c8b2_125","__typename":"Paragraph","name":"f753","text":"the window, or the maximum distance between the current and predicted word within a sentence, I’ll use the mean length of text in the corpus;","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_126":{"id":"3be3dbf9c8b2_126","__typename":"Paragraph","name":"e0a7","text":"the training algorithm, I’ll use skip-grams (sg=1) as in general it has better results.","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_127":{"id":"3be3dbf9c8b2_127","__typename":"Paragraph","name":"a5f2","text":"## fit w2v\nnlp = gensim.models.word2vec.Word2Vec(lst_corpus, size=300,   \n            window=8, min_count=1, sg=1, iter=30)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":10,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":40,"end":48,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_128":{"id":"3be3dbf9c8b2_128","__typename":"Paragraph","name":"9959","text":"We have our embedding model, so we can select any word from the corpus and transform it into a vector.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_129":{"id":"3be3dbf9c8b2_129","__typename":"Paragraph","name":"c003","text":"word = \"data\"\nnlp[word].shape","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_130":{"id":"3be3dbf9c8b2_130","__typename":"Paragraph","name":"3b95","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*nMBpatACacNe1SZYF39j3Q.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_131":{"id":"3be3dbf9c8b2_131","__typename":"Paragraph","name":"d20c","text":"We can even use it to visualize a word and its context into a smaller dimensional space (2D or 3D) by applying any dimensionality reduction algorithm (i.e. TSNE).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":156,"end":160,"type":"A","href":"https:\u002F\u002Fscikit-learn.org\u002Fstable\u002Fmodules\u002Fgenerated\u002Fsklearn.manifold.TSNE.html","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_132":{"id":"3be3dbf9c8b2_132","__typename":"Paragraph","name":"d8d1","text":"word = \"data\"\nfig = plt.figure()","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_133":{"id":"3be3dbf9c8b2_133","__typename":"Paragraph","name":"408e","text":"## word embedding\ntot_words = [word] + [tupla[0] for tupla in \n                 nlp.most_similar(word, topn=20)]\nX = nlp[tot_words]","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":17,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_134":{"id":"3be3dbf9c8b2_134","__typename":"Paragraph","name":"a680","text":"## pca to reduce dimensionality from 300 to 3\npca = manifold.TSNE(perplexity=40, n_components=3, init='pca')\nX = pca.fit_transform(X)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":45,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":61,"end":65,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_135":{"id":"3be3dbf9c8b2_135","__typename":"Paragraph","name":"c7cd","text":"## create dtf\ndtf_ = pd.DataFrame(X, index=tot_words, columns=[\"x\",\"y\",\"z\"])\ndtf_[\"input\"] = 0\ndtf_[\"input\"].iloc[0:1] = 1","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":13,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_136":{"id":"3be3dbf9c8b2_136","__typename":"Paragraph","name":"618e","text":"## plot 3d\nfrom mpl_toolkits.mplot3d import Axes3D\nax = fig.add_subplot(111, projection='3d')\nax.scatter(dtf_[dtf_[\"input\"]==0]['x'], \n           dtf_[dtf_[\"input\"]==0]['y'], \n           dtf_[dtf_[\"input\"]==0]['z'], c=\"black\")\nax.scatter(dtf_[dtf_[\"input\"]==1]['x'], \n           dtf_[dtf_[\"input\"]==1]['y'], \n           dtf_[dtf_[\"input\"]==1]['z'], c=\"red\")\nax.set(xlabel=None, ylabel=None, zlabel=None, xticklabels=[], \n       yticklabels=[], zticklabels=[])\nfor label, row in dtf_[[\"x\",\"y\",\"z\"]].iterrows():\n    x, y, z = row\n    ax.text(x, y, z, s=label)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":10,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_137":{"id":"3be3dbf9c8b2_137","__typename":"Paragraph","name":"ea42","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*T8WWibd7u8b7gfgeG0LgAA.gif"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_138":{"id":"3be3dbf9c8b2_138","__typename":"Paragraph","name":"2637","text":"That’s pretty cool and all, but how can the word embedding be useful to predict the news category? Well, the word vectors can be used in a neural network as weights. This is how:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_139":{"id":"3be3dbf9c8b2_139","__typename":"Paragraph","name":"6309","text":"First, transform the corpus into padded sequences of word ids to get a feature matrix.","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_140":{"id":"3be3dbf9c8b2_140","__typename":"Paragraph","name":"1700","text":"Then, create an embedding matrix so that the vector of the word with id N is located at the Nth row.","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":72,"end":74,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":92,"end":95,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_141":{"id":"3be3dbf9c8b2_141","__typename":"Paragraph","name":"2904","text":"Finally, build a neural network with an embedding layer that weighs every word in the sequences with the corresponding vector.","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_142":{"id":"3be3dbf9c8b2_142","__typename":"Paragraph","name":"7f89","text":"Let’s start with the Feature Engineering by transforming the same preprocessed corpus (list of lists of n-grams) given to the Word2Vec into a list of sequences using tensorflow\u002Fkeras:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":21,"end":41,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":166,"end":182,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_143":{"id":"3be3dbf9c8b2_143","__typename":"Paragraph","name":"17e9","text":"## tokenize text\ntokenizer = kprocessing.text.Tokenizer(lower=True, split=' ', \n                     oov_token=\"NaN\", \n                     filters='!\"#$%&()*+,-.\u002F:;\u003C=\u003E?@[\\\\]^_`{|}~\\t\\n')\ntokenizer.fit_on_texts(lst_corpus)\ndic_vocabulary = tokenizer.word_index","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":16,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":46,"end":55,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_144":{"id":"3be3dbf9c8b2_144","__typename":"Paragraph","name":"2793","text":"## create sequence\nlst_text2seq= tokenizer.texts_to_sequences(lst_corpus)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":18,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_145":{"id":"3be3dbf9c8b2_145","__typename":"Paragraph","name":"495c","text":"## padding sequence\nX_train = kprocessing.sequence.pad_sequences(lst_text2seq, \n                    maxlen=15, padding=\"post\", truncating=\"post\")","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":19,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":51,"end":64,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_146":{"id":"3be3dbf9c8b2_146","__typename":"Paragraph","name":"5ade","text":"The feature matrix X_train has a shape of 34,265 x 15 (Number of sequences x Sequences max length). Let’s visualize it:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":19,"end":26,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_147":{"id":"3be3dbf9c8b2_147","__typename":"Paragraph","name":"4fcd","text":"sns.heatmap(X_train==0, vmin=0, vmax=1, cbar=False)\nplt.show()","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_148":{"id":"3be3dbf9c8b2_148","__typename":"Paragraph","name":"e7c9","text":"Feature matrix (34,265 x 15)","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*tQ588zm4i96xfEBCfiUpzQ.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_149":{"id":"3be3dbf9c8b2_149","__typename":"Paragraph","name":"23eb","text":"Every text in the corpus is now an id sequence with length 15. For instance, if a text had 10 tokens in it, then the sequence is composed of 10 ids + 5 0s, which is the padding element (while the id for word not in the vocabulary is 1). Let’s print how a text from the train set has been transformed into a sequence with the padding and the vocabulary.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_150":{"id":"3be3dbf9c8b2_150","__typename":"Paragraph","name":"8b50","text":"i = 0\n\n## list of text: [\"I like this\", ...]\nlen_txt = len(dtf_train[\"text_clean\"].iloc[i].split())\nprint(\"from: \", dtf_train[\"text_clean\"].iloc[i], \"| len:\", len_txt)\n\n## sequence of token ids: [[1, 2, 3], ...]\nlen_tokens = len(X_train[i])\nprint(\"to: \", X_train[i], \"| len:\", len(X_train[i]))\n\n## vocabulary: {\"I\":1, \"like\":2, \"this\":3, ...}\nprint(\"check: \", dtf_train[\"text_clean\"].iloc[i].split()[0], \n      \" -- idx in vocabulary --\u003E\", \n      dic_vocabulary[dtf_train[\"text_clean\"].iloc[i].split()[0]])\n\nprint(\"vocabulary: \", dict(list(dic_vocabulary.items())[0:5]), \"... (padding element, 0)\")","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":7,"end":44,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":169,"end":211,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":295,"end":342,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_151":{"id":"3be3dbf9c8b2_151","__typename":"Paragraph","name":"c3da","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*vhJWO3cTN2jfrg2upX2kGQ.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_152":{"id":"3be3dbf9c8b2_152","__typename":"Paragraph","name":"a07a","text":"Before moving on, don’t forget to do the same feature engineering on the test set as well:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_153":{"id":"3be3dbf9c8b2_153","__typename":"Paragraph","name":"2ebc","text":"corpus = dtf_test[\"text_clean\"]","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":19,"end":29,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_154":{"id":"3be3dbf9c8b2_154","__typename":"Paragraph","name":"220a","text":"\n## create list of n-grams\nlst_corpus = []\nfor string in corpus:\n    lst_words = string.split()\n    lst_grams = [\" \".join(lst_words[i:i+1]) for i in range(0, \n                 len(lst_words), 1)]\n    lst_corpus.append(lst_grams)\n    ","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":26,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_155":{"id":"3be3dbf9c8b2_155","__typename":"Paragraph","name":"791a","text":"## detect common bigrams and trigrams using the fitted detectors\nlst_corpus = list(bigrams_detector[lst_corpus])\nlst_corpus = list(trigrams_detector[lst_corpus])\n","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":64,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_156":{"id":"3be3dbf9c8b2_156","__typename":"Paragraph","name":"3d90","text":"## text to sequence with the fitted tokenizer\nlst_text2seq = tokenizer.texts_to_sequences(lst_corpus)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":45,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_157":{"id":"3be3dbf9c8b2_157","__typename":"Paragraph","name":"51fd","text":"\n## padding sequence\nX_test = kprocessing.sequence.pad_sequences(lst_text2seq, maxlen=15,\n             padding=\"post\", truncating=\"post\")","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":1,"end":20,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":51,"end":64,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_158":{"id":"3be3dbf9c8b2_158","__typename":"Paragraph","name":"f0d6","text":"X_test (14,697 x 15)","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*H6ZZAT-9tmu8PcaY74Un6w.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_159":{"id":"3be3dbf9c8b2_159","__typename":"Paragraph","name":"a0f8","text":"We’ve got our X_train and X_test, now we need to create the matrix of embedding that will be used as a weight matrix in the neural network classifier.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":60,"end":79,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":14,"end":21,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":26,"end":32,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_160":{"id":"3be3dbf9c8b2_160","__typename":"Paragraph","name":"319f","text":"## start the matrix (length of vocabulary x vector size) with all 0s\nembeddings = np.zeros((len(dic_vocabulary)+1, 300))","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":68,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_161":{"id":"3be3dbf9c8b2_161","__typename":"Paragraph","name":"b807","text":"for word,idx in dic_vocabulary.items():\n    ## update the row with vector\n    try:\n        embeddings[idx] =  nlp[word]\n    ## if word not in model then skip and the row stays all 0s\n    except:\n        pass","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":44,"end":73,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":124,"end":182,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_162":{"id":"3be3dbf9c8b2_162","__typename":"Paragraph","name":"2aa4","text":"That code generates a matrix of shape 22,338 x 300 (Length of vocabulary extracted from the corpus x Vector size). It can be navigated by word id, which can be obtained from the vocabulary.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_163":{"id":"3be3dbf9c8b2_163","__typename":"Paragraph","name":"d103","text":"word = \"data\"","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_164":{"id":"3be3dbf9c8b2_164","__typename":"Paragraph","name":"80ad","text":"print(\"dic[word]:\", dic_vocabulary[word], \"|idx\")\nprint(\"embeddings[idx]:\", embeddings[dic_vocabulary[word]].shape, \n      \"|vector\")","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_165":{"id":"3be3dbf9c8b2_165","__typename":"Paragraph","name":"46b1","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*EWLnzZ0abpnhH1zR8jq5PQ.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_166":{"id":"3be3dbf9c8b2_166","__typename":"Paragraph","name":"f4fc","text":"It’s finally time to build a deep learning model. I’m going to use the embedding matrix in the first Embedding layer of the neural network that I will build and train to classify the news. Each id in the input sequence will be used as the index to access the embedding matrix. The output of this Embedding layer will be a 2D matrix with a word vector for each word id in the input sequence (Sequence length x Vector size). Let’s use the sentence “I like this article” as an example:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":29,"end":48,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":447,"end":466,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_167":{"id":"3be3dbf9c8b2_167","__typename":"Paragraph","name":"0cdb","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*KvBp0xzRThA7qTXACT4A-g.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_168":{"id":"3be3dbf9c8b2_168","__typename":"Paragraph","name":"e0c6","text":"My neural network shall be structured as follows:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_169":{"id":"3be3dbf9c8b2_169","__typename":"Paragraph","name":"e79b","text":"an Embedding layer that takes the sequences as input and the word vectors as weights, just as described before.","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_170":{"id":"3be3dbf9c8b2_170","__typename":"Paragraph","name":"b13f","text":"A simple Attention layer that won’t affect the predictions but it’s going to capture the weights of each instance and allow us to build a nice explainer (it isn't necessary for the predictions, just for the explainability, so you can skip it). The Attention mechanism was presented in this paper (2014) as a solution to the problem of the sequence models (i.e. LSTM) to understand what parts of a long text are actually relevant.","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":285,"end":295,"type":"A","href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1409.0473","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_171":{"id":"3be3dbf9c8b2_171","__typename":"Paragraph","name":"1d19","text":"Two layers of Bidirectional LSTM to model the order of words in a sequence in both directions.","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_172":{"id":"3be3dbf9c8b2_172","__typename":"Paragraph","name":"e566","text":"Two final dense layers that will predict the probability of each news category.","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_173":{"id":"3be3dbf9c8b2_173","__typename":"Paragraph","name":"3c10","text":"## code attention layer\ndef attention_layer(inputs, neurons):\n    x = layers.Permute((2,1))(inputs)\n    x = layers.Dense(neurons, activation=\"softmax\")(x)\n    x = layers.Permute((2,1), name=\"attention\")(x)\n    x = layers.multiply([inputs, x])\n    return x","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":23,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":28,"end":43,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":77,"end":84,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":115,"end":120,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":170,"end":177,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":191,"end":200,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":221,"end":229,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_174":{"id":"3be3dbf9c8b2_174","__typename":"Paragraph","name":"7930","text":"\n## input\nx_in = layers.Input(shape=(15,))","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":9,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":24,"end":29,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_175":{"id":"3be3dbf9c8b2_175","__typename":"Paragraph","name":"d8c6","text":"## embedding\nx = layers.Embedding(input_dim=embeddings.shape[0],  \n                     output_dim=embeddings.shape[1], \n                     weights=[embeddings],\n                     input_length=15, trainable=False)(x_in)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":12,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":24,"end":33,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_176":{"id":"3be3dbf9c8b2_176","__typename":"Paragraph","name":"5041","text":"## apply attention\nx = attention_layer(x, neurons=15)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":18,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_177":{"id":"3be3dbf9c8b2_177","__typename":"Paragraph","name":"70c4","text":"## 2 layers of bidirectional lstm\nx = layers.Bidirectional(layers.LSTM(units=15, dropout=0.2, \n                         return_sequences=True))(x)\nx = layers.Bidirectional(layers.LSTM(units=15, dropout=0.2))(x)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":33,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":45,"end":58,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":66,"end":70,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":158,"end":171,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":179,"end":183,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_178":{"id":"3be3dbf9c8b2_178","__typename":"Paragraph","name":"9ebc","text":"## final dense layers\nx = layers.Dense(64, activation='relu')(x)\ny_out = layers.Dense(3, activation='softmax')(x)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":21,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":33,"end":38,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":80,"end":85,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_179":{"id":"3be3dbf9c8b2_179","__typename":"Paragraph","name":"34a7","text":"## compile\nmodel = models.Model(x_in, y_out)\nmodel.compile(loss='sparse_categorical_crossentropy',\n              optimizer='adam', metrics=['accuracy'])\n\nmodel.summary()","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":10,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":26,"end":31,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_180":{"id":"3be3dbf9c8b2_180","__typename":"Paragraph","name":"0e8f","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*Dfu-2hqEMaBe6YHx1C71Uw.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_181":{"id":"3be3dbf9c8b2_181","__typename":"Paragraph","name":"9280","text":"Now we can train the model and check the performance on a subset of the training set used for validation before testing it on the actual test set.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_182":{"id":"3be3dbf9c8b2_182","__typename":"Paragraph","name":"523e","text":"## encode y\ndic_y_mapping = {n:label for n,label in \n                 enumerate(np.unique(y_train))}\ninverse_dic = {v:k for k,v in dic_y_mapping.items()}\ny_train = np.array([inverse_dic[y] for y in y_train])","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":11,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_183":{"id":"3be3dbf9c8b2_183","__typename":"Paragraph","name":"7971","text":"## train\ntraining = model.fit(x=X_train, y=y_train, batch_size=256, \n                     epochs=10, shuffle=True, verbose=0, \n                     validation_split=0.3)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":8,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_184":{"id":"3be3dbf9c8b2_184","__typename":"Paragraph","name":"40ef","text":"## plot loss and accuracy\nmetrics = [k for k in training.history.keys() if (\"loss\" not in k) and (\"val\" not in k)]\nfig, ax = plt.subplots(nrows=1, ncols=2, sharey=True)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":25,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_185":{"id":"3be3dbf9c8b2_185","__typename":"Paragraph","name":"be93","text":"ax[0].set(title=\"Training\")\nax11 = ax[0].twinx()\nax[0].plot(training.history['loss'], color='black')\nax[0].set_xlabel('Epochs')\nax[0].set_ylabel('Loss', color='black')\nfor metric in metrics:\n    ax11.plot(training.history[metric], label=metric)\nax11.set_ylabel(\"Score\", color='steelblue')\nax11.legend()","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_186":{"id":"3be3dbf9c8b2_186","__typename":"Paragraph","name":"19f7","text":"ax[1].set(title=\"Validation\")\nax22 = ax[1].twinx()\nax[1].plot(training.history['val_loss'], color='black')\nax[1].set_xlabel('Epochs')\nax[1].set_ylabel('Loss', color='black')\nfor metric in metrics:\n     ax22.plot(training.history['val_'+metric], label=metric)\nax22.set_ylabel(\"Score\", color=\"steelblue\")\nplt.show()","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_187":{"id":"3be3dbf9c8b2_187","__typename":"Paragraph","name":"7520","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*MdYjdeOju4ez8gcAc-v8qA.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_188":{"id":"3be3dbf9c8b2_188","__typename":"Paragraph","name":"759d","text":"Nice! In some epochs, the accuracy reached 0.89. In order to complete the evaluation of the Word Embedding model, let’s predict the test set and compare the same metrics used before (code for metrics is the same as before).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":74,"end":84,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_189":{"id":"3be3dbf9c8b2_189","__typename":"Paragraph","name":"f7a4","text":"## test\npredicted_prob = model.predict(X_test)\npredicted = [dic_y_mapping[np.argmax(pred)] for pred in \n             predicted_prob]","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":7,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_190":{"id":"3be3dbf9c8b2_190","__typename":"Paragraph","name":"2879","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*a39MMTNXnDaFOKFur2Z7xQ.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_191":{"id":"3be3dbf9c8b2_191","__typename":"Paragraph","name":"8636","text":"The model performs as good as the previous one, in fact, it also struggles to classify Tech news.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_192":{"id":"3be3dbf9c8b2_192","__typename":"Paragraph","name":"a333","text":"But is it explainable as well? Yes, it is! I put an Attention layer in the neural network to extract the weights of each word and understand how much those contributed to classify an instance. So I’ll try to use Attention weights to build an explainer (similar to the one seen in the previous section):","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":10,"end":22,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_193":{"id":"3be3dbf9c8b2_193","__typename":"Paragraph","name":"855e","text":"## select observation\ni = 0\ntxt_instance = dtf_test[\"text\"].iloc[i]","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":22,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":53,"end":57,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_194":{"id":"3be3dbf9c8b2_194","__typename":"Paragraph","name":"6749","text":"## check true value and predicted value\nprint(\"True:\", y_test[i], \"--\u003E Pred:\", predicted[i], \"| Prob:\", round(np.max(predicted_prob[i]),2))","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":39,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_195":{"id":"3be3dbf9c8b2_195","__typename":"Paragraph","name":"81ae","text":"\n## show explanation\n### 1. preprocess input\nlst_corpus = []\nfor string in [re.sub(r'[^\\w\\s]','', txt_instance.lower().strip())]:\n    lst_words = string.split()\n    lst_grams = [\" \".join(lst_words[i:i+1]) for i in range(0, \n                 len(lst_words), 1)]\n    lst_corpus.append(lst_grams)\nlst_corpus = list(bigrams_detector[lst_corpus])\nlst_corpus = list(trigrams_detector[lst_corpus])\nX_instance = kprocessing.sequence.pad_sequences(\n              tokenizer.texts_to_sequences(corpus), maxlen=15, \n              padding=\"post\", truncating=\"post\")","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":45,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_196":{"id":"3be3dbf9c8b2_196","__typename":"Paragraph","name":"932d","text":"### 2. get attention weights\nlayer = [layer for layer in model.layers if \"attention\" in \n         layer.name][0]\nfunc = K.function([model.input], [layer.output])\nweights = func(X_instance)[0]\nweights = np.mean(weights, axis=2).flatten()","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":28,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":74,"end":83,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_197":{"id":"3be3dbf9c8b2_197","__typename":"Paragraph","name":"0d81","text":"### 3. rescale weights, remove null vector, map word-weight\nweights = preprocessing.MinMaxScaler(feature_range=(0,1)).fit_transform(np.array(weights).reshape(-1,1)).reshape(-1)\nweights = [weights[n] for n,idx in enumerate(X_instance[0]) if idx \n           != 0]\ndic_word_weigth = {word:weights[n] for n,word in \n                   enumerate(lst_corpus[0]) if word in \n                   tokenizer.word_index.keys()}","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":59,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_198":{"id":"3be3dbf9c8b2_198","__typename":"Paragraph","name":"6dcb","text":"### 4. barplot\nif len(dic_word_weigth) \u003E 0:\n   dtf = pd.DataFrame.from_dict(dic_word_weigth, orient='index', \n                                columns=[\"score\"])\n   dtf.sort_values(by=\"score\", \n           ascending=True).tail(top).plot(kind=\"barh\", \n           legend=False).grid(axis='x')\n   plt.show()\nelse:\n   print(\"--- No word recognized ---\")","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":14,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_199":{"id":"3be3dbf9c8b2_199","__typename":"Paragraph","name":"2f30","text":"### 5. produce html visualization\ntext = []\nfor word in lst_corpus[0]:\n    weight = dic_word_weigth.get(word)\n    if weight is not None:\n         text.append('\u003Cb\u003E\u003Cspan style=\"background-color:rgba(100,149,237,' + str(weight) + ');\"\u003E' + word + '\u003C\u002Fspan\u003E\u003C\u002Fb\u003E')\n    else:\n         text.append(word)\ntext = ' '.join(text)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":33,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_200":{"id":"3be3dbf9c8b2_200","__typename":"Paragraph","name":"ce30","text":"### 6. visualize on notebook\nprint(\"\\033[1m\"+\"Text with highlighted words\")\nfrom IPython.core.display import display, HTML\ndisplay(HTML(text))","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":29,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":36,"end":40,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_201":{"id":"3be3dbf9c8b2_201","__typename":"Paragraph","name":"d68e","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*21iP8FD4XOBDn3XS5LKiJA.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_202":{"id":"3be3dbf9c8b2_202","__typename":"Paragraph","name":"a88c","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*CrFeNLHgDzBxAmgGiXR8lg.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_203":{"id":"3be3dbf9c8b2_203","__typename":"Paragraph","name":"3f7e","text":"Just like before, the words “clinton” and “gop” activated the neurons of the model, but this time also “high” and “benghazi” have been considered slightly relevant for the prediction.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":29,"end":36,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":43,"end":46,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":104,"end":108,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":115,"end":123,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_204":{"id":"3be3dbf9c8b2_204","__typename":"Paragraph","name":"8831","text":"Language Models","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_205":{"id":"3be3dbf9c8b2_205","__typename":"Paragraph","name":"0455","text":"Language Models, or Contextualized\u002FDynamic Word Embeddings, overcome the biggest limitation of the classic Word Embedding approach: polysemy disambiguation, a word with different meanings (e.g. “ bank” or “stick”) is identified by just one vector. One of the first popular ones was ELMO (2018), which doesn’t apply a fixed embedding but, using a bidirectional LSTM, looks at the entire sentence and then assigns an embedding to each word.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":15,"type":"A","href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FLanguage_model","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":58,"end":60,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":196,"end":200,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":206,"end":211,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_206":{"id":"3be3dbf9c8b2_206","__typename":"Paragraph","name":"9efe","text":"Enter Transformers: a new modeling technique presented by Google’s paper Attention is All You Need (2017) in which it was demonstrated that sequence models (like LSTM) can be totally replaced by Attention mechanisms, even obtaining better performances.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":73,"end":98,"type":"A","href":"https:\u002F\u002Farxiv.org\u002Fabs\u002F1706.03762","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":73,"end":99,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":105,"end":106,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_207":{"id":"3be3dbf9c8b2_207","__typename":"Paragraph","name":"0e55","text":"Google’s BERT (Bidirectional Encoder Representations from Transformers, 2018) combines ELMO context embedding and several Transformers, plus it’s bidirectional (which was a big novelty for Transformers). The vector BERT assigns to a word is a function of the entire sentence, therefore, a word can have different vectors based on the contexts. Let’s try it using transformers:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":9,"end":13,"type":"A","href":"https:\u002F\u002Fen.wikipedia.org\u002Fwiki\u002FBERT_(language_model)","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":9,"end":14,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":363,"end":375,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_208":{"id":"3be3dbf9c8b2_208","__typename":"Paragraph","name":"0f5f","text":"txt = \"bank river\"","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":6,"end":18,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_209":{"id":"3be3dbf9c8b2_209","__typename":"Paragraph","name":"8739","text":"## bert tokenizer\ntokenizer = transformers.BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":17,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":43,"end":56,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":57,"end":72,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_210":{"id":"3be3dbf9c8b2_210","__typename":"Paragraph","name":"b7ff","text":"## bert model\nnlp = transformers.TFBertModel.from_pretrained('bert-base-uncased')","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":13,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":33,"end":44,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":45,"end":60,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_211":{"id":"3be3dbf9c8b2_211","__typename":"Paragraph","name":"20f4","text":"## return hidden layer with embeddings\ninput_ids = np.array(tokenizer.encode(txt))[None,:]  \nembedding = nlp(input_ids)\nembedding[0][0]","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":38,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":119,"end":120,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_212":{"id":"3be3dbf9c8b2_212","__typename":"Paragraph","name":"b81f","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*f7-l1PPDu5Q6rgUBzgz12A.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_213":{"id":"3be3dbf9c8b2_213","__typename":"Paragraph","name":"cfbd","text":"If we change the input text into “bank money”, we get this instead:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":34,"end":44,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_214":{"id":"3be3dbf9c8b2_214","__typename":"Paragraph","name":"ff09","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*V2iWL5kNy9WBzIFPfoVZYQ.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_215":{"id":"3be3dbf9c8b2_215","__typename":"Paragraph","name":"bd4c","text":"In order to complete a text classification task, you can use BERT in 3 different ways:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_216":{"id":"3be3dbf9c8b2_216","__typename":"Paragraph","name":"31c1","text":"train it all from scratches and use it as classifier.","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_217":{"id":"3be3dbf9c8b2_217","__typename":"Paragraph","name":"32ee","text":"Extract the word embeddings and use them in an embedding layer (like I did with Word2Vec).","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_218":{"id":"3be3dbf9c8b2_218","__typename":"Paragraph","name":"4e55","text":"Fine-tuning the pre-trained model (transfer learning).","type":"ULI","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_219":{"id":"3be3dbf9c8b2_219","__typename":"Paragraph","name":"8eb7","text":"I’m going with the latter and do transfer learning from a pre-trained lighter version of BERT, called Distil-BERT (66 million of parameters instead of 110 million!).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":102,"end":113,"type":"A","href":"https:\u002F\u002Fhuggingface.co\u002Ftransformers\u002Fmodel_doc\u002Fdistilbert.html","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_220":{"id":"3be3dbf9c8b2_220","__typename":"Paragraph","name":"ce0e","text":"## distil-bert tokenizer\ntokenizer = transformers.AutoTokenizer.from_pretrained('distilbert-base-uncased', do_lower_case=True)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":24,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":50,"end":63,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":64,"end":79,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_221":{"id":"3be3dbf9c8b2_221","__typename":"Paragraph","name":"9ddd","text":"As usual, before fitting the model there is some Feature Engineering to do, but this time it’s gonna be a little trickier. To give an illustration of what I’m going to do, let’s take as an example our beloved sentence “I like this article”, which has to be transformed into 3 vectors (Ids, Mask, Segment):","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":49,"end":68,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":219,"end":238,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_222":{"id":"3be3dbf9c8b2_222","__typename":"Paragraph","name":"0617","text":"Shape: 3 x Sequence length","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*rENCe-2FhlIBIfUstVHRhA.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_223":{"id":"3be3dbf9c8b2_223","__typename":"Paragraph","name":"c6b3","text":"First of all, we need to select the sequence max length. This time I’m gonna choose a much larger number (i.e. 50) because BERT splits unknown words into sub-tokens until it finds a known unigrams. For example, if a made-up word like “zzdata” is given, BERT would split it into [“z”, “##z”, “##data”]. Moreover, we have to insert special tokens into the input text, then generate masks and segments. Finally, put all together in a tensor to get the feature matrix that will have the shape of 3 (ids, masks, segments) x Number of documents in the corpus x Sequence length.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":235,"end":241,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":280,"end":281,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":285,"end":288,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":292,"end":298,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_224":{"id":"3be3dbf9c8b2_224","__typename":"Paragraph","name":"37c8","text":"Please note that I’m using the raw text as corpus (so far I’ve been using the clean_text column).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":78,"end":89,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_225":{"id":"3be3dbf9c8b2_225","__typename":"Paragraph","name":"19fd","text":"corpus = dtf_train[\"text\"]\nmaxlen = 50","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":20,"end":24,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_226":{"id":"3be3dbf9c8b2_226","__typename":"Paragraph","name":"f2b8","text":"\n## add special tokens\nmaxqnans = np.int((maxlen-20)\u002F2)\ncorpus_tokenized = [\"[CLS] \"+\n             \" \".join(tokenizer.tokenize(re.sub(r'[^\\w\\s]+|\\n', '', \n             str(txt).lower().strip()))[:maxqnans])+\n             \" [SEP] \" for txt in corpus]\n\n## generate masks\nmasks = [[1]*len(txt.split(\" \")) + [0]*(maxlen - len(\n           txt.split(\" \"))) for txt in corpus_tokenized]\n    \n## padding\ntxt2seq = [txt + \" [PAD]\"*(maxlen-len(txt.split(\" \"))) if len(txt.split(\" \")) != maxlen else txt for txt in corpus_tokenized]\n    \n## generate idx\nidx = [tokenizer.encode(seq.split(\" \")) for seq in txt2seq]\n    \n## generate segments\nsegments = [] \nfor seq in txt2seq:\n    temp, i = [], 0\n    for token in seq.split(\" \"):\n        temp.append(i)\n        if token == \"[SEP]\":\n             i += 1\n    segments.append(temp)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":22,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":251,"end":268,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":385,"end":395,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":527,"end":542,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":608,"end":628,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_227":{"id":"3be3dbf9c8b2_227","__typename":"Paragraph","name":"3481","text":"## feature matrix\nX_train = [np.asarray(idx, dtype='int32'), \n           np.asarray(masks, dtype='int32'), \n           np.asarray(segments, dtype='int32')]","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":17,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_228":{"id":"3be3dbf9c8b2_228","__typename":"Paragraph","name":"5aa3","text":"The feature matrix X_train has a shape of 3 x 34,265 x 50. We can check a random observation from the feature matrix:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":19,"end":26,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_229":{"id":"3be3dbf9c8b2_229","__typename":"Paragraph","name":"9e38","text":"i = 0","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_230":{"id":"3be3dbf9c8b2_230","__typename":"Paragraph","name":"6e88","text":"print(\"txt: \", dtf_train[\"text\"].iloc[0])\nprint(\"tokenized:\", [tokenizer.convert_ids_to_tokens(idx) for idx in X_train[0][i].tolist()])\nprint(\"idx: \", X_train[0][i])\nprint(\"mask: \", X_train[1][i])\nprint(\"segment: \", X_train[2][i])","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_231":{"id":"3be3dbf9c8b2_231","__typename":"Paragraph","name":"4313","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*vRUkclqzuGDvmgu1VFs-5A.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_232":{"id":"3be3dbf9c8b2_232","__typename":"Paragraph","name":"a1fc","text":"You can take the same code and apply it to dtf_test[“text”] to get X_test.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":67,"end":73,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_233":{"id":"3be3dbf9c8b2_233","__typename":"Paragraph","name":"1d18","text":"Now, I’m going to build the deep learning model with transfer learning from the pre-trained BERT. Basically, I’m going to summarize the output of BERT into one vector with Average Pooling and then add two final Dense layers to predict the probability of each news category.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":28,"end":70,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_234":{"id":"3be3dbf9c8b2_234","__typename":"Paragraph","name":"f6c7","text":"If you want to use the original versions of BERT, here’s the code (remember to redo the feature engineering with the right tokenizer):","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_235":{"id":"3be3dbf9c8b2_235","__typename":"Paragraph","name":"ad59","text":"## inputs\nidx = layers.Input((50), dtype=\"int32\", name=\"input_idx\")\nmasks = layers.Input((50), dtype=\"int32\", name=\"input_masks\")\nsegments = layers.Input((50), dtype=\"int32\", name=\"input_segments\")","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":9,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":23,"end":28,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":83,"end":88,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_236":{"id":"3be3dbf9c8b2_236","__typename":"Paragraph","name":"e1dc","text":"## pre-trained bert\nnlp = transformers.TFBertModel.from_pretrained(\"bert-base-uncased\")\nbert_out, _ = nlp([idx, masks, segments])","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":19,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":39,"end":66,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_237":{"id":"3be3dbf9c8b2_237","__typename":"Paragraph","name":"454a","text":"## fine-tuning\nx = layers.GlobalAveragePooling1D()(bert_out)\nx = layers.Dense(64, activation=\"relu\")(x)\ny_out = layers.Dense(len(np.unique(y_train)), \n                     activation='softmax')(x)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":14,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":26,"end":48,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":72,"end":77,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":119,"end":124,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_238":{"id":"3be3dbf9c8b2_238","__typename":"Paragraph","name":"ee3d","text":"## compile\nmodel = models.Model([idx, masks, segments], y_out)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":10,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":26,"end":31,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_239":{"id":"3be3dbf9c8b2_239","__typename":"Paragraph","name":"aaca","text":"for layer in model.layers[:4]:\n    layer.trainable = False","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_240":{"id":"3be3dbf9c8b2_240","__typename":"Paragraph","name":"cd6e","text":"model.compile(loss='sparse_categorical_crossentropy', \n              optimizer='adam', metrics=['accuracy'])","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_241":{"id":"3be3dbf9c8b2_241","__typename":"Paragraph","name":"5e3e","text":"model.summary()","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_242":{"id":"3be3dbf9c8b2_242","__typename":"Paragraph","name":"9f9b","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*riJ2LlNVz_0MJvqAbYG3Bw.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_243":{"id":"3be3dbf9c8b2_243","__typename":"Paragraph","name":"ee15","text":"As I said, I’m going to use the lighter version instead, Distil-BERT:","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_244":{"id":"3be3dbf9c8b2_244","__typename":"Paragraph","name":"6928","text":"## inputs\nidx = layers.Input((50), dtype=\"int32\", name=\"input_idx\")\nmasks = layers.Input((50), dtype=\"int32\", name=\"input_masks\")","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":9,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":23,"end":28,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":83,"end":88,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_245":{"id":"3be3dbf9c8b2_245","__typename":"Paragraph","name":"aff6","text":"## pre-trained bert with config\nconfig = transformers.DistilBertConfig(dropout=0.2, \n           attention_dropout=0.2)\nconfig.output_hidden_states = False","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":31,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_246":{"id":"3be3dbf9c8b2_246","__typename":"Paragraph","name":"d4a7","text":"nlp = transformers.TFDistilBertModel.from_pretrained('distilbert-\n                  base-uncased', config=config)\nbert_out = nlp(idx, attention_mask=masks)[0]","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":19,"end":52,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_247":{"id":"3be3dbf9c8b2_247","__typename":"Paragraph","name":"cc29","text":"## fine-tuning\nx = layers.GlobalAveragePooling1D()(bert_out)\nx = layers.Dense(64, activation=\"relu\")(x)\ny_out = layers.Dense(len(np.unique(y_train)), \n                     activation='softmax')(x)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":14,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":26,"end":48,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":72,"end":77,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":119,"end":124,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_248":{"id":"3be3dbf9c8b2_248","__typename":"Paragraph","name":"65af","text":"## compile\nmodel = models.Model([idx, masks], y_out)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":10,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":26,"end":31,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_249":{"id":"3be3dbf9c8b2_249","__typename":"Paragraph","name":"5a09","text":"for layer in model.layers[:3]:\n    layer.trainable = False","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_250":{"id":"3be3dbf9c8b2_250","__typename":"Paragraph","name":"46a9","text":"model.compile(loss='sparse_categorical_crossentropy', \n              optimizer='adam', metrics=['accuracy'])","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_251":{"id":"3be3dbf9c8b2_251","__typename":"Paragraph","name":"dcef","text":"model.summary()","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_252":{"id":"3be3dbf9c8b2_252","__typename":"Paragraph","name":"6c98","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*p5sc1h4l3DewgrHvZDrr9g.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_253":{"id":"3be3dbf9c8b2_253","__typename":"Paragraph","name":"ed0a","text":"Let’s train, test, evaluate this bad boy (code for evaluation is the same):","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":6,"end":27,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_254":{"id":"3be3dbf9c8b2_254","__typename":"Paragraph","name":"6ad3","text":"## encode y\ndic_y_mapping = {n:label for n,label in \n                 enumerate(np.unique(y_train))}\ninverse_dic = {v:k for k,v in dic_y_mapping.items()}\ny_train = np.array([inverse_dic[y] for y in y_train])","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":11,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_255":{"id":"3be3dbf9c8b2_255","__typename":"Paragraph","name":"04a7","text":"## train\ntraining = model.fit(x=X_train, y=y_train, batch_size=64, \n                     epochs=1, shuffle=True, verbose=1, \n                     validation_split=0.3)","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":8,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_256":{"id":"3be3dbf9c8b2_256","__typename":"Paragraph","name":"d959","text":"## test\npredicted_prob = model.predict(X_test)\npredicted = [dic_y_mapping[np.argmax(pred)] for pred in \n             predicted_prob]","type":"PRE","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":0,"end":7,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_257":{"id":"3be3dbf9c8b2_257","__typename":"Paragraph","name":"22f4","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*oS2xD1Y0IWPGDg8uVx2dPQ.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_258":{"id":"3be3dbf9c8b2_258","__typename":"Paragraph","name":"2378","text":"","type":"IMG","href":null,"layout":"INSET_CENTER","metadata":{"__ref":"ImageMetadata:1*NsiKi7b0JGlCQPLpeVkftA.png"},"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_259":{"id":"3be3dbf9c8b2_259","__typename":"Paragraph","name":"1319","text":"The performance of BERT is slightly better than the previous models, in fact, it can recognize more Tech news than the others.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_260":{"id":"3be3dbf9c8b2_260","__typename":"Paragraph","name":"64fd","text":"Conclusion","type":"H4","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_261":{"id":"3be3dbf9c8b2_261","__typename":"Paragraph","name":"9228","text":"This article has been a tutorial to demonstrate how to apply different NLP models to a multiclass classification use case. I compared 3 popular approaches: Bag-of-Words with Tf-Idf, Word Embedding with Word2Vec, and Language model with BERT. I went through Feature Engineering & Selection, Model Design & Testing, Evaluation & Explainability, comparing the 3 models in each step (where possible).","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":48,"end":121,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_262":{"id":"3be3dbf9c8b2_262","__typename":"Paragraph","name":"9ace","text":"Please note that I haven’t covered explainability for BERT as I’m still working on that, but I will update this article as soon as I can. If you have any useful resources about that, feel free to contact me.","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_263":{"id":"3be3dbf9c8b2_263","__typename":"Paragraph","name":"3822","text":"This article is part of the series NLP with Python, see also:","type":"BQ","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":35,"end":50,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_264":{"id":"3be3dbf9c8b2_264","__typename":"Paragraph","name":"cf4e","text":"Text Analysis & Feature Engineering with NLP\nLanguage Detection, Text Cleaning, Length, Sentiment, Named-Entity Recognition, N-grams Frequency, Word Vectors, Topic…towardsdatascience.com","type":"MIXTAPE_EMBED","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":{"__typename":"MixtapeMetadata","href":"https:\u002F\u002Ftowardsdatascience.com\u002Ftext-analysis-feature-engineering-with-nlp-502d6ea9225d","thumbnailImageId":"1*pZVocDqN8uRdnGyrrWbnlQ.png"},"markups":[{"__typename":"Markup","start":0,"end":186,"type":"A","href":"https:\u002F\u002Ftowardsdatascience.com\u002Ftext-analysis-feature-engineering-with-nlp-502d6ea9225d","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":0,"end":44,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":45,"end":164,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_265":{"id":"3be3dbf9c8b2_265","__typename":"Paragraph","name":"8b2b","text":"BERT for Text Classification with NO model training\nUse BERT, Word Embedding, and Vector Similarity when you don’t have a labeled training settowardsdatascience.com","type":"MIXTAPE_EMBED","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":{"__typename":"MixtapeMetadata","href":"https:\u002F\u002Ftowardsdatascience.com\u002Ftext-classification-with-no-model-training-935fe0e42180","thumbnailImageId":"1*t5Cte8JzO1x4btdQ7VS0iA.png"},"markups":[{"__typename":"Markup","start":0,"end":164,"type":"A","href":"https:\u002F\u002Ftowardsdatascience.com\u002Ftext-classification-with-no-model-training-935fe0e42180","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":0,"end":51,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":52,"end":142,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_266":{"id":"3be3dbf9c8b2_266","__typename":"Paragraph","name":"7f63","text":"String Matching: Surpass Excel VLOOKUP with Python & NLP\nBuild a String Matching App for all the Excel lovers (and haters)towardsdatascience.com","type":"MIXTAPE_EMBED","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":{"__typename":"MixtapeMetadata","href":"https:\u002F\u002Ftowardsdatascience.com\u002Fsurpass-excel-vlookup-with-python-and-nlp-ab20d56c4a1a","thumbnailImageId":"1*uo1-f6SPeaXqAvy-viO39w.png"},"markups":[{"__typename":"Markup","start":0,"end":144,"type":"A","href":"https:\u002F\u002Ftowardsdatascience.com\u002Fsurpass-excel-vlookup-with-python-and-nlp-ab20d56c4a1a","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":0,"end":56,"type":"STRONG","href":null,"anchorType":null,"userId":null,"linkMetadata":null},{"__typename":"Markup","start":57,"end":122,"type":"EM","href":null,"anchorType":null,"userId":null,"linkMetadata":null}],"dropCapImage":null},"Paragraph:3be3dbf9c8b2_267":{"id":"3be3dbf9c8b2_267","__typename":"Paragraph","name":"f2df","text":"Connect: LinkedIn | Instagram | Twitter | GitHub","type":"P","href":null,"layout":null,"metadata":null,"hasDropCap":null,"iframe":null,"mixtapeMetadata":null,"markups":[{"__typename":"Markup","start":9,"end":17,"type":"A","href":"https:\u002F\u002Fwww.linkedin.com\u002Fin\u002Fmauro-di-pietro-56a1366b\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":20,"end":29,"type":"A","href":"https:\u002F\u002Fwww.instagram.com\u002Fmaurodp09\u002F","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":32,"end":39,"type":"A","href":"https:\u002F\u002Ftwitter.com\u002Fmaurodp90?lang=en","anchorType":"LINK","userId":null,"linkMetadata":null},{"__typename":"Markup","start":42,"end":48,"type":"A","href":"https:\u002F\u002Fgithub.com\u002Fmdipietro09","anchorType":"LINK","userId":null,"linkMetadata":null}],"dropCapImage":null},"ImageMetadata:1*LgqxDMP5qD1HE_uM33zZrg.png":{"id":"1*LgqxDMP5qD1HE_uM33zZrg.png","__typename":"ImageMetadata","originalHeight":132,"originalWidth":1205,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*T8WWibd7u8b7gfgeG0LgAA.gif":{"id":"1*T8WWibd7u8b7gfgeG0LgAA.gif","__typename":"ImageMetadata","originalHeight":756,"originalWidth":1548,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*N7xAYy2MBRJHKMBXnxMi0A.png":{"id":"1*N7xAYy2MBRJHKMBXnxMi0A.png","__typename":"ImageMetadata","originalHeight":325,"originalWidth":2145,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*iurA976CkC9i1Yi1L6hIIw.png":{"id":"1*iurA976CkC9i1Yi1L6hIIw.png","__typename":"ImageMetadata","originalHeight":538,"originalWidth":1532,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*b7hN7kENZzF4wsck1ne0QA.png":{"id":"1*b7hN7kENZzF4wsck1ne0QA.png","__typename":"ImageMetadata","originalHeight":466,"originalWidth":1067,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*k1fsJU_S0_WPZku6gg-qOQ.png":{"id":"1*k1fsJU_S0_WPZku6gg-qOQ.png","__typename":"ImageMetadata","originalHeight":227,"originalWidth":1095,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*t-R6djtHnK4cBVqFrjfLgA.png":{"id":"1*t-R6djtHnK4cBVqFrjfLgA.png","__typename":"ImageMetadata","originalHeight":333,"originalWidth":1494,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*m1O25pvl8R5DlkhuJjRrDw.png":{"id":"1*m1O25pvl8R5DlkhuJjRrDw.png","__typename":"ImageMetadata","originalHeight":297,"originalWidth":1601,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*CpZ9fxPY5iSEzgdyS021_Q.png":{"id":"1*CpZ9fxPY5iSEzgdyS021_Q.png","__typename":"ImageMetadata","originalHeight":499,"originalWidth":1427,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*Fo0EjcD4Ibo2Jz1y6eNL0A.png":{"id":"1*Fo0EjcD4Ibo2Jz1y6eNL0A.png","__typename":"ImageMetadata","originalHeight":315,"originalWidth":1479,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*O8lMt_obkHbMXuOSg1bTRA.png":{"id":"1*O8lMt_obkHbMXuOSg1bTRA.png","__typename":"ImageMetadata","originalHeight":504,"originalWidth":1430,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*iPL_8iJOuTJ_mrLvftwUEw.png":{"id":"1*iPL_8iJOuTJ_mrLvftwUEw.png","__typename":"ImageMetadata","originalHeight":1344,"originalWidth":1437,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*CFZTX1Ud0jOwNZMhrr4cWA.png":{"id":"1*CFZTX1Ud0jOwNZMhrr4cWA.png","__typename":"ImageMetadata","originalHeight":44,"originalWidth":607,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*qcAV1wvucxNogDz_eh3dpQ.png":{"id":"1*qcAV1wvucxNogDz_eh3dpQ.png","__typename":"ImageMetadata","originalHeight":269,"originalWidth":1947,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*u67szEvNSMqrQeitdahw_A.png":{"id":"1*u67szEvNSMqrQeitdahw_A.png","__typename":"ImageMetadata","originalHeight":401,"originalWidth":1012,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*nMBpatACacNe1SZYF39j3Q.png":{"id":"1*nMBpatACacNe1SZYF39j3Q.png","__typename":"ImageMetadata","originalHeight":32,"originalWidth":120,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*tQ588zm4i96xfEBCfiUpzQ.png":{"id":"1*tQ588zm4i96xfEBCfiUpzQ.png","__typename":"ImageMetadata","originalHeight":483,"originalWidth":1453,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*vhJWO3cTN2jfrg2upX2kGQ.png":{"id":"1*vhJWO3cTN2jfrg2upX2kGQ.png","__typename":"ImageMetadata","originalHeight":148,"originalWidth":1333,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*H6ZZAT-9tmu8PcaY74Un6w.png":{"id":"1*H6ZZAT-9tmu8PcaY74Un6w.png","__typename":"ImageMetadata","originalHeight":477,"originalWidth":1432,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*EWLnzZ0abpnhH1zR8jq5PQ.png":{"id":"1*EWLnzZ0abpnhH1zR8jq5PQ.png","__typename":"ImageMetadata","originalHeight":52,"originalWidth":490,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*KvBp0xzRThA7qTXACT4A-g.png":{"id":"1*KvBp0xzRThA7qTXACT4A-g.png","__typename":"ImageMetadata","originalHeight":969,"originalWidth":1627,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*Dfu-2hqEMaBe6YHx1C71Uw.png":{"id":"1*Dfu-2hqEMaBe6YHx1C71Uw.png","__typename":"ImageMetadata","originalHeight":756,"originalWidth":1241,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*MdYjdeOju4ez8gcAc-v8qA.png":{"id":"1*MdYjdeOju4ez8gcAc-v8qA.png","__typename":"ImageMetadata","originalHeight":358,"originalWidth":1514,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*a39MMTNXnDaFOKFur2Z7xQ.png":{"id":"1*a39MMTNXnDaFOKFur2Z7xQ.png","__typename":"ImageMetadata","originalHeight":1380,"originalWidth":1430,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*21iP8FD4XOBDn3XS5LKiJA.png":{"id":"1*21iP8FD4XOBDn3XS5LKiJA.png","__typename":"ImageMetadata","originalHeight":38,"originalWidth":594,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*CrFeNLHgDzBxAmgGiXR8lg.png":{"id":"1*CrFeNLHgDzBxAmgGiXR8lg.png","__typename":"ImageMetadata","originalHeight":422,"originalWidth":867,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*f7-l1PPDu5Q6rgUBzgz12A.png":{"id":"1*f7-l1PPDu5Q6rgUBzgz12A.png","__typename":"ImageMetadata","originalHeight":191,"originalWidth":1039,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*V2iWL5kNy9WBzIFPfoVZYQ.png":{"id":"1*V2iWL5kNy9WBzIFPfoVZYQ.png","__typename":"ImageMetadata","originalHeight":180,"originalWidth":1037,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*rENCe-2FhlIBIfUstVHRhA.png":{"id":"1*rENCe-2FhlIBIfUstVHRhA.png","__typename":"ImageMetadata","originalHeight":529,"originalWidth":1611,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*vRUkclqzuGDvmgu1VFs-5A.png":{"id":"1*vRUkclqzuGDvmgu1VFs-5A.png","__typename":"ImageMetadata","originalHeight":491,"originalWidth":1921,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*riJ2LlNVz_0MJvqAbYG3Bw.png":{"id":"1*riJ2LlNVz_0MJvqAbYG3Bw.png","__typename":"ImageMetadata","originalHeight":784,"originalWidth":1552,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*p5sc1h4l3DewgrHvZDrr9g.png":{"id":"1*p5sc1h4l3DewgrHvZDrr9g.png","__typename":"ImageMetadata","originalHeight":634,"originalWidth":1532,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*oS2xD1Y0IWPGDg8uVx2dPQ.png":{"id":"1*oS2xD1Y0IWPGDg8uVx2dPQ.png","__typename":"ImageMetadata","originalHeight":117,"originalWidth":1965,"focusPercentX":null,"focusPercentY":null,"alt":null},"ImageMetadata:1*NsiKi7b0JGlCQPLpeVkftA.png":{"id":"1*NsiKi7b0JGlCQPLpeVkftA.png","__typename":"ImageMetadata","originalHeight":1360,"originalWidth":1464,"focusPercentX":null,"focusPercentY":null,"alt":null},"User:895063a310f4":{"id":"895063a310f4","__typename":"User","name":"Ludovic Benistant","username":"ludobenistant","newsletterV3":{"__ref":"NewsletterV3:2375c85c8da7"}},"NewsletterV3:2375c85c8da7":{"id":"2375c85c8da7","__typename":"NewsletterV3"},"Tag:data-science":{"id":"data-science","__typename":"Tag","displayTitle":"Data Science","normalizedTagSlug":"data-science"},"Tag:artificial-intelligence":{"id":"artificial-intelligence","__typename":"Tag","displayTitle":"Artificial Intelligence","normalizedTagSlug":"artificial-intelligence"},"Tag:machine-learning":{"id":"machine-learning","__typename":"Tag","displayTitle":"Machine Learning","normalizedTagSlug":"machine-learning"},"Tag:programming":{"id":"programming","__typename":"Tag","displayTitle":"Programming","normalizedTagSlug":"programming"},"Tag:nlp":{"id":"nlp","__typename":"Tag","displayTitle":"NLP","normalizedTagSlug":"nlp"},"PostViewerEdge:postId:41ff868d1794-viewerId:ad0cdc98db0b":{"id":"postId:41ff868d1794-viewerId:ad0cdc98db0b","__typename":"PostViewerEdge","catalogsConnection":{"__typename":"EntityCatalogsConnection","catalogsContainingThis({\"type\":\"LISTS\"})":[],"predefinedContainingThis":[]}},"Post:41ff868d1794":{"id":"41ff868d1794","__typename":"Post","creator":{"__ref":"User:44a176cd070a"},"canonicalUrl":"","collection":{"__ref":"Collection:7f60cf5620c9"},"content({\"postMeteringOptions\":{\"referrer\":\"https:\u002F\u002Fwww.google.com\u002F\",\"sk\":null,\"source\":null}})":{"__typename":"PostContent","isLockedPreviewOnly":false,"validatedShareKey":"","bodyModel":{"__typename":"RichText","paragraphs":[{"__ref":"Paragraph:3be3dbf9c8b2_0"},{"__ref":"Paragraph:3be3dbf9c8b2_1"},{"__ref":"Paragraph:3be3dbf9c8b2_2"},{"__ref":"Paragraph:3be3dbf9c8b2_3"},{"__ref":"Paragraph:3be3dbf9c8b2_4"},{"__ref":"Paragraph:3be3dbf9c8b2_5"},{"__ref":"Paragraph:3be3dbf9c8b2_6"},{"__ref":"Paragraph:3be3dbf9c8b2_7"},{"__ref":"Paragraph:3be3dbf9c8b2_8"},{"__ref":"Paragraph:3be3dbf9c8b2_9"},{"__ref":"Paragraph:3be3dbf9c8b2_10"},{"__ref":"Paragraph:3be3dbf9c8b2_11"},{"__ref":"Paragraph:3be3dbf9c8b2_12"},{"__ref":"Paragraph:3be3dbf9c8b2_13"},{"__ref":"Paragraph:3be3dbf9c8b2_14"},{"__ref":"Paragraph:3be3dbf9c8b2_15"},{"__ref":"Paragraph:3be3dbf9c8b2_16"},{"__ref":"Paragraph:3be3dbf9c8b2_17"},{"__ref":"Paragraph:3be3dbf9c8b2_18"},{"__ref":"Paragraph:3be3dbf9c8b2_19"},{"__ref":"Paragraph:3be3dbf9c8b2_20"},{"__ref":"Paragraph:3be3dbf9c8b2_21"},{"__ref":"Paragraph:3be3dbf9c8b2_22"},{"__ref":"Paragraph:3be3dbf9c8b2_23"},{"__ref":"Paragraph:3be3dbf9c8b2_24"},{"__ref":"Paragraph:3be3dbf9c8b2_25"},{"__ref":"Paragraph:3be3dbf9c8b2_26"},{"__ref":"Paragraph:3be3dbf9c8b2_27"},{"__ref":"Paragraph:3be3dbf9c8b2_28"},{"__ref":"Paragraph:3be3dbf9c8b2_29"},{"__ref":"Paragraph:3be3dbf9c8b2_30"},{"__ref":"Paragraph:3be3dbf9c8b2_31"},{"__ref":"Paragraph:3be3dbf9c8b2_32"},{"__ref":"Paragraph:3be3dbf9c8b2_33"},{"__ref":"Paragraph:3be3dbf9c8b2_34"},{"__ref":"Paragraph:3be3dbf9c8b2_35"},{"__ref":"Paragraph:3be3dbf9c8b2_36"},{"__ref":"Paragraph:3be3dbf9c8b2_37"},{"__ref":"Paragraph:3be3dbf9c8b2_38"},{"__ref":"Paragraph:3be3dbf9c8b2_39"},{"__ref":"Paragraph:3be3dbf9c8b2_40"},{"__ref":"Paragraph:3be3dbf9c8b2_41"},{"__ref":"Paragraph:3be3dbf9c8b2_42"},{"__ref":"Paragraph:3be3dbf9c8b2_43"},{"__ref":"Paragraph:3be3dbf9c8b2_44"},{"__ref":"Paragraph:3be3dbf9c8b2_45"},{"__ref":"Paragraph:3be3dbf9c8b2_46"},{"__ref":"Paragraph:3be3dbf9c8b2_47"},{"__ref":"Paragraph:3be3dbf9c8b2_48"},{"__ref":"Paragraph:3be3dbf9c8b2_49"},{"__ref":"Paragraph:3be3dbf9c8b2_50"},{"__ref":"Paragraph:3be3dbf9c8b2_51"},{"__ref":"Paragraph:3be3dbf9c8b2_52"},{"__ref":"Paragraph:3be3dbf9c8b2_53"},{"__ref":"Paragraph:3be3dbf9c8b2_54"},{"__ref":"Paragraph:3be3dbf9c8b2_55"},{"__ref":"Paragraph:3be3dbf9c8b2_56"},{"__ref":"Paragraph:3be3dbf9c8b2_57"},{"__ref":"Paragraph:3be3dbf9c8b2_58"},{"__ref":"Paragraph:3be3dbf9c8b2_59"},{"__ref":"Paragraph:3be3dbf9c8b2_60"},{"__ref":"Paragraph:3be3dbf9c8b2_61"},{"__ref":"Paragraph:3be3dbf9c8b2_62"},{"__ref":"Paragraph:3be3dbf9c8b2_63"},{"__ref":"Paragraph:3be3dbf9c8b2_64"},{"__ref":"Paragraph:3be3dbf9c8b2_65"},{"__ref":"Paragraph:3be3dbf9c8b2_66"},{"__ref":"Paragraph:3be3dbf9c8b2_67"},{"__ref":"Paragraph:3be3dbf9c8b2_68"},{"__ref":"Paragraph:3be3dbf9c8b2_69"},{"__ref":"Paragraph:3be3dbf9c8b2_70"},{"__ref":"Paragraph:3be3dbf9c8b2_71"},{"__ref":"Paragraph:3be3dbf9c8b2_72"},{"__ref":"Paragraph:3be3dbf9c8b2_73"},{"__ref":"Paragraph:3be3dbf9c8b2_74"},{"__ref":"Paragraph:3be3dbf9c8b2_75"},{"__ref":"Paragraph:3be3dbf9c8b2_76"},{"__ref":"Paragraph:3be3dbf9c8b2_77"},{"__ref":"Paragraph:3be3dbf9c8b2_78"},{"__ref":"Paragraph:3be3dbf9c8b2_79"},{"__ref":"Paragraph:3be3dbf9c8b2_80"},{"__ref":"Paragraph:3be3dbf9c8b2_81"},{"__ref":"Paragraph:3be3dbf9c8b2_82"},{"__ref":"Paragraph:3be3dbf9c8b2_83"},{"__ref":"Paragraph:3be3dbf9c8b2_84"},{"__ref":"Paragraph:3be3dbf9c8b2_85"},{"__ref":"Paragraph:3be3dbf9c8b2_86"},{"__ref":"Paragraph:3be3dbf9c8b2_87"},{"__ref":"Paragraph:3be3dbf9c8b2_88"},{"__ref":"Paragraph:3be3dbf9c8b2_89"},{"__ref":"Paragraph:3be3dbf9c8b2_90"},{"__ref":"Paragraph:3be3dbf9c8b2_91"},{"__ref":"Paragraph:3be3dbf9c8b2_92"},{"__ref":"Paragraph:3be3dbf9c8b2_93"},{"__ref":"Paragraph:3be3dbf9c8b2_94"},{"__ref":"Paragraph:3be3dbf9c8b2_95"},{"__ref":"Paragraph:3be3dbf9c8b2_96"},{"__ref":"Paragraph:3be3dbf9c8b2_97"},{"__ref":"Paragraph:3be3dbf9c8b2_98"},{"__ref":"Paragraph:3be3dbf9c8b2_99"},{"__ref":"Paragraph:3be3dbf9c8b2_100"},{"__ref":"Paragraph:3be3dbf9c8b2_101"},{"__ref":"Paragraph:3be3dbf9c8b2_102"},{"__ref":"Paragraph:3be3dbf9c8b2_103"},{"__ref":"Paragraph:3be3dbf9c8b2_104"},{"__ref":"Paragraph:3be3dbf9c8b2_105"},{"__ref":"Paragraph:3be3dbf9c8b2_106"},{"__ref":"Paragraph:3be3dbf9c8b2_107"},{"__ref":"Paragraph:3be3dbf9c8b2_108"},{"__ref":"Paragraph:3be3dbf9c8b2_109"},{"__ref":"Paragraph:3be3dbf9c8b2_110"},{"__ref":"Paragraph:3be3dbf9c8b2_111"},{"__ref":"Paragraph:3be3dbf9c8b2_112"},{"__ref":"Paragraph:3be3dbf9c8b2_113"},{"__ref":"Paragraph:3be3dbf9c8b2_114"},{"__ref":"Paragraph:3be3dbf9c8b2_115"},{"__ref":"Paragraph:3be3dbf9c8b2_116"},{"__ref":"Paragraph:3be3dbf9c8b2_117"},{"__ref":"Paragraph:3be3dbf9c8b2_118"},{"__ref":"Paragraph:3be3dbf9c8b2_119"},{"__ref":"Paragraph:3be3dbf9c8b2_120"},{"__ref":"Paragraph:3be3dbf9c8b2_121"},{"__ref":"Paragraph:3be3dbf9c8b2_122"},{"__ref":"Paragraph:3be3dbf9c8b2_123"},{"__ref":"Paragraph:3be3dbf9c8b2_124"},{"__ref":"Paragraph:3be3dbf9c8b2_125"},{"__ref":"Paragraph:3be3dbf9c8b2_126"},{"__ref":"Paragraph:3be3dbf9c8b2_127"},{"__ref":"Paragraph:3be3dbf9c8b2_128"},{"__ref":"Paragraph:3be3dbf9c8b2_129"},{"__ref":"Paragraph:3be3dbf9c8b2_130"},{"__ref":"Paragraph:3be3dbf9c8b2_131"},{"__ref":"Paragraph:3be3dbf9c8b2_132"},{"__ref":"Paragraph:3be3dbf9c8b2_133"},{"__ref":"Paragraph:3be3dbf9c8b2_134"},{"__ref":"Paragraph:3be3dbf9c8b2_135"},{"__ref":"Paragraph:3be3dbf9c8b2_136"},{"__ref":"Paragraph:3be3dbf9c8b2_137"},{"__ref":"Paragraph:3be3dbf9c8b2_138"},{"__ref":"Paragraph:3be3dbf9c8b2_139"},{"__ref":"Paragraph:3be3dbf9c8b2_140"},{"__ref":"Paragraph:3be3dbf9c8b2_141"},{"__ref":"Paragraph:3be3dbf9c8b2_142"},{"__ref":"Paragraph:3be3dbf9c8b2_143"},{"__ref":"Paragraph:3be3dbf9c8b2_144"},{"__ref":"Paragraph:3be3dbf9c8b2_145"},{"__ref":"Paragraph:3be3dbf9c8b2_146"},{"__ref":"Paragraph:3be3dbf9c8b2_147"},{"__ref":"Paragraph:3be3dbf9c8b2_148"},{"__ref":"Paragraph:3be3dbf9c8b2_149"},{"__ref":"Paragraph:3be3dbf9c8b2_150"},{"__ref":"Paragraph:3be3dbf9c8b2_151"},{"__ref":"Paragraph:3be3dbf9c8b2_152"},{"__ref":"Paragraph:3be3dbf9c8b2_153"},{"__ref":"Paragraph:3be3dbf9c8b2_154"},{"__ref":"Paragraph:3be3dbf9c8b2_155"},{"__ref":"Paragraph:3be3dbf9c8b2_156"},{"__ref":"Paragraph:3be3dbf9c8b2_157"},{"__ref":"Paragraph:3be3dbf9c8b2_158"},{"__ref":"Paragraph:3be3dbf9c8b2_159"},{"__ref":"Paragraph:3be3dbf9c8b2_160"},{"__ref":"Paragraph:3be3dbf9c8b2_161"},{"__ref":"Paragraph:3be3dbf9c8b2_162"},{"__ref":"Paragraph:3be3dbf9c8b2_163"},{"__ref":"Paragraph:3be3dbf9c8b2_164"},{"__ref":"Paragraph:3be3dbf9c8b2_165"},{"__ref":"Paragraph:3be3dbf9c8b2_166"},{"__ref":"Paragraph:3be3dbf9c8b2_167"},{"__ref":"Paragraph:3be3dbf9c8b2_168"},{"__ref":"Paragraph:3be3dbf9c8b2_169"},{"__ref":"Paragraph:3be3dbf9c8b2_170"},{"__ref":"Paragraph:3be3dbf9c8b2_171"},{"__ref":"Paragraph:3be3dbf9c8b2_172"},{"__ref":"Paragraph:3be3dbf9c8b2_173"},{"__ref":"Paragraph:3be3dbf9c8b2_174"},{"__ref":"Paragraph:3be3dbf9c8b2_175"},{"__ref":"Paragraph:3be3dbf9c8b2_176"},{"__ref":"Paragraph:3be3dbf9c8b2_177"},{"__ref":"Paragraph:3be3dbf9c8b2_178"},{"__ref":"Paragraph:3be3dbf9c8b2_179"},{"__ref":"Paragraph:3be3dbf9c8b2_180"},{"__ref":"Paragraph:3be3dbf9c8b2_181"},{"__ref":"Paragraph:3be3dbf9c8b2_182"},{"__ref":"Paragraph:3be3dbf9c8b2_183"},{"__ref":"Paragraph:3be3dbf9c8b2_184"},{"__ref":"Paragraph:3be3dbf9c8b2_185"},{"__ref":"Paragraph:3be3dbf9c8b2_186"},{"__ref":"Paragraph:3be3dbf9c8b2_187"},{"__ref":"Paragraph:3be3dbf9c8b2_188"},{"__ref":"Paragraph:3be3dbf9c8b2_189"},{"__ref":"Paragraph:3be3dbf9c8b2_190"},{"__ref":"Paragraph:3be3dbf9c8b2_191"},{"__ref":"Paragraph:3be3dbf9c8b2_192"},{"__ref":"Paragraph:3be3dbf9c8b2_193"},{"__ref":"Paragraph:3be3dbf9c8b2_194"},{"__ref":"Paragraph:3be3dbf9c8b2_195"},{"__ref":"Paragraph:3be3dbf9c8b2_196"},{"__ref":"Paragraph:3be3dbf9c8b2_197"},{"__ref":"Paragraph:3be3dbf9c8b2_198"},{"__ref":"Paragraph:3be3dbf9c8b2_199"},{"__ref":"Paragraph:3be3dbf9c8b2_200"},{"__ref":"Paragraph:3be3dbf9c8b2_201"},{"__ref":"Paragraph:3be3dbf9c8b2_202"},{"__ref":"Paragraph:3be3dbf9c8b2_203"},{"__ref":"Paragraph:3be3dbf9c8b2_204"},{"__ref":"Paragraph:3be3dbf9c8b2_205"},{"__ref":"Paragraph:3be3dbf9c8b2_206"},{"__ref":"Paragraph:3be3dbf9c8b2_207"},{"__ref":"Paragraph:3be3dbf9c8b2_208"},{"__ref":"Paragraph:3be3dbf9c8b2_209"},{"__ref":"Paragraph:3be3dbf9c8b2_210"},{"__ref":"Paragraph:3be3dbf9c8b2_211"},{"__ref":"Paragraph:3be3dbf9c8b2_212"},{"__ref":"Paragraph:3be3dbf9c8b2_213"},{"__ref":"Paragraph:3be3dbf9c8b2_214"},{"__ref":"Paragraph:3be3dbf9c8b2_215"},{"__ref":"Paragraph:3be3dbf9c8b2_216"},{"__ref":"Paragraph:3be3dbf9c8b2_217"},{"__ref":"Paragraph:3be3dbf9c8b2_218"},{"__ref":"Paragraph:3be3dbf9c8b2_219"},{"__ref":"Paragraph:3be3dbf9c8b2_220"},{"__ref":"Paragraph:3be3dbf9c8b2_221"},{"__ref":"Paragraph:3be3dbf9c8b2_222"},{"__ref":"Paragraph:3be3dbf9c8b2_223"},{"__ref":"Paragraph:3be3dbf9c8b2_224"},{"__ref":"Paragraph:3be3dbf9c8b2_225"},{"__ref":"Paragraph:3be3dbf9c8b2_226"},{"__ref":"Paragraph:3be3dbf9c8b2_227"},{"__ref":"Paragraph:3be3dbf9c8b2_228"},{"__ref":"Paragraph:3be3dbf9c8b2_229"},{"__ref":"Paragraph:3be3dbf9c8b2_230"},{"__ref":"Paragraph:3be3dbf9c8b2_231"},{"__ref":"Paragraph:3be3dbf9c8b2_232"},{"__ref":"Paragraph:3be3dbf9c8b2_233"},{"__ref":"Paragraph:3be3dbf9c8b2_234"},{"__ref":"Paragraph:3be3dbf9c8b2_235"},{"__ref":"Paragraph:3be3dbf9c8b2_236"},{"__ref":"Paragraph:3be3dbf9c8b2_237"},{"__ref":"Paragraph:3be3dbf9c8b2_238"},{"__ref":"Paragraph:3be3dbf9c8b2_239"},{"__ref":"Paragraph:3be3dbf9c8b2_240"},{"__ref":"Paragraph:3be3dbf9c8b2_241"},{"__ref":"Paragraph:3be3dbf9c8b2_242"},{"__ref":"Paragraph:3be3dbf9c8b2_243"},{"__ref":"Paragraph:3be3dbf9c8b2_244"},{"__ref":"Paragraph:3be3dbf9c8b2_245"},{"__ref":"Paragraph:3be3dbf9c8b2_246"},{"__ref":"Paragraph:3be3dbf9c8b2_247"},{"__ref":"Paragraph:3be3dbf9c8b2_248"},{"__ref":"Paragraph:3be3dbf9c8b2_249"},{"__ref":"Paragraph:3be3dbf9c8b2_250"},{"__ref":"Paragraph:3be3dbf9c8b2_251"},{"__ref":"Paragraph:3be3dbf9c8b2_252"},{"__ref":"Paragraph:3be3dbf9c8b2_253"},{"__ref":"Paragraph:3be3dbf9c8b2_254"},{"__ref":"Paragraph:3be3dbf9c8b2_255"},{"__ref":"Paragraph:3be3dbf9c8b2_256"},{"__ref":"Paragraph:3be3dbf9c8b2_257"},{"__ref":"Paragraph:3be3dbf9c8b2_258"},{"__ref":"Paragraph:3be3dbf9c8b2_259"},{"__ref":"Paragraph:3be3dbf9c8b2_260"},{"__ref":"Paragraph:3be3dbf9c8b2_261"},{"__ref":"Paragraph:3be3dbf9c8b2_262"},{"__ref":"Paragraph:3be3dbf9c8b2_263"},{"__ref":"Paragraph:3be3dbf9c8b2_264"},{"__ref":"Paragraph:3be3dbf9c8b2_265"},{"__ref":"Paragraph:3be3dbf9c8b2_266"},{"__ref":"Paragraph:3be3dbf9c8b2_267"}],"sections":[{"__typename":"Section","name":"3b08","startIndex":0,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"79b7","startIndex":17,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null},{"__typename":"Section","name":"849a","startIndex":263,"textLayout":null,"imageLayout":null,"backgroundImage":null,"videoLayout":null,"backgroundVideo":null}]}},"customStyleSheet":{"__ref":"CustomStyleSheet:63d23b36fcaa"},"firstPublishedAt":1595105842467,"isLocked":true,"isPublished":true,"isShortform":false,"layerCake":3,"primaryTopic":{"__ref":"Topic:1eca0103fff3"},"title":"Text Classification with NLP: Tf-Idf vs Word2Vec vs BERT","isMarkedPaywallOnly":false,"mediumUrl":"https:\u002F\u002Ftowardsdatascience.com\u002Ftext-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794","isLimitedState":false,"visibility":"LOCKED","license":"ALL_RIGHTS_RESERVED","allowResponses":true,"newsletterId":"","sequence":null,"tags":[{"__ref":"Tag:data-science"},{"__ref":"Tag:artificial-intelligence"},{"__ref":"Tag:machine-learning"},{"__ref":"Tag:programming"},{"__ref":"Tag:nlp"}],"topics":[{"__typename":"Topic","topicId":"1af65db9c2f8","name":"Artificial Intelligence"},{"__typename":"Topic","topicId":"1eca0103fff3","name":"Machine Learning"}],"inResponseToPostResult":null,"isNewsletter":false,"socialTitle":"","socialDek":"","noIndex":null,"curationStatus":null,"metaDescription":"","latestPublishedAt":1626337461612,"readingTime":21.89622641509434,"previewContent":{"__typename":"PreviewContent","subtitle":"Preprocessing, Model Design, Evaluation, Explainability for Bag-of-Words, Word Embedding, Language models"},"previewImage":{"__ref":"ImageMetadata:1*T8WWibd7u8b7gfgeG0LgAA.gif"},"clapCount":1589,"postResponses":{"__typename":"PostResponses","count":17},"isSuspended":false,"pendingCollection":null,"statusForCollection":"APPROVED","lockedSource":"LOCKED_POST_SOURCE_UGC","pinnedAt":0,"pinnedByCreatorAt":0,"curationEligibleAt":1595104727667,"responseDistribution":"NOT_DISTRIBUTED","viewerEdge":{"__ref":"PostViewerEdge:postId:41ff868d1794-viewerId:ad0cdc98db0b"},"collaborators":[],"translationSourcePost":null,"inResponseToMediaResource":null,"audioVersionUrl":"","seoTitle":"","updatedAt":1626337465430,"shortformType":"SHORTFORM_TYPE_LINK","structuredData":"","seoDescription":"","isIndexable":true,"latestPublishedVersion":"3be3dbf9c8b2","isPublishToEmail":false,"voterCount":481,"recommenders":[]}}</script><script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/manifest.57699318.js"></script><script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/9115.1a9358c4.js"></script><script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/main.94b1390b.js"></script><script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/5573.159bf40f.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/instrumentation.6fa29f8a.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/reporting.6471519f.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/6506.eabfef9b.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/192.e3ad013d.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1645.857c77e3.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/2388.e6e22665.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/3930.c5902e0c.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/8940.408c3b3f.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1801.44c1dd57.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/8694.f0b41de1.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/7753.2f5de15e.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/4881.32b3f082.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/5727.8c32a0a2.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/9068.81cc309b.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/7883.d48af7b1.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/7001.0ecbfc37.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/700.05d687ed.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/8607.eb32b1d9.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1479.3e92dbb9.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/3673.7872fb25.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/609.c42a3b96.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/8886.309da8fb.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/7031.06e0422a.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/9972.370e8bc2.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/7515.a3b27bac.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/2182.b5e49e57.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/5435.046a7578.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/9293.391b012a.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/1794.352c336e.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/9264.f1bd66fc.chunk.js"></script>
<script src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/Post.f1d7c8fd.chunk.js"></script><script>window.main();</script><div><div class="fm fy vq fv vr ac vs" tabindex="-1"><div class="ed gp ew vt" style="position: absolute; inset: 0px auto auto 0px; transform: translate3d(1032px, 689px, 0px);" data-popper-reference-hidden="false" data-popper-escaped="false" data-popper-placement="bottom"><div class="gn gp"><div class="wc ub"><span tabindex="0"></span><div class="vl wd s"><p class="bf b nc bh ew">Click to add this story to a list.</p><div class="na s"><p class="bf b nc bh es"><button class="cw dm bl bm bn bo bp bq br bs fg fh bv fi fj"><span class="hs">Got it</span></button></p></div></div><span tabindex="0"></span></div><div class="fm gw vu rj vv vw gz vx vy vz wa wb" style="position: absolute; left: 0px; transform: translate3d(93px, 0px, 0px);"></div></div></div></div></div><iframe src="./Text Classification with NLP_ Tf-Idf vs Word2Vec vs BERT _ by Mauro Di Pietro _ Towards Data Science_files/a16180790160.html" hidden="" tabindex="-1" title="Optimizely Internal Frame" height="0" width="0" style="display: none;"></iframe></body></html>